{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ga88fRoXmt9BRzgWvgj2kWoMoLNrBUMm","timestamp":1548224580383}]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tao2w0W4IwEO"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2Pkm2A7Ss0g1"},"source":["### Learning Objective"]},{"cell_type":"markdown","metadata":{"id":"YEjLydYUgjDV"},"source":["At the end of the experiment, you will be able to :\n","\n","- Understand the concept of Gradient descent method\n","- Observe the effect of learning rate"]},{"cell_type":"code","metadata":{"id":"XDbRvIzDp_K8","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","HTML(\"\"\"<video width='854' height='480' controls>\n","<source src=\"https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/gradient_descent.mp4\" type='video/mp4'>\n","</video>\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"og1EHpbjgrHP"},"source":["##Dataset"]},{"cell_type":"markdown","metadata":{"id":"rSwgCEA_uJ40"},"source":["###Description"]},{"cell_type":"markdown","metadata":{"id":"mDRTMSzJTJXD"},"source":["\n","The dataset consists of two columns and 90 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is  $l∝t^2$ .\n"]},{"cell_type":"markdown","metadata":{"id":"nn9drxhC3f6e"},"source":["##AI/ML Technique"]},{"cell_type":"markdown","metadata":{"id":"tgqxbZf394do"},"source":["#### Gradient Descent\n","\n","Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on first order gradients, that tweaks it’s parameters iteratively to minimize a given function to its local minimum and global minima if the function is convex."]},{"cell_type":"markdown","metadata":{"id":"CGdoLqkdFmDm"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEzlYL4CDrmE"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","import re\n","ipython = get_ipython()\n","\n","notebook= \"U2W6_21_BatchGD_LRdecay_B\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    ipython.magic(\"sx wget -qq https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Exp1/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\")\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_inclass_mentor\": Mentor_support}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","def getWalkthrough():\n","  try:\n","    if not Walkthrough:\n","      raise NameError\n","    else:\n","      return Walkthrough\n","  except NameError:\n","    print (\"Please answer Walkthrough Question\")\n","    return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHOajhjcuOGA"},"source":["## Import the required Packages\n"]},{"cell_type":"code","metadata":{"id":"HxcZUqq5IwEY"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecrcyzhvuRu2"},"source":["## Load the data"]},{"cell_type":"code","metadata":{"id":"5Ttnpu44IwEb"},"source":["# Load the data by using pandas read_csv()\n","data = pd.read_csv(\"AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n","\n","# YOUR CODE HERE: Display the first 5 rows of dataframe 'data'\n","\n","# YOUR CODE HERE: Display the last 5 rows of dataframe 'data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGdRFp1LIwEd"},"source":["# Get the length and time period values from the dataset\n","l = data['l'].values\n","t = data['t'].values\n","# Get the square of the time variable to form a linear equation\n","tsq = t * t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79f75Z8KRRIs"},"source":["## Gradient Descent\n","\n","Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function.\n","\n","A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference or distance between the predicted value and the actual value."]},{"cell_type":"markdown","metadata":{"id":"jOAEeIxAdUXD"},"source":["### Batch Gradient Descent\n","\n","Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated.\n","\n","Firstly, we calculate the cost function which is $E$ = $(y - y_i)^2$ and we tweak the slope m and bias c to reduce the cost function.\n","\n","$\\frac{\\partial E_i }{\\partial m}$ = $ \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (mx_i + c)) * x_i$\n","\n","$\\frac{\\partial E_i }{\\partial c}$ = $ \\frac{-2}{n} \\sum_{i=1}^{n}(y_i - (mx_i + c))$\n","\n","And then we update the slope and bias with with change in slope $\\Delta m$ and change in bias $\\Delta c$ with learning rate $eta$\n","\n","$m$  = $m - \\Delta m * eta$\n","\n","$c$  = $c - \\Delta c * eta$\n","\n"]},{"cell_type":"code","metadata":{"id":"8m6BE6v-IwEg"},"source":["\"\"\"\n","The function 'train' updates the values of m and c and calculates error.\n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized error is returned.\n","\"\"\"\n","def train(x, y, m, c, eta):\n","    const = - 2.0/len(y)\n","    ycalc = m * x + c\n","    # YOUR CODE HERE: To calculate the gradients (\"delta_m\" and \"delta_c\")\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    error = sum((y - ycalc)**2)/len(y)\n","    return m, c, error"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHjYX45BIwEj"},"source":["### Effect of varying LR (learning rate, $\\eta$) on error and final line\n","\n","Let us vary LR and find how the error decreases in each case, and how the final line looks, by training each case for the same number of iterations - 2000."]},{"cell_type":"markdown","metadata":{"id":"ABokUtARNHkl"},"source":["#### $\\eta$ (Learning rate) = 0.1"]},{"cell_type":"code","metadata":{"id":"qjQJXM-4IwEl"},"source":["# Save errors\n","errs_1 = []\n","# Initialize m,c value\n","m, c = 0, 0\n","eta = 0.1 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.1.\n","for iteration in range(2000):\n","    m, c, error,_,_ = # YOUR CODE HERE : Call 'train' function defined above by passing 'l', 'tsq', 'm', 'c', 'eta' as it's arguments.\n","    # YOUR CODE HERE: To append the errors to \"errs_1\"\n","\n","# Save final line\n","m_1, c_1 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tB40sJhTIwEp"},"source":["#### $\\eta$ = 0.01"]},{"cell_type":"code","metadata":{"id":"cyLOvjaLIwEq"},"source":["# Save errors\n","errs_01 = []\n","m, c = 0, 0\n","eta = 0.01  # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.01.\n","for iteration in range(2000):\n","    m, c, error,_,_ = # YOUR CODE HERE : Call 'train' function defined above by passing 'l', 'tsq', 'm', 'c', 'eta' as it's arguments.\n","    # YOUR CODE HERE: To append the errors to \"errs_01\"\n","\n","# Save final line\n","m_01, c_01 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E22O2IakIwEt"},"source":["#### $\\eta$ = 0.001"]},{"cell_type":"code","metadata":{"id":"Cvl81aceIwEt"},"source":["# Save errors\n","errs_001 = []\n","m, c = 0, 0\n","eta = 0.001 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.001.\n","for iteration in range(2000):\n","    m, c, error,_,_ = train(l, tsq, m, c, eta)\n","    # YOUR CODE HERE: To append the errors to \"errs_001\"\n","\n","# Save final line\n","m_001, c_001 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIdlxdYIIwEv"},"source":["#### $\\eta$ = 0.0001"]},{"cell_type":"code","metadata":{"id":"Y_e1NK5qIwEw"},"source":["# Save errors\n","errs_0001 = []\n","m, c = 0, 0\n","eta = 0.0001 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.0001.\n","for iteration in range(2000):\n","    m, c, error,_,_ = train(l, tsq, m, c, eta)\n","    # YOUR CODE HERE: To append the errors to \"errs_0001\"\n","\n","# Save final line\n","m_0001, c_0001 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fguUY_h6N4mX"},"source":["### Plot of lines vs $\\eta$ (Learning rate)"]},{"cell_type":"code","metadata":{"id":"XWFue0e6IwE1"},"source":["# Find the lines\n","y_1 = m_1 * l + c_1\n","y_01 = m_01 * l + c_01\n","y_001 = m_001 * l + c_001\n","y_0001 = m_0001 * l + c_0001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyCs_U80IwE4"},"source":["plt.figure(figsize=(15, 8))\n","\n","# YOUR CODE HERE: For all the lines above i.e. y_1, y_01, y_001, y_0001 plot them (in a different color) against the lines \"l\"\n","\n","plt.legend([\"l vs tsq\",\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWp-qhiZIwE6"},"source":["Thus, we see that higher learning rates reach the best fit faster than lower learning rates (obviously)."]},{"cell_type":"markdown","metadata":{"id":"d7nl_icQOLku"},"source":["### Plot of errors vs epochs for each $\\eta$ (Learning rate)"]},{"cell_type":"code","metadata":{"id":"ds8LI8XBIwE7"},"source":["epochs = range(0,2000)\n","plt.figure(figsize=(16,10))\n","# YOUR CODE HERE: To plot \"epochs\" versus errs_1, errs_01, errs_001, errs_0001 (each in a different color)\n","plt.legend([\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n","plt.xlabel('Epochs')\n","plt.ylabel('Errors')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqL3Qd4dIwFA"},"source":["### With LR (learning rate, $\\eta$) Decay\n","\n","In some cases, the learning rate might be too high to give good fitting lines. For example, let us train with constant LR (learning rate, $\\eta$) of 0.8 and get the final line after 1000 iterations:"]},{"cell_type":"markdown","metadata":{"id":"eoW3VvR8IwFB"},"source":["#### $\\eta$ = 0.8"]},{"cell_type":"code","metadata":{"id":"A_Z-use6IwFC"},"source":["errs = []\n","m, c = 0, 0\n","eta = 0.8\n","\n","# Call the train() method for 1000 iterations to update m and c and get error value with constant eta = 0.8.\n","for times in range(1000):\n","    m, c, error,_,_ = # YOUR CODE HERE: Call 'train' function defined above by passing 'l', 'tsq', 'm', 'c', 'eta' as it's arguments.\n","    # YOUR CODE HERE: To append the errors to \"errs\"\n","\n","m_normal, c_normal = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjmDAryWIwFG"},"source":["Let us see the plot of error vs iterations:"]},{"cell_type":"code","metadata":{"id":"s0N0o3NKIwFH"},"source":["plt.plot(range(len(errs)), errs)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yN5_kJn0IwFJ"},"source":["We see that the error quickly goes to almost 0, but after some iterations it blows up.\n","\n","Let us check the \"best fit\" line that is found:"]},{"cell_type":"code","metadata":{"id":"DXumozZ-IwFL"},"source":["print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_normal, c_normal, errs[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zB7RTTOIwFM"},"source":["y = m_normal * l + c_normal\n","plt.plot(l, tsq, '.k', label = 'Actual')\n","plt.plot(l, y, \"r\", label = 'Prediction')\n","plt.xlabel(\"Length (L)\")\n","plt.ylabel(\"T^2\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fqmvtu-_IwFQ"},"source":["Clearly this is not ideal.\n","\n","This was a simple case where we can see the learning rate is too high. There might be cases where it is not so simple to identify this. Also, having a low learning rate is not good because training time would be too high!\n","\n","**Solution: Decay the learning rate.**\n","\n","Now let us train another model with decaying lr. But let us not decay lr below 0.0001."]},{"cell_type":"code","metadata":{"id":"edZDbSyBIwFR"},"source":["errs_decay = []\n","m, c = 0, 0\n","eta = 0.5\n","decay_factor = 0.99\n","\n","# Call the train() method for 1000 iterations to update m and c and get error value with decaying eta.\n","for iteration in range(1000):\n","    eta = max(0.0001, eta * decay_factor)\n","    m, c, error,_,_ = # YOUR CODE HERE: Call 'train' function defined above by passing 'l', 'tsq', 'm', 'c', 'eta' as it's arguments.\n","    # YOUR CODE HERE: To append the errors to \"errs_decay\"\n","\n","m_decay, c_decay = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbEAsf4cIwFS"},"source":["print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_decay, c_decay, errs_decay[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clqa9StFVKqP"},"source":["Let us see the plot of error vs iterations:"]},{"cell_type":"code","metadata":{"id":"WNUxY6ZyIwFV"},"source":["plt.plot(range(len(errs_decay)), errs_decay)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRoeB71EIwFW"},"source":["y = m_decay * l + c_decay\n","plt.plot(l, tsq, '.k')\n","plt.plot(l, y, \"r\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djZn9yLqIwFa"},"source":["Thus, this is correct."]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kmvdJ4aNmGjR"},"source":["#@title State True or False: Gradient descent always finds the global minima in a single iteration? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\",\"True\",\"False\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJ6Y8DkRGZkF"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clKRj7eGGZkG"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r35isHfTVGKc"},"source":["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}