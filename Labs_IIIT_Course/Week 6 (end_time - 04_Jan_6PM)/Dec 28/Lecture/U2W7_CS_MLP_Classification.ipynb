{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WYDPGccf_JXv"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"03BMEV88bUoa"},"source":["###Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"M3KlC2Y6_P1E"},"source":["\n","####Classification using MLP"]},{"cell_type":"markdown","metadata":{"id":"d2gjZ5oU_hrk"},"source":["The objective of this case study is to understand classification i.e., if the price of the house is low (0) or high (1) using Multilayer perceptron."]},{"cell_type":"markdown","metadata":{"id":"jHSeCQcxJRJr"},"source":["## Setup Steps"]},{"cell_type":"code","metadata":{"id":"2hxRMCMwJRJr"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"aiml_pg_25\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nUtfFS0JRJr"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"4521452411\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1750021757310,"user_tz":-330,"elapsed":127095,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}},"outputId":"8db1476c-f846-4f5a-af66-932dcab12872","cellView":"form","id":"r1NlgkaOJRJs"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"U2W7_CS_MLP_Classification\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Iris.csv\")\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts() and getComments():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"id\" : Id, \"file_hash\" : file_hash,\n","              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n","        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=aiml_pg_25&recordId=3969\"></script>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Setup completed successfully\n"]}]},{"cell_type":"markdown","metadata":{"id":"6u0c6-VXdWjO"},"source":["## Importing required packages"]},{"cell_type":"code","metadata":{"id":"T2vxSk8VpJ0L"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HkNbwhPJUKBL"},"source":["The attributes of related House price are stored in \"X\" as features and the House price whether it is low (0) or high (1) are stored in \"y\" as labels."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-11-23T07:34:08.691158Z","start_time":"2018-11-23T07:34:08.685467Z"},"id":"KhJrSQ0wmQDN"},"source":["X = np.array([[3, 2000, 90], [2, 800, 143], [2, 850, 167], [1, 550, 267], [4, 2000, 396]])\n","y = np.array([1, 0, 0, 0 , 1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IapENOaWURwB"},"source":["###Standard scaling the features \"X\""]},{"cell_type":"code","metadata":{"id":"rUBlUjsLpxQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750021759800,"user_tz":-330,"elapsed":28,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}},"outputId":"e9ff227e-214d-465e-89d6-4b983af6706a"},"source":["ss = StandardScaler()\n","ss.fit(X)\n","X = ss.transform(X)\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.58834841,  1.20863526, -1.13296108],\n","       [-0.39223227, -0.6997362 , -0.64318182],\n","       [-0.39223227, -0.62022073, -0.42139498],\n","       [-1.37281295, -1.09731359,  0.50271682],\n","       [ 1.56892908,  1.20863526,  1.69482106]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"dynamic-score"},"source":["### MLP Classifier"]},{"cell_type":"code","metadata":{"id":"caroline-massachusetts"},"source":["from sklearn.neural_network import MLPClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"parental-little"},"source":["clf = MLPClassifier(activation= 'relu', solver= 'sgd',max_iter= 5000, hidden_layer_sizes= (4), learning_rate_init= 0.01, tol= 1e-7, verbose= 1, random_state= 123)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"broadband-fishing","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1750021763210,"user_tz":-330,"elapsed":2901,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}},"outputId":"e50b4620-e4cc-40ca-8cf3-0a6d4099504c"},"source":["clf.fit(X, y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.81585065\n","Iteration 2, loss = 0.81237303\n","Iteration 3, loss = 0.80748595\n","Iteration 4, loss = 0.80139996\n","Iteration 5, loss = 0.79431169\n","Iteration 6, loss = 0.78640000\n","Iteration 7, loss = 0.77782366\n","Iteration 8, loss = 0.76872025\n","Iteration 9, loss = 0.75924102\n","Iteration 10, loss = 0.74949035\n","Iteration 11, loss = 0.73953262\n","Iteration 12, loss = 0.72943346\n","Iteration 13, loss = 0.71924359\n","Iteration 14, loss = 0.70900086\n","Iteration 15, loss = 0.69873236\n","Iteration 16, loss = 0.68845625\n","Iteration 17, loss = 0.67818356\n","Iteration 18, loss = 0.66791976\n","Iteration 19, loss = 0.65766612\n","Iteration 20, loss = 0.64745909\n","Iteration 21, loss = 0.63780625\n","Iteration 22, loss = 0.62826277\n","Iteration 23, loss = 0.61882764\n","Iteration 24, loss = 0.60949873\n","Iteration 25, loss = 0.60076635\n","Iteration 26, loss = 0.59282400\n","Iteration 27, loss = 0.58488338\n","Iteration 28, loss = 0.57694468\n","Iteration 29, loss = 0.56916301\n","Iteration 30, loss = 0.56200453\n","Iteration 31, loss = 0.55480474\n","Iteration 32, loss = 0.54756856\n","Iteration 33, loss = 0.54030104\n","Iteration 34, loss = 0.53300738\n","Iteration 35, loss = 0.52569295\n","Iteration 36, loss = 0.51836329\n","Iteration 37, loss = 0.51102412\n","Iteration 38, loss = 0.50368135\n","Iteration 39, loss = 0.49634102\n","Iteration 40, loss = 0.48900933\n","Iteration 41, loss = 0.48169256\n","Iteration 42, loss = 0.47439708\n","Iteration 43, loss = 0.46712926\n","Iteration 44, loss = 0.45989549\n","Iteration 45, loss = 0.45270210\n","Iteration 46, loss = 0.44555530\n","Iteration 47, loss = 0.43846118\n","Iteration 48, loss = 0.43142564\n","Iteration 49, loss = 0.42445436\n","Iteration 50, loss = 0.41755276\n","Iteration 51, loss = 0.41072596\n","Iteration 52, loss = 0.40397880\n","Iteration 53, loss = 0.39731572\n","Iteration 54, loss = 0.39074085\n","Iteration 55, loss = 0.38425792\n","Iteration 56, loss = 0.37787029\n","Iteration 57, loss = 0.37158093\n","Iteration 58, loss = 0.36539241\n","Iteration 59, loss = 0.35930695\n","Iteration 60, loss = 0.35332636\n","Iteration 61, loss = 0.34745212\n","Iteration 62, loss = 0.34168533\n","Iteration 63, loss = 0.33602680\n","Iteration 64, loss = 0.33047697\n","Iteration 65, loss = 0.32503604\n","Iteration 66, loss = 0.31970391\n","Iteration 67, loss = 0.31448021\n","Iteration 68, loss = 0.30936438\n","Iteration 69, loss = 0.30435562\n","Iteration 70, loss = 0.29945295\n","Iteration 71, loss = 0.29465522\n","Iteration 72, loss = 0.28996113\n","Iteration 73, loss = 0.28536925\n","Iteration 74, loss = 0.28087805\n","Iteration 75, loss = 0.27648589\n","Iteration 76, loss = 0.27219105\n","Iteration 77, loss = 0.26799174\n","Iteration 78, loss = 0.26388615\n","Iteration 79, loss = 0.25987238\n","Iteration 80, loss = 0.25594854\n","Iteration 81, loss = 0.25211270\n","Iteration 82, loss = 0.24836292\n","Iteration 83, loss = 0.24469726\n","Iteration 84, loss = 0.24111378\n","Iteration 85, loss = 0.23761057\n","Iteration 86, loss = 0.23418569\n","Iteration 87, loss = 0.23083726\n","Iteration 88, loss = 0.22756341\n","Iteration 89, loss = 0.22436229\n","Iteration 90, loss = 0.22123208\n","Iteration 91, loss = 0.21817100\n","Iteration 92, loss = 0.21517729\n","Iteration 93, loss = 0.21224923\n","Iteration 94, loss = 0.20938514\n","Iteration 95, loss = 0.20658337\n","Iteration 96, loss = 0.20384231\n","Iteration 97, loss = 0.20116039\n","Iteration 98, loss = 0.19853606\n","Iteration 99, loss = 0.19596784\n","Iteration 100, loss = 0.19345426\n","Iteration 101, loss = 0.19099389\n","Iteration 102, loss = 0.18858535\n","Iteration 103, loss = 0.18622728\n","Iteration 104, loss = 0.18391837\n","Iteration 105, loss = 0.18165734\n","Iteration 106, loss = 0.17944295\n","Iteration 107, loss = 0.17727397\n","Iteration 108, loss = 0.17514924\n","Iteration 109, loss = 0.17306761\n","Iteration 110, loss = 0.17102795\n","Iteration 111, loss = 0.16902920\n","Iteration 112, loss = 0.16707030\n","Iteration 113, loss = 0.16515022\n","Iteration 114, loss = 0.16326797\n","Iteration 115, loss = 0.16142259\n","Iteration 116, loss = 0.15961314\n","Iteration 117, loss = 0.15783870\n","Iteration 118, loss = 0.15609839\n","Iteration 119, loss = 0.15439135\n","Iteration 120, loss = 0.15271675\n","Iteration 121, loss = 0.15107378\n","Iteration 122, loss = 0.14946163\n","Iteration 123, loss = 0.14787956\n","Iteration 124, loss = 0.14632681\n","Iteration 125, loss = 0.14480267\n","Iteration 126, loss = 0.14330642\n","Iteration 127, loss = 0.14183740\n","Iteration 128, loss = 0.14039493\n","Iteration 129, loss = 0.13897838\n","Iteration 130, loss = 0.13758711\n","Iteration 131, loss = 0.13622052\n","Iteration 132, loss = 0.13487803\n","Iteration 133, loss = 0.13355905\n","Iteration 134, loss = 0.13226303\n","Iteration 135, loss = 0.13098943\n","Iteration 136, loss = 0.12973772\n","Iteration 137, loss = 0.12850739\n","Iteration 138, loss = 0.12729794\n","Iteration 139, loss = 0.12610889\n","Iteration 140, loss = 0.12493977\n","Iteration 141, loss = 0.12379012\n","Iteration 142, loss = 0.12265950\n","Iteration 143, loss = 0.12154748\n","Iteration 144, loss = 0.12045363\n","Iteration 145, loss = 0.11937756\n","Iteration 146, loss = 0.11831885\n","Iteration 147, loss = 0.11727712\n","Iteration 148, loss = 0.11625200\n","Iteration 149, loss = 0.11524313\n","Iteration 150, loss = 0.11425014\n","Iteration 151, loss = 0.11327269\n","Iteration 152, loss = 0.11231044\n","Iteration 153, loss = 0.11136307\n","Iteration 154, loss = 0.11043025\n","Iteration 155, loss = 0.10951167\n","Iteration 156, loss = 0.10860704\n","Iteration 157, loss = 0.10771605\n","Iteration 158, loss = 0.10683842\n","Iteration 159, loss = 0.10597387\n","Iteration 160, loss = 0.10512213\n","Iteration 161, loss = 0.10428294\n","Iteration 162, loss = 0.10345602\n","Iteration 163, loss = 0.10264114\n","Iteration 164, loss = 0.10183804\n","Iteration 165, loss = 0.10104649\n","Iteration 166, loss = 0.10026626\n","Iteration 167, loss = 0.09949711\n","Iteration 168, loss = 0.09873883\n","Iteration 169, loss = 0.09799120\n","Iteration 170, loss = 0.09725401\n","Iteration 171, loss = 0.09652705\n","Iteration 172, loss = 0.09581013\n","Iteration 173, loss = 0.09510304\n","Iteration 174, loss = 0.09440561\n","Iteration 175, loss = 0.09371763\n","Iteration 176, loss = 0.09303894\n","Iteration 177, loss = 0.09236935\n","Iteration 178, loss = 0.09170869\n","Iteration 179, loss = 0.09105679\n","Iteration 180, loss = 0.09041350\n","Iteration 181, loss = 0.08977863\n","Iteration 182, loss = 0.08915205\n","Iteration 183, loss = 0.08853360\n","Iteration 184, loss = 0.08792312\n","Iteration 185, loss = 0.08732048\n","Iteration 186, loss = 0.08672552\n","Iteration 187, loss = 0.08613811\n","Iteration 188, loss = 0.08555812\n","Iteration 189, loss = 0.08498541\n","Iteration 190, loss = 0.08441984\n","Iteration 191, loss = 0.08386130\n","Iteration 192, loss = 0.08330966\n","Iteration 193, loss = 0.08276479\n","Iteration 194, loss = 0.08222659\n","Iteration 195, loss = 0.08169493\n","Iteration 196, loss = 0.08116969\n","Iteration 197, loss = 0.08065078\n","Iteration 198, loss = 0.08013807\n","Iteration 199, loss = 0.07963147\n","Iteration 200, loss = 0.07913086\n","Iteration 201, loss = 0.07863616\n","Iteration 202, loss = 0.07814725\n","Iteration 203, loss = 0.07766405\n","Iteration 204, loss = 0.07718645\n","Iteration 205, loss = 0.07671437\n","Iteration 206, loss = 0.07624770\n","Iteration 207, loss = 0.07578637\n","Iteration 208, loss = 0.07533029\n","Iteration 209, loss = 0.07487937\n","Iteration 210, loss = 0.07443352\n","Iteration 211, loss = 0.07399267\n","Iteration 212, loss = 0.07355673\n","Iteration 213, loss = 0.07312563\n","Iteration 214, loss = 0.07269928\n","Iteration 215, loss = 0.07227762\n","Iteration 216, loss = 0.07186057\n","Iteration 217, loss = 0.07144806\n","Iteration 218, loss = 0.07104001\n","Iteration 219, loss = 0.07063636\n","Iteration 220, loss = 0.07023703\n","Iteration 221, loss = 0.06984197\n","Iteration 222, loss = 0.06945111\n","Iteration 223, loss = 0.06906437\n","Iteration 224, loss = 0.06868171\n","Iteration 225, loss = 0.06830305\n","Iteration 226, loss = 0.06792834\n","Iteration 227, loss = 0.06755752\n","Iteration 228, loss = 0.06719053\n","Iteration 229, loss = 0.06682731\n","Iteration 230, loss = 0.06646781\n","Iteration 231, loss = 0.06611197\n","Iteration 232, loss = 0.06575974\n","Iteration 233, loss = 0.06541106\n","Iteration 234, loss = 0.06506589\n","Iteration 235, loss = 0.06472417\n","Iteration 236, loss = 0.06438585\n","Iteration 237, loss = 0.06405089\n","Iteration 238, loss = 0.06371924\n","Iteration 239, loss = 0.06339084\n","Iteration 240, loss = 0.06306565\n","Iteration 241, loss = 0.06274364\n","Iteration 242, loss = 0.06242474\n","Iteration 243, loss = 0.06210893\n","Iteration 244, loss = 0.06179615\n","Iteration 245, loss = 0.06148636\n","Iteration 246, loss = 0.06117953\n","Iteration 247, loss = 0.06087560\n","Iteration 248, loss = 0.06057455\n","Iteration 249, loss = 0.06027634\n","Iteration 250, loss = 0.05998091\n","Iteration 251, loss = 0.05968824\n","Iteration 252, loss = 0.05939829\n","Iteration 253, loss = 0.05911103\n","Iteration 254, loss = 0.05882640\n","Iteration 255, loss = 0.05854439\n","Iteration 256, loss = 0.05826496\n","Iteration 257, loss = 0.05798807\n","Iteration 258, loss = 0.05771368\n","Iteration 259, loss = 0.05744177\n","Iteration 260, loss = 0.05717231\n","Iteration 261, loss = 0.05690526\n","Iteration 262, loss = 0.05664058\n","Iteration 263, loss = 0.05637826\n","Iteration 264, loss = 0.05611825\n","Iteration 265, loss = 0.05586054\n","Iteration 266, loss = 0.05560508\n","Iteration 267, loss = 0.05535186\n","Iteration 268, loss = 0.05510083\n","Iteration 269, loss = 0.05485199\n","Iteration 270, loss = 0.05460529\n","Iteration 271, loss = 0.05436071\n","Iteration 272, loss = 0.05411823\n","Iteration 273, loss = 0.05387781\n","Iteration 274, loss = 0.05363944\n","Iteration 275, loss = 0.05340309\n","Iteration 276, loss = 0.05316872\n","Iteration 277, loss = 0.05293633\n","Iteration 278, loss = 0.05270588\n","Iteration 279, loss = 0.05247735\n","Iteration 280, loss = 0.05225072\n","Iteration 281, loss = 0.05202596\n","Iteration 282, loss = 0.05180306\n","Iteration 283, loss = 0.05158198\n","Iteration 284, loss = 0.05136271\n","Iteration 285, loss = 0.05114523\n","Iteration 286, loss = 0.05092951\n","Iteration 287, loss = 0.05071554\n","Iteration 288, loss = 0.05050329\n","Iteration 289, loss = 0.05029275\n","Iteration 290, loss = 0.05008389\n","Iteration 291, loss = 0.04987669\n","Iteration 292, loss = 0.04967114\n","Iteration 293, loss = 0.04946721\n","Iteration 294, loss = 0.04926489\n","Iteration 295, loss = 0.04906416\n","Iteration 296, loss = 0.04886499\n","Iteration 297, loss = 0.04866739\n","Iteration 298, loss = 0.04847131\n","Iteration 299, loss = 0.04827675\n","Iteration 300, loss = 0.04808370\n","Iteration 301, loss = 0.04789212\n","Iteration 302, loss = 0.04770202\n","Iteration 303, loss = 0.04751336\n","Iteration 304, loss = 0.04732614\n","Iteration 305, loss = 0.04714034\n","Iteration 306, loss = 0.04695594\n","Iteration 307, loss = 0.04677292\n","Iteration 308, loss = 0.04659128\n","Iteration 309, loss = 0.04641100\n","Iteration 310, loss = 0.04623206\n","Iteration 311, loss = 0.04605444\n","Iteration 312, loss = 0.04587814\n","Iteration 313, loss = 0.04570314\n","Iteration 314, loss = 0.04552943\n","Iteration 315, loss = 0.04535698\n","Iteration 316, loss = 0.04518579\n","Iteration 317, loss = 0.04501585\n","Iteration 318, loss = 0.04484714\n","Iteration 319, loss = 0.04467964\n","Iteration 320, loss = 0.04451336\n","Iteration 321, loss = 0.04434826\n","Iteration 322, loss = 0.04418434\n","Iteration 323, loss = 0.04402160\n","Iteration 324, loss = 0.04386000\n","Iteration 325, loss = 0.04369956\n","Iteration 326, loss = 0.04354024\n","Iteration 327, loss = 0.04338204\n","Iteration 328, loss = 0.04322496\n","Iteration 329, loss = 0.04306897\n","Iteration 330, loss = 0.04291406\n","Iteration 331, loss = 0.04276023\n","Iteration 332, loss = 0.04260747\n","Iteration 333, loss = 0.04245575\n","Iteration 334, loss = 0.04230509\n","Iteration 335, loss = 0.04215545\n","Iteration 336, loss = 0.04200683\n","Iteration 337, loss = 0.04185923\n","Iteration 338, loss = 0.04171263\n","Iteration 339, loss = 0.04156702\n","Iteration 340, loss = 0.04142239\n","Iteration 341, loss = 0.04127874\n","Iteration 342, loss = 0.04113604\n","Iteration 343, loss = 0.04099430\n","Iteration 344, loss = 0.04085351\n","Iteration 345, loss = 0.04071365\n","Iteration 346, loss = 0.04057471\n","Iteration 347, loss = 0.04043669\n","Iteration 348, loss = 0.04029958\n","Iteration 349, loss = 0.04016337\n","Iteration 350, loss = 0.04002804\n","Iteration 351, loss = 0.03989360\n","Iteration 352, loss = 0.03976004\n","Iteration 353, loss = 0.03962734\n","Iteration 354, loss = 0.03949549\n","Iteration 355, loss = 0.03936450\n","Iteration 356, loss = 0.03923434\n","Iteration 357, loss = 0.03910502\n","Iteration 358, loss = 0.03897652\n","Iteration 359, loss = 0.03884884\n","Iteration 360, loss = 0.03872197\n","Iteration 361, loss = 0.03859590\n","Iteration 362, loss = 0.03847063\n","Iteration 363, loss = 0.03834615\n","Iteration 364, loss = 0.03822244\n","Iteration 365, loss = 0.03809951\n","Iteration 366, loss = 0.03797734\n","Iteration 367, loss = 0.03785594\n","Iteration 368, loss = 0.03773528\n","Iteration 369, loss = 0.03761537\n","Iteration 370, loss = 0.03749620\n","Iteration 371, loss = 0.03737776\n","Iteration 372, loss = 0.03726005\n","Iteration 373, loss = 0.03714305\n","Iteration 374, loss = 0.03702677\n","Iteration 375, loss = 0.03691119\n","Iteration 376, loss = 0.03679631\n","Iteration 377, loss = 0.03668213\n","Iteration 378, loss = 0.03656863\n","Iteration 379, loss = 0.03645582\n","Iteration 380, loss = 0.03634368\n","Iteration 381, loss = 0.03623220\n","Iteration 382, loss = 0.03612140\n","Iteration 383, loss = 0.03601125\n","Iteration 384, loss = 0.03590175\n","Iteration 385, loss = 0.03579289\n","Iteration 386, loss = 0.03568468\n","Iteration 387, loss = 0.03557711\n","Iteration 388, loss = 0.03547016\n","Iteration 389, loss = 0.03536384\n","Iteration 390, loss = 0.03525813\n","Iteration 391, loss = 0.03515304\n","Iteration 392, loss = 0.03504856\n","Iteration 393, loss = 0.03494468\n","Iteration 394, loss = 0.03484140\n","Iteration 395, loss = 0.03473872\n","Iteration 396, loss = 0.03463662\n","Iteration 397, loss = 0.03453510\n","Iteration 398, loss = 0.03443416\n","Iteration 399, loss = 0.03433380\n","Iteration 400, loss = 0.03423400\n","Iteration 401, loss = 0.03413477\n","Iteration 402, loss = 0.03403609\n","Iteration 403, loss = 0.03393790\n","Iteration 404, loss = 0.03384014\n","Iteration 405, loss = 0.03374289\n","Iteration 406, loss = 0.03364616\n","Iteration 407, loss = 0.03354994\n","Iteration 408, loss = 0.03345424\n","Iteration 409, loss = 0.03335905\n","Iteration 410, loss = 0.03326436\n","Iteration 411, loss = 0.03317019\n","Iteration 412, loss = 0.03307652\n","Iteration 413, loss = 0.03298335\n","Iteration 414, loss = 0.03289068\n","Iteration 415, loss = 0.03279851\n","Iteration 416, loss = 0.03270683\n","Iteration 417, loss = 0.03261565\n","Iteration 418, loss = 0.03252495\n","Iteration 419, loss = 0.03243474\n","Iteration 420, loss = 0.03234500\n","Iteration 421, loss = 0.03225575\n","Iteration 422, loss = 0.03216697\n","Iteration 423, loss = 0.03207867\n","Iteration 424, loss = 0.03199083\n","Iteration 425, loss = 0.03190346\n","Iteration 426, loss = 0.03181655\n","Iteration 427, loss = 0.03173010\n","Iteration 428, loss = 0.03164410\n","Iteration 429, loss = 0.03155856\n","Iteration 430, loss = 0.03147347\n","Iteration 431, loss = 0.03138882\n","Iteration 432, loss = 0.03130461\n","Iteration 433, loss = 0.03122085\n","Iteration 434, loss = 0.03113752\n","Iteration 435, loss = 0.03105462\n","Iteration 436, loss = 0.03097216\n","Iteration 437, loss = 0.03089012\n","Iteration 438, loss = 0.03080850\n","Iteration 439, loss = 0.03072730\n","Iteration 440, loss = 0.03064652\n","Iteration 441, loss = 0.03056616\n","Iteration 442, loss = 0.03048620\n","Iteration 443, loss = 0.03040666\n","Iteration 444, loss = 0.03032752\n","Iteration 445, loss = 0.03024878\n","Iteration 446, loss = 0.03017044\n","Iteration 447, loss = 0.03009249\n","Iteration 448, loss = 0.03001494\n","Iteration 449, loss = 0.02993778\n","Iteration 450, loss = 0.02986101\n","Iteration 451, loss = 0.02978462\n","Iteration 452, loss = 0.02970861\n","Iteration 453, loss = 0.02963298\n","Iteration 454, loss = 0.02955773\n","Iteration 455, loss = 0.02948285\n","Iteration 456, loss = 0.02940834\n","Iteration 457, loss = 0.02933420\n","Iteration 458, loss = 0.02926043\n","Iteration 459, loss = 0.02918702\n","Iteration 460, loss = 0.02911397\n","Iteration 461, loss = 0.02904127\n","Iteration 462, loss = 0.02896893\n","Iteration 463, loss = 0.02889694\n","Iteration 464, loss = 0.02882531\n","Iteration 465, loss = 0.02875402\n","Iteration 466, loss = 0.02868307\n","Iteration 467, loss = 0.02861247\n","Iteration 468, loss = 0.02854221\n","Iteration 469, loss = 0.02847228\n","Iteration 470, loss = 0.02840269\n","Iteration 471, loss = 0.02833343\n","Iteration 472, loss = 0.02826451\n","Iteration 473, loss = 0.02819591\n","Iteration 474, loss = 0.02812764\n","Iteration 475, loss = 0.02805969\n","Iteration 476, loss = 0.02799206\n","Iteration 477, loss = 0.02792475\n","Iteration 478, loss = 0.02785776\n","Iteration 479, loss = 0.02779108\n","Iteration 480, loss = 0.02772471\n","Iteration 481, loss = 0.02765866\n","Iteration 482, loss = 0.02759291\n","Iteration 483, loss = 0.02752747\n","Iteration 484, loss = 0.02746233\n","Iteration 485, loss = 0.02739750\n","Iteration 486, loss = 0.02733296\n","Iteration 487, loss = 0.02726873\n","Iteration 488, loss = 0.02720478\n","Iteration 489, loss = 0.02714114\n","Iteration 490, loss = 0.02707778\n","Iteration 491, loss = 0.02701471\n","Iteration 492, loss = 0.02695193\n","Iteration 493, loss = 0.02688944\n","Iteration 494, loss = 0.02682723\n","Iteration 495, loss = 0.02676530\n","Iteration 496, loss = 0.02670366\n","Iteration 497, loss = 0.02664229\n","Iteration 498, loss = 0.02658119\n","Iteration 499, loss = 0.02652038\n","Iteration 500, loss = 0.02645983\n","Iteration 501, loss = 0.02639956\n","Iteration 502, loss = 0.02633955\n","Iteration 503, loss = 0.02627981\n","Iteration 504, loss = 0.02622034\n","Iteration 505, loss = 0.02616113\n","Iteration 506, loss = 0.02610219\n","Iteration 507, loss = 0.02604350\n","Iteration 508, loss = 0.02598507\n","Iteration 509, loss = 0.02592690\n","Iteration 510, loss = 0.02586899\n","Iteration 511, loss = 0.02581133\n","Iteration 512, loss = 0.02575392\n","Iteration 513, loss = 0.02569676\n","Iteration 514, loss = 0.02563985\n","Iteration 515, loss = 0.02558318\n","Iteration 516, loss = 0.02552677\n","Iteration 517, loss = 0.02547059\n","Iteration 518, loss = 0.02541466\n","Iteration 519, loss = 0.02535897\n","Iteration 520, loss = 0.02530352\n","Iteration 521, loss = 0.02524831\n","Iteration 522, loss = 0.02519334\n","Iteration 523, loss = 0.02513859\n","Iteration 524, loss = 0.02508409\n","Iteration 525, loss = 0.02502981\n","Iteration 526, loss = 0.02497577\n","Iteration 527, loss = 0.02492195\n","Iteration 528, loss = 0.02486836\n","Iteration 529, loss = 0.02481500\n","Iteration 530, loss = 0.02476186\n","Iteration 531, loss = 0.02470895\n","Iteration 532, loss = 0.02465625\n","Iteration 533, loss = 0.02460378\n","Iteration 534, loss = 0.02455153\n","Iteration 535, loss = 0.02449949\n","Iteration 536, loss = 0.02444768\n","Iteration 537, loss = 0.02439607\n","Iteration 538, loss = 0.02434468\n","Iteration 539, loss = 0.02429351\n","Iteration 540, loss = 0.02424254\n","Iteration 541, loss = 0.02419179\n","Iteration 542, loss = 0.02414124\n","Iteration 543, loss = 0.02409090\n","Iteration 544, loss = 0.02404077\n","Iteration 545, loss = 0.02399084\n","Iteration 546, loss = 0.02394112\n","Iteration 547, loss = 0.02389160\n","Iteration 548, loss = 0.02384227\n","Iteration 549, loss = 0.02379315\n","Iteration 550, loss = 0.02374423\n","Iteration 551, loss = 0.02369551\n","Iteration 552, loss = 0.02364698\n","Iteration 553, loss = 0.02359865\n","Iteration 554, loss = 0.02355051\n","Iteration 555, loss = 0.02350257\n","Iteration 556, loss = 0.02345481\n","Iteration 557, loss = 0.02340725\n","Iteration 558, loss = 0.02335988\n","Iteration 559, loss = 0.02331269\n","Iteration 560, loss = 0.02326569\n","Iteration 561, loss = 0.02321888\n","Iteration 562, loss = 0.02317226\n","Iteration 563, loss = 0.02312582\n","Iteration 564, loss = 0.02307956\n","Iteration 565, loss = 0.02303348\n","Iteration 566, loss = 0.02298759\n","Iteration 567, loss = 0.02294187\n","Iteration 568, loss = 0.02289633\n","Iteration 569, loss = 0.02285097\n","Iteration 570, loss = 0.02280579\n","Iteration 571, loss = 0.02276078\n","Iteration 572, loss = 0.02271595\n","Iteration 573, loss = 0.02267129\n","Iteration 574, loss = 0.02262681\n","Iteration 575, loss = 0.02258249\n","Iteration 576, loss = 0.02253835\n","Iteration 577, loss = 0.02249437\n","Iteration 578, loss = 0.02245057\n","Iteration 579, loss = 0.02240693\n","Iteration 580, loss = 0.02236346\n","Iteration 581, loss = 0.02232016\n","Iteration 582, loss = 0.02227702\n","Iteration 583, loss = 0.02223404\n","Iteration 584, loss = 0.02219123\n","Iteration 585, loss = 0.02214858\n","Iteration 586, loss = 0.02210609\n","Iteration 587, loss = 0.02206376\n","Iteration 588, loss = 0.02202159\n","Iteration 589, loss = 0.02197958\n","Iteration 590, loss = 0.02193773\n","Iteration 591, loss = 0.02189604\n","Iteration 592, loss = 0.02185450\n","Iteration 593, loss = 0.02181311\n","Iteration 594, loss = 0.02177188\n","Iteration 595, loss = 0.02173081\n","Iteration 596, loss = 0.02168988\n","Iteration 597, loss = 0.02164911\n","Iteration 598, loss = 0.02160849\n","Iteration 599, loss = 0.02156802\n","Iteration 600, loss = 0.02152770\n","Iteration 601, loss = 0.02148752\n","Iteration 602, loss = 0.02144750\n","Iteration 603, loss = 0.02140762\n","Iteration 604, loss = 0.02136789\n","Iteration 605, loss = 0.02132830\n","Iteration 606, loss = 0.02128885\n","Iteration 607, loss = 0.02124956\n","Iteration 608, loss = 0.02121040\n","Iteration 609, loss = 0.02117138\n","Iteration 610, loss = 0.02113251\n","Iteration 611, loss = 0.02109378\n","Iteration 612, loss = 0.02105519\n","Iteration 613, loss = 0.02101673\n","Iteration 614, loss = 0.02097842\n","Iteration 615, loss = 0.02094024\n","Iteration 616, loss = 0.02090220\n","Iteration 617, loss = 0.02086430\n","Iteration 618, loss = 0.02082653\n","Iteration 619, loss = 0.02078889\n","Iteration 620, loss = 0.02075139\n","Iteration 621, loss = 0.02071403\n","Iteration 622, loss = 0.02067679\n","Iteration 623, loss = 0.02063969\n","Iteration 624, loss = 0.02060272\n","Iteration 625, loss = 0.02056588\n","Iteration 626, loss = 0.02052917\n","Iteration 627, loss = 0.02049259\n","Iteration 628, loss = 0.02045613\n","Iteration 629, loss = 0.02041981\n","Iteration 630, loss = 0.02038361\n","Iteration 631, loss = 0.02034754\n","Iteration 632, loss = 0.02031159\n","Iteration 633, loss = 0.02027577\n","Iteration 634, loss = 0.02024008\n","Iteration 635, loss = 0.02020451\n","Iteration 636, loss = 0.02016906\n","Iteration 637, loss = 0.02013373\n","Iteration 638, loss = 0.02009853\n","Iteration 639, loss = 0.02006345\n","Iteration 640, loss = 0.02002849\n","Iteration 641, loss = 0.01999364\n","Iteration 642, loss = 0.01995892\n","Iteration 643, loss = 0.01992432\n","Iteration 644, loss = 0.01988983\n","Iteration 645, loss = 0.01985547\n","Iteration 646, loss = 0.01982122\n","Iteration 647, loss = 0.01978709\n","Iteration 648, loss = 0.01975307\n","Iteration 649, loss = 0.01971917\n","Iteration 650, loss = 0.01968538\n","Iteration 651, loss = 0.01965171\n","Iteration 652, loss = 0.01961815\n","Iteration 653, loss = 0.01958470\n","Iteration 654, loss = 0.01955137\n","Iteration 655, loss = 0.01951815\n","Iteration 656, loss = 0.01948504\n","Iteration 657, loss = 0.01945204\n","Iteration 658, loss = 0.01941915\n","Iteration 659, loss = 0.01938637\n","Iteration 660, loss = 0.01935370\n","Iteration 661, loss = 0.01932114\n","Iteration 662, loss = 0.01928869\n","Iteration 663, loss = 0.01925634\n","Iteration 664, loss = 0.01922411\n","Iteration 665, loss = 0.01919197\n","Iteration 666, loss = 0.01915995\n","Iteration 667, loss = 0.01912803\n","Iteration 668, loss = 0.01909621\n","Iteration 669, loss = 0.01906450\n","Iteration 670, loss = 0.01903290\n","Iteration 671, loss = 0.01900139\n","Iteration 672, loss = 0.01896999\n","Iteration 673, loss = 0.01893870\n","Iteration 674, loss = 0.01890750\n","Iteration 675, loss = 0.01887641\n","Iteration 676, loss = 0.01884541\n","Iteration 677, loss = 0.01881452\n","Iteration 678, loss = 0.01878373\n","Iteration 679, loss = 0.01875304\n","Iteration 680, loss = 0.01872244\n","Iteration 681, loss = 0.01869195\n","Iteration 682, loss = 0.01866155\n","Iteration 683, loss = 0.01863125\n","Iteration 684, loss = 0.01860105\n","Iteration 685, loss = 0.01857094\n","Iteration 686, loss = 0.01854093\n","Iteration 687, loss = 0.01851102\n","Iteration 688, loss = 0.01848120\n","Iteration 689, loss = 0.01845148\n","Iteration 690, loss = 0.01842185\n","Iteration 691, loss = 0.01839231\n","Iteration 692, loss = 0.01836287\n","Iteration 693, loss = 0.01833353\n","Iteration 694, loss = 0.01830427\n","Iteration 695, loss = 0.01827511\n","Iteration 696, loss = 0.01824604\n","Iteration 697, loss = 0.01821706\n","Iteration 698, loss = 0.01818817\n","Iteration 699, loss = 0.01815937\n","Iteration 700, loss = 0.01813066\n","Iteration 701, loss = 0.01810204\n","Iteration 702, loss = 0.01807352\n","Iteration 703, loss = 0.01804508\n","Iteration 704, loss = 0.01801672\n","Iteration 705, loss = 0.01798846\n","Iteration 706, loss = 0.01796028\n","Iteration 707, loss = 0.01793220\n","Iteration 708, loss = 0.01790419\n","Iteration 709, loss = 0.01787628\n","Iteration 710, loss = 0.01784845\n","Iteration 711, loss = 0.01782071\n","Iteration 712, loss = 0.01779305\n","Iteration 713, loss = 0.01776548\n","Iteration 714, loss = 0.01773799\n","Iteration 715, loss = 0.01771058\n","Iteration 716, loss = 0.01768326\n","Iteration 717, loss = 0.01765602\n","Iteration 718, loss = 0.01762887\n","Iteration 719, loss = 0.01760180\n","Iteration 720, loss = 0.01757481\n","Iteration 721, loss = 0.01754790\n","Iteration 722, loss = 0.01752108\n","Iteration 723, loss = 0.01749433\n","Iteration 724, loss = 0.01746767\n","Iteration 725, loss = 0.01744108\n","Iteration 726, loss = 0.01741458\n","Iteration 727, loss = 0.01738815\n","Iteration 728, loss = 0.01736181\n","Iteration 729, loss = 0.01733554\n","Iteration 730, loss = 0.01730936\n","Iteration 731, loss = 0.01728325\n","Iteration 732, loss = 0.01725722\n","Iteration 733, loss = 0.01723126\n","Iteration 734, loss = 0.01720539\n","Iteration 735, loss = 0.01717959\n","Iteration 736, loss = 0.01715387\n","Iteration 737, loss = 0.01712822\n","Iteration 738, loss = 0.01710265\n","Iteration 739, loss = 0.01707716\n","Iteration 740, loss = 0.01705174\n","Iteration 741, loss = 0.01702639\n","Iteration 742, loss = 0.01700112\n","Iteration 743, loss = 0.01697593\n","Iteration 744, loss = 0.01695080\n","Iteration 745, loss = 0.01692576\n","Iteration 746, loss = 0.01690078\n","Iteration 747, loss = 0.01687588\n","Iteration 748, loss = 0.01685105\n","Iteration 749, loss = 0.01682629\n","Iteration 750, loss = 0.01680161\n","Iteration 751, loss = 0.01677699\n","Iteration 752, loss = 0.01675245\n","Iteration 753, loss = 0.01672798\n","Iteration 754, loss = 0.01670358\n","Iteration 755, loss = 0.01667925\n","Iteration 756, loss = 0.01665499\n","Iteration 757, loss = 0.01663080\n","Iteration 758, loss = 0.01660669\n","Iteration 759, loss = 0.01658263\n","Iteration 760, loss = 0.01655865\n","Iteration 761, loss = 0.01653474\n","Iteration 762, loss = 0.01651090\n","Iteration 763, loss = 0.01648712\n","Iteration 764, loss = 0.01646341\n","Iteration 765, loss = 0.01643977\n","Iteration 766, loss = 0.01641620\n","Iteration 767, loss = 0.01639269\n","Iteration 768, loss = 0.01636925\n","Iteration 769, loss = 0.01634588\n","Iteration 770, loss = 0.01632257\n","Iteration 771, loss = 0.01629933\n","Iteration 772, loss = 0.01627615\n","Iteration 773, loss = 0.01625304\n","Iteration 774, loss = 0.01623000\n","Iteration 775, loss = 0.01620702\n","Iteration 776, loss = 0.01618410\n","Iteration 777, loss = 0.01616125\n","Iteration 778, loss = 0.01613846\n","Iteration 779, loss = 0.01611574\n","Iteration 780, loss = 0.01609308\n","Iteration 781, loss = 0.01607048\n","Iteration 782, loss = 0.01604794\n","Iteration 783, loss = 0.01602547\n","Iteration 784, loss = 0.01600306\n","Iteration 785, loss = 0.01598072\n","Iteration 786, loss = 0.01595843\n","Iteration 787, loss = 0.01593621\n","Iteration 788, loss = 0.01591404\n","Iteration 789, loss = 0.01589194\n","Iteration 790, loss = 0.01586990\n","Iteration 791, loss = 0.01584792\n","Iteration 792, loss = 0.01582600\n","Iteration 793, loss = 0.01580414\n","Iteration 794, loss = 0.01578234\n","Iteration 795, loss = 0.01576060\n","Iteration 796, loss = 0.01573892\n","Iteration 797, loss = 0.01571730\n","Iteration 798, loss = 0.01569574\n","Iteration 799, loss = 0.01567423\n","Iteration 800, loss = 0.01565279\n","Iteration 801, loss = 0.01563140\n","Iteration 802, loss = 0.01561007\n","Iteration 803, loss = 0.01558880\n","Iteration 804, loss = 0.01556759\n","Iteration 805, loss = 0.01554643\n","Iteration 806, loss = 0.01552533\n","Iteration 807, loss = 0.01550429\n","Iteration 808, loss = 0.01548330\n","Iteration 809, loss = 0.01546237\n","Iteration 810, loss = 0.01544150\n","Iteration 811, loss = 0.01542068\n","Iteration 812, loss = 0.01539992\n","Iteration 813, loss = 0.01537921\n","Iteration 814, loss = 0.01535856\n","Iteration 815, loss = 0.01533796\n","Iteration 816, loss = 0.01531742\n","Iteration 817, loss = 0.01529693\n","Iteration 818, loss = 0.01527650\n","Iteration 819, loss = 0.01525612\n","Iteration 820, loss = 0.01523580\n","Iteration 821, loss = 0.01521553\n","Iteration 822, loss = 0.01519531\n","Iteration 823, loss = 0.01517515\n","Iteration 824, loss = 0.01515504\n","Iteration 825, loss = 0.01513498\n","Iteration 826, loss = 0.01511497\n","Iteration 827, loss = 0.01509502\n","Iteration 828, loss = 0.01507512\n","Iteration 829, loss = 0.01505527\n","Iteration 830, loss = 0.01503547\n","Iteration 831, loss = 0.01501573\n","Iteration 832, loss = 0.01499604\n","Iteration 833, loss = 0.01497639\n","Iteration 834, loss = 0.01495680\n","Iteration 835, loss = 0.01493726\n","Iteration 836, loss = 0.01491777\n","Iteration 837, loss = 0.01489834\n","Iteration 838, loss = 0.01487895\n","Iteration 839, loss = 0.01485961\n","Iteration 840, loss = 0.01484032\n","Iteration 841, loss = 0.01482108\n","Iteration 842, loss = 0.01480189\n","Iteration 843, loss = 0.01478275\n","Iteration 844, loss = 0.01476366\n","Iteration 845, loss = 0.01474462\n","Iteration 846, loss = 0.01472563\n","Iteration 847, loss = 0.01470668\n","Iteration 848, loss = 0.01468779\n","Iteration 849, loss = 0.01466894\n","Iteration 850, loss = 0.01465014\n","Iteration 851, loss = 0.01463139\n","Iteration 852, loss = 0.01461269\n","Iteration 853, loss = 0.01459403\n","Iteration 854, loss = 0.01457542\n","Iteration 855, loss = 0.01455686\n","Iteration 856, loss = 0.01453834\n","Iteration 857, loss = 0.01451987\n","Iteration 858, loss = 0.01450145\n","Iteration 859, loss = 0.01448308\n","Iteration 860, loss = 0.01446475\n","Iteration 861, loss = 0.01444647\n","Iteration 862, loss = 0.01442823\n","Iteration 863, loss = 0.01441004\n","Iteration 864, loss = 0.01439189\n","Iteration 865, loss = 0.01437379\n","Iteration 866, loss = 0.01435574\n","Iteration 867, loss = 0.01433773\n","Iteration 868, loss = 0.01431976\n","Iteration 869, loss = 0.01430184\n","Iteration 870, loss = 0.01428397\n","Iteration 871, loss = 0.01426614\n","Iteration 872, loss = 0.01424835\n","Iteration 873, loss = 0.01423061\n","Iteration 874, loss = 0.01421291\n","Iteration 875, loss = 0.01419525\n","Iteration 876, loss = 0.01417764\n","Iteration 877, loss = 0.01416007\n","Iteration 878, loss = 0.01414255\n","Iteration 879, loss = 0.01412507\n","Iteration 880, loss = 0.01410763\n","Iteration 881, loss = 0.01409023\n","Iteration 882, loss = 0.01407288\n","Iteration 883, loss = 0.01405557\n","Iteration 884, loss = 0.01403830\n","Iteration 885, loss = 0.01402107\n","Iteration 886, loss = 0.01400389\n","Iteration 887, loss = 0.01398675\n","Iteration 888, loss = 0.01396965\n","Iteration 889, loss = 0.01395259\n","Iteration 890, loss = 0.01393557\n","Iteration 891, loss = 0.01391860\n","Iteration 892, loss = 0.01390166\n","Iteration 893, loss = 0.01388477\n","Iteration 894, loss = 0.01386791\n","Iteration 895, loss = 0.01385110\n","Iteration 896, loss = 0.01383433\n","Iteration 897, loss = 0.01381760\n","Iteration 898, loss = 0.01380091\n","Iteration 899, loss = 0.01378425\n","Iteration 900, loss = 0.01376764\n","Iteration 901, loss = 0.01375107\n","Iteration 902, loss = 0.01373454\n","Iteration 903, loss = 0.01371805\n","Iteration 904, loss = 0.01370160\n","Iteration 905, loss = 0.01368518\n","Iteration 906, loss = 0.01366881\n","Iteration 907, loss = 0.01365247\n","Iteration 908, loss = 0.01363618\n","Iteration 909, loss = 0.01361992\n","Iteration 910, loss = 0.01360370\n","Iteration 911, loss = 0.01358752\n","Iteration 912, loss = 0.01357138\n","Iteration 913, loss = 0.01355527\n","Iteration 914, loss = 0.01353920\n","Iteration 915, loss = 0.01352318\n","Iteration 916, loss = 0.01350719\n","Iteration 917, loss = 0.01349123\n","Iteration 918, loss = 0.01347532\n","Iteration 919, loss = 0.01345944\n","Iteration 920, loss = 0.01344360\n","Iteration 921, loss = 0.01342780\n","Iteration 922, loss = 0.01341203\n","Iteration 923, loss = 0.01339630\n","Iteration 924, loss = 0.01338061\n","Iteration 925, loss = 0.01336495\n","Iteration 926, loss = 0.01334933\n","Iteration 927, loss = 0.01333375\n","Iteration 928, loss = 0.01331820\n","Iteration 929, loss = 0.01330269\n","Iteration 930, loss = 0.01328721\n","Iteration 931, loss = 0.01327177\n","Iteration 932, loss = 0.01325637\n","Iteration 933, loss = 0.01324100\n","Iteration 934, loss = 0.01322567\n","Iteration 935, loss = 0.01321037\n","Iteration 936, loss = 0.01319511\n","Iteration 937, loss = 0.01317988\n","Iteration 938, loss = 0.01316469\n","Iteration 939, loss = 0.01314953\n","Iteration 940, loss = 0.01313441\n","Iteration 941, loss = 0.01311932\n","Iteration 942, loss = 0.01310427\n","Iteration 943, loss = 0.01308925\n","Iteration 944, loss = 0.01307426\n","Iteration 945, loss = 0.01305931\n","Iteration 946, loss = 0.01304440\n","Iteration 947, loss = 0.01302952\n","Iteration 948, loss = 0.01301467\n","Iteration 949, loss = 0.01299985\n","Iteration 950, loss = 0.01298507\n","Iteration 951, loss = 0.01297032\n","Iteration 952, loss = 0.01295561\n","Iteration 953, loss = 0.01294093\n","Iteration 954, loss = 0.01292628\n","Iteration 955, loss = 0.01291167\n","Iteration 956, loss = 0.01289708\n","Iteration 957, loss = 0.01288253\n","Iteration 958, loss = 0.01286802\n","Iteration 959, loss = 0.01285353\n","Iteration 960, loss = 0.01283908\n","Iteration 961, loss = 0.01282466\n","Iteration 962, loss = 0.01281028\n","Iteration 963, loss = 0.01279592\n","Iteration 964, loss = 0.01278160\n","Iteration 965, loss = 0.01276731\n","Iteration 966, loss = 0.01275305\n","Iteration 967, loss = 0.01273883\n","Iteration 968, loss = 0.01272463\n","Iteration 969, loss = 0.01271047\n","Iteration 970, loss = 0.01269633\n","Iteration 971, loss = 0.01268223\n","Iteration 972, loss = 0.01266816\n","Iteration 973, loss = 0.01265412\n","Iteration 974, loss = 0.01264012\n","Iteration 975, loss = 0.01262614\n","Iteration 976, loss = 0.01261219\n","Iteration 977, loss = 0.01259828\n","Iteration 978, loss = 0.01258439\n","Iteration 979, loss = 0.01257054\n","Iteration 980, loss = 0.01255672\n","Iteration 981, loss = 0.01254292\n","Iteration 982, loss = 0.01252916\n","Iteration 983, loss = 0.01251543\n","Iteration 984, loss = 0.01250172\n","Iteration 985, loss = 0.01248805\n","Iteration 986, loss = 0.01247441\n","Iteration 987, loss = 0.01246080\n","Iteration 988, loss = 0.01244721\n","Iteration 989, loss = 0.01243366\n","Iteration 990, loss = 0.01242013\n","Iteration 991, loss = 0.01240664\n","Iteration 992, loss = 0.01239317\n","Iteration 993, loss = 0.01237973\n","Iteration 994, loss = 0.01236633\n","Iteration 995, loss = 0.01235295\n","Iteration 996, loss = 0.01233960\n","Iteration 997, loss = 0.01232627\n","Iteration 998, loss = 0.01231298\n","Iteration 999, loss = 0.01229972\n","Iteration 1000, loss = 0.01228648\n","Iteration 1001, loss = 0.01227327\n","Iteration 1002, loss = 0.01226010\n","Iteration 1003, loss = 0.01224695\n","Iteration 1004, loss = 0.01223382\n","Iteration 1005, loss = 0.01222073\n","Iteration 1006, loss = 0.01220766\n","Iteration 1007, loss = 0.01219462\n","Iteration 1008, loss = 0.01218161\n","Iteration 1009, loss = 0.01216863\n","Iteration 1010, loss = 0.01215567\n","Iteration 1011, loss = 0.01214275\n","Iteration 1012, loss = 0.01212985\n","Iteration 1013, loss = 0.01211697\n","Iteration 1014, loss = 0.01210413\n","Iteration 1015, loss = 0.01209131\n","Iteration 1016, loss = 0.01207852\n","Iteration 1017, loss = 0.01206575\n","Iteration 1018, loss = 0.01205301\n","Iteration 1019, loss = 0.01204030\n","Iteration 1020, loss = 0.01202762\n","Iteration 1021, loss = 0.01201496\n","Iteration 1022, loss = 0.01200233\n","Iteration 1023, loss = 0.01198973\n","Iteration 1024, loss = 0.01197715\n","Iteration 1025, loss = 0.01196460\n","Iteration 1026, loss = 0.01195207\n","Iteration 1027, loss = 0.01193957\n","Iteration 1028, loss = 0.01192710\n","Iteration 1029, loss = 0.01191465\n","Iteration 1030, loss = 0.01190223\n","Iteration 1031, loss = 0.01188984\n","Iteration 1032, loss = 0.01187747\n","Iteration 1033, loss = 0.01186512\n","Iteration 1034, loss = 0.01185281\n","Iteration 1035, loss = 0.01184051\n","Iteration 1036, loss = 0.01182825\n","Iteration 1037, loss = 0.01181600\n","Iteration 1038, loss = 0.01180379\n","Iteration 1039, loss = 0.01179160\n","Iteration 1040, loss = 0.01177943\n","Iteration 1041, loss = 0.01176729\n","Iteration 1042, loss = 0.01175517\n","Iteration 1043, loss = 0.01174308\n","Iteration 1044, loss = 0.01173102\n","Iteration 1045, loss = 0.01171897\n","Iteration 1046, loss = 0.01170696\n","Iteration 1047, loss = 0.01169497\n","Iteration 1048, loss = 0.01168300\n","Iteration 1049, loss = 0.01167106\n","Iteration 1050, loss = 0.01165914\n","Iteration 1051, loss = 0.01164724\n","Iteration 1052, loss = 0.01163537\n","Iteration 1053, loss = 0.01162353\n","Iteration 1054, loss = 0.01161170\n","Iteration 1055, loss = 0.01159991\n","Iteration 1056, loss = 0.01158813\n","Iteration 1057, loss = 0.01157638\n","Iteration 1058, loss = 0.01156466\n","Iteration 1059, loss = 0.01155295\n","Iteration 1060, loss = 0.01154127\n","Iteration 1061, loss = 0.01152962\n","Iteration 1062, loss = 0.01151799\n","Iteration 1063, loss = 0.01150638\n","Iteration 1064, loss = 0.01149479\n","Iteration 1065, loss = 0.01148323\n","Iteration 1066, loss = 0.01147169\n","Iteration 1067, loss = 0.01146018\n","Iteration 1068, loss = 0.01144869\n","Iteration 1069, loss = 0.01143722\n","Iteration 1070, loss = 0.01142577\n","Iteration 1071, loss = 0.01141435\n","Iteration 1072, loss = 0.01140295\n","Iteration 1073, loss = 0.01139157\n","Iteration 1074, loss = 0.01138022\n","Iteration 1075, loss = 0.01136888\n","Iteration 1076, loss = 0.01135757\n","Iteration 1077, loss = 0.01134629\n","Iteration 1078, loss = 0.01133502\n","Iteration 1079, loss = 0.01132378\n","Iteration 1080, loss = 0.01131256\n","Iteration 1081, loss = 0.01130136\n","Iteration 1082, loss = 0.01129019\n","Iteration 1083, loss = 0.01127903\n","Iteration 1084, loss = 0.01126790\n","Iteration 1085, loss = 0.01125679\n","Iteration 1086, loss = 0.01124570\n","Iteration 1087, loss = 0.01123464\n","Iteration 1088, loss = 0.01122359\n","Iteration 1089, loss = 0.01121257\n","Iteration 1090, loss = 0.01120157\n","Iteration 1091, loss = 0.01119059\n","Iteration 1092, loss = 0.01117963\n","Iteration 1093, loss = 0.01116870\n","Iteration 1094, loss = 0.01115778\n","Iteration 1095, loss = 0.01114689\n","Iteration 1096, loss = 0.01113602\n","Iteration 1097, loss = 0.01112516\n","Iteration 1098, loss = 0.01111433\n","Iteration 1099, loss = 0.01110353\n","Iteration 1100, loss = 0.01109274\n","Iteration 1101, loss = 0.01108197\n","Iteration 1102, loss = 0.01107123\n","Iteration 1103, loss = 0.01106050\n","Iteration 1104, loss = 0.01104980\n","Iteration 1105, loss = 0.01103911\n","Iteration 1106, loss = 0.01102845\n","Iteration 1107, loss = 0.01101781\n","Iteration 1108, loss = 0.01100719\n","Iteration 1109, loss = 0.01099658\n","Iteration 1110, loss = 0.01098600\n","Iteration 1111, loss = 0.01097544\n","Iteration 1112, loss = 0.01096490\n","Iteration 1113, loss = 0.01095438\n","Iteration 1114, loss = 0.01094388\n","Iteration 1115, loss = 0.01093340\n","Iteration 1116, loss = 0.01092294\n","Iteration 1117, loss = 0.01091250\n","Iteration 1118, loss = 0.01090209\n","Iteration 1119, loss = 0.01089169\n","Iteration 1120, loss = 0.01088131\n","Iteration 1121, loss = 0.01087095\n","Iteration 1122, loss = 0.01086061\n","Iteration 1123, loss = 0.01085029\n","Iteration 1124, loss = 0.01083998\n","Iteration 1125, loss = 0.01082970\n","Iteration 1126, loss = 0.01081944\n","Iteration 1127, loss = 0.01080920\n","Iteration 1128, loss = 0.01079898\n","Iteration 1129, loss = 0.01078877\n","Iteration 1130, loss = 0.01077859\n","Iteration 1131, loss = 0.01076842\n","Iteration 1132, loss = 0.01075828\n","Iteration 1133, loss = 0.01074815\n","Iteration 1134, loss = 0.01073804\n","Iteration 1135, loss = 0.01072795\n","Iteration 1136, loss = 0.01071789\n","Iteration 1137, loss = 0.01070783\n","Iteration 1138, loss = 0.01069780\n","Iteration 1139, loss = 0.01068779\n","Iteration 1140, loss = 0.01067780\n","Iteration 1141, loss = 0.01066782\n","Iteration 1142, loss = 0.01065786\n","Iteration 1143, loss = 0.01064793\n","Iteration 1144, loss = 0.01063801\n","Iteration 1145, loss = 0.01062810\n","Iteration 1146, loss = 0.01061822\n","Iteration 1147, loss = 0.01060836\n","Iteration 1148, loss = 0.01059851\n","Iteration 1149, loss = 0.01058868\n","Iteration 1150, loss = 0.01057887\n","Iteration 1151, loss = 0.01056908\n","Iteration 1152, loss = 0.01055931\n","Iteration 1153, loss = 0.01054955\n","Iteration 1154, loss = 0.01053982\n","Iteration 1155, loss = 0.01053010\n","Iteration 1156, loss = 0.01052040\n","Iteration 1157, loss = 0.01051072\n","Iteration 1158, loss = 0.01050105\n","Iteration 1159, loss = 0.01049140\n","Iteration 1160, loss = 0.01048177\n","Iteration 1161, loss = 0.01047216\n","Iteration 1162, loss = 0.01046257\n","Iteration 1163, loss = 0.01045299\n","Iteration 1164, loss = 0.01044343\n","Iteration 1165, loss = 0.01043389\n","Iteration 1166, loss = 0.01042437\n","Iteration 1167, loss = 0.01041486\n","Iteration 1168, loss = 0.01040537\n","Iteration 1169, loss = 0.01039590\n","Iteration 1170, loss = 0.01038644\n","Iteration 1171, loss = 0.01037700\n","Iteration 1172, loss = 0.01036758\n","Iteration 1173, loss = 0.01035818\n","Iteration 1174, loss = 0.01034880\n","Iteration 1175, loss = 0.01033943\n","Iteration 1176, loss = 0.01033007\n","Iteration 1177, loss = 0.01032074\n","Iteration 1178, loss = 0.01031142\n","Iteration 1179, loss = 0.01030212\n","Iteration 1180, loss = 0.01029284\n","Iteration 1181, loss = 0.01028357\n","Iteration 1182, loss = 0.01027432\n","Iteration 1183, loss = 0.01026508\n","Iteration 1184, loss = 0.01025586\n","Iteration 1185, loss = 0.01024666\n","Iteration 1186, loss = 0.01023748\n","Iteration 1187, loss = 0.01022831\n","Iteration 1188, loss = 0.01021916\n","Iteration 1189, loss = 0.01021002\n","Iteration 1190, loss = 0.01020090\n","Iteration 1191, loss = 0.01019180\n","Iteration 1192, loss = 0.01018272\n","Iteration 1193, loss = 0.01017365\n","Iteration 1194, loss = 0.01016459\n","Iteration 1195, loss = 0.01015555\n","Iteration 1196, loss = 0.01014653\n","Iteration 1197, loss = 0.01013753\n","Iteration 1198, loss = 0.01012854\n","Iteration 1199, loss = 0.01011957\n","Iteration 1200, loss = 0.01011061\n","Iteration 1201, loss = 0.01010167\n","Iteration 1202, loss = 0.01009274\n","Iteration 1203, loss = 0.01008383\n","Iteration 1204, loss = 0.01007494\n","Iteration 1205, loss = 0.01006606\n","Iteration 1206, loss = 0.01005720\n","Iteration 1207, loss = 0.01004835\n","Iteration 1208, loss = 0.01003952\n","Iteration 1209, loss = 0.01003070\n","Iteration 1210, loss = 0.01002190\n","Iteration 1211, loss = 0.01001312\n","Iteration 1212, loss = 0.01000435\n","Iteration 1213, loss = 0.00999559\n","Iteration 1214, loss = 0.00998685\n","Iteration 1215, loss = 0.00997813\n","Iteration 1216, loss = 0.00996942\n","Iteration 1217, loss = 0.00996073\n","Iteration 1218, loss = 0.00995205\n","Iteration 1219, loss = 0.00994339\n","Iteration 1220, loss = 0.00993474\n","Iteration 1221, loss = 0.00992611\n","Iteration 1222, loss = 0.00991749\n","Iteration 1223, loss = 0.00990889\n","Iteration 1224, loss = 0.00990030\n","Iteration 1225, loss = 0.00989173\n","Iteration 1226, loss = 0.00988317\n","Iteration 1227, loss = 0.00987463\n","Iteration 1228, loss = 0.00986610\n","Iteration 1229, loss = 0.00985759\n","Iteration 1230, loss = 0.00984909\n","Iteration 1231, loss = 0.00984061\n","Iteration 1232, loss = 0.00983214\n","Iteration 1233, loss = 0.00982369\n","Iteration 1234, loss = 0.00981525\n","Iteration 1235, loss = 0.00980682\n","Iteration 1236, loss = 0.00979841\n","Iteration 1237, loss = 0.00979001\n","Iteration 1238, loss = 0.00978163\n","Iteration 1239, loss = 0.00977326\n","Iteration 1240, loss = 0.00976491\n","Iteration 1241, loss = 0.00975657\n","Iteration 1242, loss = 0.00974825\n","Iteration 1243, loss = 0.00973994\n","Iteration 1244, loss = 0.00973164\n","Iteration 1245, loss = 0.00972336\n","Iteration 1246, loss = 0.00971509\n","Iteration 1247, loss = 0.00970684\n","Iteration 1248, loss = 0.00969860\n","Iteration 1249, loss = 0.00969038\n","Iteration 1250, loss = 0.00968217\n","Iteration 1251, loss = 0.00967397\n","Iteration 1252, loss = 0.00966578\n","Iteration 1253, loss = 0.00965762\n","Iteration 1254, loss = 0.00964946\n","Iteration 1255, loss = 0.00964132\n","Iteration 1256, loss = 0.00963319\n","Iteration 1257, loss = 0.00962508\n","Iteration 1258, loss = 0.00961698\n","Iteration 1259, loss = 0.00960889\n","Iteration 1260, loss = 0.00960082\n","Iteration 1261, loss = 0.00959276\n","Iteration 1262, loss = 0.00958471\n","Iteration 1263, loss = 0.00957668\n","Iteration 1264, loss = 0.00956866\n","Iteration 1265, loss = 0.00956065\n","Iteration 1266, loss = 0.00955266\n","Iteration 1267, loss = 0.00954468\n","Iteration 1268, loss = 0.00953672\n","Iteration 1269, loss = 0.00952877\n","Iteration 1270, loss = 0.00952083\n","Iteration 1271, loss = 0.00951290\n","Iteration 1272, loss = 0.00950499\n","Iteration 1273, loss = 0.00949709\n","Iteration 1274, loss = 0.00948921\n","Iteration 1275, loss = 0.00948133\n","Iteration 1276, loss = 0.00947347\n","Iteration 1277, loss = 0.00946563\n","Iteration 1278, loss = 0.00945780\n","Iteration 1279, loss = 0.00944998\n","Iteration 1280, loss = 0.00944217\n","Iteration 1281, loss = 0.00943437\n","Iteration 1282, loss = 0.00942659\n","Iteration 1283, loss = 0.00941882\n","Iteration 1284, loss = 0.00941107\n","Iteration 1285, loss = 0.00940333\n","Iteration 1286, loss = 0.00939560\n","Iteration 1287, loss = 0.00938788\n","Iteration 1288, loss = 0.00938018\n","Iteration 1289, loss = 0.00937248\n","Iteration 1290, loss = 0.00936480\n","Iteration 1291, loss = 0.00935714\n","Iteration 1292, loss = 0.00934948\n","Iteration 1293, loss = 0.00934184\n","Iteration 1294, loss = 0.00933422\n","Iteration 1295, loss = 0.00932660\n","Iteration 1296, loss = 0.00931900\n","Iteration 1297, loss = 0.00931140\n","Iteration 1298, loss = 0.00930383\n","Iteration 1299, loss = 0.00929626\n","Iteration 1300, loss = 0.00928871\n","Iteration 1301, loss = 0.00928116\n","Iteration 1302, loss = 0.00927363\n","Iteration 1303, loss = 0.00926612\n","Iteration 1304, loss = 0.00925861\n","Iteration 1305, loss = 0.00925112\n","Iteration 1306, loss = 0.00924364\n","Iteration 1307, loss = 0.00923617\n","Iteration 1308, loss = 0.00922872\n","Iteration 1309, loss = 0.00922127\n","Iteration 1310, loss = 0.00921384\n","Iteration 1311, loss = 0.00920642\n","Iteration 1312, loss = 0.00919901\n","Iteration 1313, loss = 0.00919162\n","Iteration 1314, loss = 0.00918423\n","Iteration 1315, loss = 0.00917686\n","Iteration 1316, loss = 0.00916950\n","Iteration 1317, loss = 0.00916215\n","Iteration 1318, loss = 0.00915482\n","Iteration 1319, loss = 0.00914749\n","Iteration 1320, loss = 0.00914018\n","Iteration 1321, loss = 0.00913288\n","Iteration 1322, loss = 0.00912559\n","Iteration 1323, loss = 0.00911831\n","Iteration 1324, loss = 0.00911105\n","Iteration 1325, loss = 0.00910379\n","Iteration 1326, loss = 0.00909655\n","Iteration 1327, loss = 0.00908932\n","Iteration 1328, loss = 0.00908210\n","Iteration 1329, loss = 0.00907489\n","Iteration 1330, loss = 0.00906769\n","Iteration 1331, loss = 0.00906051\n","Iteration 1332, loss = 0.00905333\n","Iteration 1333, loss = 0.00904617\n","Iteration 1334, loss = 0.00903902\n","Iteration 1335, loss = 0.00903188\n","Iteration 1336, loss = 0.00902475\n","Iteration 1337, loss = 0.00901764\n","Iteration 1338, loss = 0.00901053\n","Iteration 1339, loss = 0.00900344\n","Iteration 1340, loss = 0.00899636\n","Iteration 1341, loss = 0.00898928\n","Iteration 1342, loss = 0.00898222\n","Iteration 1343, loss = 0.00897517\n","Iteration 1344, loss = 0.00896814\n","Iteration 1345, loss = 0.00896111\n","Iteration 1346, loss = 0.00895409\n","Iteration 1347, loss = 0.00894709\n","Iteration 1348, loss = 0.00894009\n","Iteration 1349, loss = 0.00893311\n","Iteration 1350, loss = 0.00892614\n","Iteration 1351, loss = 0.00891918\n","Iteration 1352, loss = 0.00891223\n","Iteration 1353, loss = 0.00890529\n","Iteration 1354, loss = 0.00889836\n","Iteration 1355, loss = 0.00889144\n","Iteration 1356, loss = 0.00888454\n","Iteration 1357, loss = 0.00887764\n","Iteration 1358, loss = 0.00887075\n","Iteration 1359, loss = 0.00886388\n","Iteration 1360, loss = 0.00885702\n","Iteration 1361, loss = 0.00885016\n","Iteration 1362, loss = 0.00884332\n","Iteration 1363, loss = 0.00883649\n","Iteration 1364, loss = 0.00882967\n","Iteration 1365, loss = 0.00882286\n","Iteration 1366, loss = 0.00881606\n","Iteration 1367, loss = 0.00880927\n","Iteration 1368, loss = 0.00880249\n","Iteration 1369, loss = 0.00879572\n","Iteration 1370, loss = 0.00878896\n","Iteration 1371, loss = 0.00878222\n","Iteration 1372, loss = 0.00877548\n","Iteration 1373, loss = 0.00876875\n","Iteration 1374, loss = 0.00876204\n","Iteration 1375, loss = 0.00875533\n","Iteration 1376, loss = 0.00874864\n","Iteration 1377, loss = 0.00874195\n","Iteration 1378, loss = 0.00873528\n","Iteration 1379, loss = 0.00872861\n","Iteration 1380, loss = 0.00872196\n","Iteration 1381, loss = 0.00871531\n","Iteration 1382, loss = 0.00870868\n","Iteration 1383, loss = 0.00870206\n","Iteration 1384, loss = 0.00869544\n","Iteration 1385, loss = 0.00868884\n","Iteration 1386, loss = 0.00868225\n","Iteration 1387, loss = 0.00867566\n","Iteration 1388, loss = 0.00866909\n","Iteration 1389, loss = 0.00866253\n","Iteration 1390, loss = 0.00865597\n","Iteration 1391, loss = 0.00864943\n","Iteration 1392, loss = 0.00864290\n","Iteration 1393, loss = 0.00863637\n","Iteration 1394, loss = 0.00862986\n","Iteration 1395, loss = 0.00862336\n","Iteration 1396, loss = 0.00861686\n","Iteration 1397, loss = 0.00861038\n","Iteration 1398, loss = 0.00860390\n","Iteration 1399, loss = 0.00859744\n","Iteration 1400, loss = 0.00859099\n","Iteration 1401, loss = 0.00858454\n","Iteration 1402, loss = 0.00857811\n","Iteration 1403, loss = 0.00857168\n","Iteration 1404, loss = 0.00856527\n","Iteration 1405, loss = 0.00855886\n","Iteration 1406, loss = 0.00855246\n","Iteration 1407, loss = 0.00854608\n","Iteration 1408, loss = 0.00853970\n","Iteration 1409, loss = 0.00853333\n","Iteration 1410, loss = 0.00852697\n","Iteration 1411, loss = 0.00852063\n","Iteration 1412, loss = 0.00851429\n","Iteration 1413, loss = 0.00850796\n","Iteration 1414, loss = 0.00850164\n","Iteration 1415, loss = 0.00849533\n","Iteration 1416, loss = 0.00848903\n","Iteration 1417, loss = 0.00848274\n","Iteration 1418, loss = 0.00847645\n","Iteration 1419, loss = 0.00847018\n","Iteration 1420, loss = 0.00846392\n","Iteration 1421, loss = 0.00845766\n","Iteration 1422, loss = 0.00845142\n","Iteration 1423, loss = 0.00844518\n","Iteration 1424, loss = 0.00843896\n","Iteration 1425, loss = 0.00843274\n","Iteration 1426, loss = 0.00842653\n","Iteration 1427, loss = 0.00842034\n","Iteration 1428, loss = 0.00841415\n","Iteration 1429, loss = 0.00840797\n","Iteration 1430, loss = 0.00840180\n","Iteration 1431, loss = 0.00839563\n","Iteration 1432, loss = 0.00838948\n","Iteration 1433, loss = 0.00838334\n","Iteration 1434, loss = 0.00837720\n","Iteration 1435, loss = 0.00837108\n","Iteration 1436, loss = 0.00836496\n","Iteration 1437, loss = 0.00835885\n","Iteration 1438, loss = 0.00835276\n","Iteration 1439, loss = 0.00834667\n","Iteration 1440, loss = 0.00834059\n","Iteration 1441, loss = 0.00833451\n","Iteration 1442, loss = 0.00832845\n","Iteration 1443, loss = 0.00832240\n","Iteration 1444, loss = 0.00831635\n","Iteration 1445, loss = 0.00831032\n","Iteration 1446, loss = 0.00830429\n","Iteration 1447, loss = 0.00829827\n","Iteration 1448, loss = 0.00829226\n","Iteration 1449, loss = 0.00828626\n","Iteration 1450, loss = 0.00828027\n","Iteration 1451, loss = 0.00827429\n","Iteration 1452, loss = 0.00826831\n","Iteration 1453, loss = 0.00826235\n","Iteration 1454, loss = 0.00825639\n","Iteration 1455, loss = 0.00825044\n","Iteration 1456, loss = 0.00824450\n","Iteration 1457, loss = 0.00823857\n","Iteration 1458, loss = 0.00823265\n","Iteration 1459, loss = 0.00822673\n","Iteration 1460, loss = 0.00822083\n","Iteration 1461, loss = 0.00821493\n","Iteration 1462, loss = 0.00820904\n","Iteration 1463, loss = 0.00820316\n","Iteration 1464, loss = 0.00819729\n","Iteration 1465, loss = 0.00819143\n","Iteration 1466, loss = 0.00818557\n","Iteration 1467, loss = 0.00817973\n","Iteration 1468, loss = 0.00817389\n","Iteration 1469, loss = 0.00816806\n","Iteration 1470, loss = 0.00816224\n","Iteration 1471, loss = 0.00815643\n","Iteration 1472, loss = 0.00815062\n","Iteration 1473, loss = 0.00814483\n","Iteration 1474, loss = 0.00813904\n","Iteration 1475, loss = 0.00813326\n","Iteration 1476, loss = 0.00812749\n","Iteration 1477, loss = 0.00812173\n","Iteration 1478, loss = 0.00811597\n","Iteration 1479, loss = 0.00811023\n","Iteration 1480, loss = 0.00810449\n","Iteration 1481, loss = 0.00809876\n","Iteration 1482, loss = 0.00809304\n","Iteration 1483, loss = 0.00808732\n","Iteration 1484, loss = 0.00808162\n","Iteration 1485, loss = 0.00807592\n","Iteration 1486, loss = 0.00807023\n","Iteration 1487, loss = 0.00806455\n","Iteration 1488, loss = 0.00805888\n","Iteration 1489, loss = 0.00805321\n","Iteration 1490, loss = 0.00804756\n","Iteration 1491, loss = 0.00804191\n","Iteration 1492, loss = 0.00803627\n","Iteration 1493, loss = 0.00803063\n","Iteration 1494, loss = 0.00802501\n","Iteration 1495, loss = 0.00801939\n","Iteration 1496, loss = 0.00801378\n","Iteration 1497, loss = 0.00800818\n","Iteration 1498, loss = 0.00800259\n","Iteration 1499, loss = 0.00799700\n","Iteration 1500, loss = 0.00799143\n","Iteration 1501, loss = 0.00798586\n","Iteration 1502, loss = 0.00798030\n","Iteration 1503, loss = 0.00797474\n","Iteration 1504, loss = 0.00796920\n","Iteration 1505, loss = 0.00796366\n","Iteration 1506, loss = 0.00795813\n","Iteration 1507, loss = 0.00795260\n","Iteration 1508, loss = 0.00794709\n","Iteration 1509, loss = 0.00794158\n","Iteration 1510, loss = 0.00793608\n","Iteration 1511, loss = 0.00793059\n","Iteration 1512, loss = 0.00792511\n","Iteration 1513, loss = 0.00791963\n","Iteration 1514, loss = 0.00791416\n","Iteration 1515, loss = 0.00790870\n","Iteration 1516, loss = 0.00790325\n","Iteration 1517, loss = 0.00789780\n","Iteration 1518, loss = 0.00789236\n","Iteration 1519, loss = 0.00788693\n","Iteration 1520, loss = 0.00788151\n","Iteration 1521, loss = 0.00787609\n","Iteration 1522, loss = 0.00787068\n","Iteration 1523, loss = 0.00786528\n","Iteration 1524, loss = 0.00785989\n","Iteration 1525, loss = 0.00785450\n","Iteration 1526, loss = 0.00784912\n","Iteration 1527, loss = 0.00784375\n","Iteration 1528, loss = 0.00783839\n","Iteration 1529, loss = 0.00783303\n","Iteration 1530, loss = 0.00782768\n","Iteration 1531, loss = 0.00782234\n","Iteration 1532, loss = 0.00781701\n","Iteration 1533, loss = 0.00781168\n","Iteration 1534, loss = 0.00780636\n","Iteration 1535, loss = 0.00780105\n","Iteration 1536, loss = 0.00779575\n","Iteration 1537, loss = 0.00779045\n","Iteration 1538, loss = 0.00778516\n","Iteration 1539, loss = 0.00777988\n","Iteration 1540, loss = 0.00777460\n","Iteration 1541, loss = 0.00776933\n","Iteration 1542, loss = 0.00776407\n","Iteration 1543, loss = 0.00775882\n","Iteration 1544, loss = 0.00775357\n","Iteration 1545, loss = 0.00774833\n","Iteration 1546, loss = 0.00774310\n","Iteration 1547, loss = 0.00773787\n","Iteration 1548, loss = 0.00773265\n","Iteration 1549, loss = 0.00772744\n","Iteration 1550, loss = 0.00772224\n","Iteration 1551, loss = 0.00771704\n","Iteration 1552, loss = 0.00771185\n","Iteration 1553, loss = 0.00770667\n","Iteration 1554, loss = 0.00770149\n","Iteration 1555, loss = 0.00769633\n","Iteration 1556, loss = 0.00769116\n","Iteration 1557, loss = 0.00768601\n","Iteration 1558, loss = 0.00768086\n","Iteration 1559, loss = 0.00767572\n","Iteration 1560, loss = 0.00767059\n","Iteration 1561, loss = 0.00766546\n","Iteration 1562, loss = 0.00766034\n","Iteration 1563, loss = 0.00765523\n","Iteration 1564, loss = 0.00765012\n","Iteration 1565, loss = 0.00764502\n","Iteration 1566, loss = 0.00763993\n","Iteration 1567, loss = 0.00763484\n","Iteration 1568, loss = 0.00762976\n","Iteration 1569, loss = 0.00762469\n","Iteration 1570, loss = 0.00761963\n","Iteration 1571, loss = 0.00761457\n","Iteration 1572, loss = 0.00760952\n","Iteration 1573, loss = 0.00760447\n","Iteration 1574, loss = 0.00759944\n","Iteration 1575, loss = 0.00759440\n","Iteration 1576, loss = 0.00758938\n","Iteration 1577, loss = 0.00758436\n","Iteration 1578, loss = 0.00757935\n","Iteration 1579, loss = 0.00757435\n","Iteration 1580, loss = 0.00756935\n","Iteration 1581, loss = 0.00756436\n","Iteration 1582, loss = 0.00755937\n","Iteration 1583, loss = 0.00755440\n","Iteration 1584, loss = 0.00754943\n","Iteration 1585, loss = 0.00754446\n","Iteration 1586, loss = 0.00753950\n","Iteration 1587, loss = 0.00753455\n","Iteration 1588, loss = 0.00752961\n","Iteration 1589, loss = 0.00752467\n","Iteration 1590, loss = 0.00751974\n","Iteration 1591, loss = 0.00751481\n","Iteration 1592, loss = 0.00750989\n","Iteration 1593, loss = 0.00750498\n","Iteration 1594, loss = 0.00750008\n","Iteration 1595, loss = 0.00749518\n","Iteration 1596, loss = 0.00749029\n","Iteration 1597, loss = 0.00748540\n","Iteration 1598, loss = 0.00748052\n","Iteration 1599, loss = 0.00747565\n","Iteration 1600, loss = 0.00747078\n","Iteration 1601, loss = 0.00746592\n","Iteration 1602, loss = 0.00746107\n","Iteration 1603, loss = 0.00745622\n","Iteration 1604, loss = 0.00745138\n","Iteration 1605, loss = 0.00744654\n","Iteration 1606, loss = 0.00744171\n","Iteration 1607, loss = 0.00743689\n","Iteration 1608, loss = 0.00743208\n","Iteration 1609, loss = 0.00742727\n","Iteration 1610, loss = 0.00742246\n","Iteration 1611, loss = 0.00741767\n","Iteration 1612, loss = 0.00741288\n","Iteration 1613, loss = 0.00740809\n","Iteration 1614, loss = 0.00740331\n","Iteration 1615, loss = 0.00739854\n","Iteration 1616, loss = 0.00739378\n","Iteration 1617, loss = 0.00738902\n","Iteration 1618, loss = 0.00738426\n","Iteration 1619, loss = 0.00737952\n","Iteration 1620, loss = 0.00737477\n","Iteration 1621, loss = 0.00737004\n","Iteration 1622, loss = 0.00736531\n","Iteration 1623, loss = 0.00736059\n","Iteration 1624, loss = 0.00735587\n","Iteration 1625, loss = 0.00735116\n","Iteration 1626, loss = 0.00734646\n","Iteration 1627, loss = 0.00734176\n","Iteration 1628, loss = 0.00733707\n","Iteration 1629, loss = 0.00733238\n","Iteration 1630, loss = 0.00732770\n","Iteration 1631, loss = 0.00732303\n","Iteration 1632, loss = 0.00731836\n","Iteration 1633, loss = 0.00731370\n","Iteration 1634, loss = 0.00730904\n","Iteration 1635, loss = 0.00730439\n","Iteration 1636, loss = 0.00729975\n","Iteration 1637, loss = 0.00729511\n","Iteration 1638, loss = 0.00729048\n","Iteration 1639, loss = 0.00728586\n","Iteration 1640, loss = 0.00728124\n","Iteration 1641, loss = 0.00727662\n","Iteration 1642, loss = 0.00727201\n","Iteration 1643, loss = 0.00726741\n","Iteration 1644, loss = 0.00726282\n","Iteration 1645, loss = 0.00725823\n","Iteration 1646, loss = 0.00725364\n","Iteration 1647, loss = 0.00724906\n","Iteration 1648, loss = 0.00724449\n","Iteration 1649, loss = 0.00723992\n","Iteration 1650, loss = 0.00723536\n","Iteration 1651, loss = 0.00723081\n","Iteration 1652, loss = 0.00722626\n","Iteration 1653, loss = 0.00722171\n","Iteration 1654, loss = 0.00721718\n","Iteration 1655, loss = 0.00721264\n","Iteration 1656, loss = 0.00720812\n","Iteration 1657, loss = 0.00720360\n","Iteration 1658, loss = 0.00719908\n","Iteration 1659, loss = 0.00719457\n","Iteration 1660, loss = 0.00719007\n","Iteration 1661, loss = 0.00718557\n","Iteration 1662, loss = 0.00718108\n","Iteration 1663, loss = 0.00717659\n","Iteration 1664, loss = 0.00717211\n","Iteration 1665, loss = 0.00716764\n","Iteration 1666, loss = 0.00716317\n","Iteration 1667, loss = 0.00715871\n","Iteration 1668, loss = 0.00715425\n","Iteration 1669, loss = 0.00714980\n","Iteration 1670, loss = 0.00714535\n","Iteration 1671, loss = 0.00714091\n","Iteration 1672, loss = 0.00713647\n","Iteration 1673, loss = 0.00713204\n","Iteration 1674, loss = 0.00712762\n","Iteration 1675, loss = 0.00712320\n","Iteration 1676, loss = 0.00711879\n","Iteration 1677, loss = 0.00711438\n","Iteration 1678, loss = 0.00710998\n","Iteration 1679, loss = 0.00710558\n","Iteration 1680, loss = 0.00710119\n","Iteration 1681, loss = 0.00709680\n","Iteration 1682, loss = 0.00709242\n","Iteration 1683, loss = 0.00708805\n","Iteration 1684, loss = 0.00708368\n","Iteration 1685, loss = 0.00707932\n","Iteration 1686, loss = 0.00707496\n","Iteration 1687, loss = 0.00707060\n","Iteration 1688, loss = 0.00706626\n","Iteration 1689, loss = 0.00706191\n","Iteration 1690, loss = 0.00705758\n","Iteration 1691, loss = 0.00705325\n","Iteration 1692, loss = 0.00704892\n","Iteration 1693, loss = 0.00704460\n","Iteration 1694, loss = 0.00704029\n","Iteration 1695, loss = 0.00703598\n","Iteration 1696, loss = 0.00703167\n","Iteration 1697, loss = 0.00702737\n","Iteration 1698, loss = 0.00702308\n","Iteration 1699, loss = 0.00701879\n","Iteration 1700, loss = 0.00701451\n","Iteration 1701, loss = 0.00701023\n","Iteration 1702, loss = 0.00700596\n","Iteration 1703, loss = 0.00700169\n","Iteration 1704, loss = 0.00699743\n","Iteration 1705, loss = 0.00699317\n","Iteration 1706, loss = 0.00698892\n","Iteration 1707, loss = 0.00698467\n","Iteration 1708, loss = 0.00698043\n","Iteration 1709, loss = 0.00697620\n","Iteration 1710, loss = 0.00697197\n","Iteration 1711, loss = 0.00696774\n","Iteration 1712, loss = 0.00696352\n","Iteration 1713, loss = 0.00695931\n","Iteration 1714, loss = 0.00695510\n","Iteration 1715, loss = 0.00695089\n","Iteration 1716, loss = 0.00694669\n","Iteration 1717, loss = 0.00694250\n","Iteration 1718, loss = 0.00693831\n","Iteration 1719, loss = 0.00693412\n","Iteration 1720, loss = 0.00692995\n","Iteration 1721, loss = 0.00692577\n","Iteration 1722, loss = 0.00692160\n","Iteration 1723, loss = 0.00691744\n","Iteration 1724, loss = 0.00691328\n","Iteration 1725, loss = 0.00690913\n","Iteration 1726, loss = 0.00690498\n","Iteration 1727, loss = 0.00690084\n","Iteration 1728, loss = 0.00689670\n","Iteration 1729, loss = 0.00689256\n","Iteration 1730, loss = 0.00688844\n","Iteration 1731, loss = 0.00688431\n","Iteration 1732, loss = 0.00688019\n","Iteration 1733, loss = 0.00687608\n","Iteration 1734, loss = 0.00687197\n","Iteration 1735, loss = 0.00686787\n","Iteration 1736, loss = 0.00686377\n","Iteration 1737, loss = 0.00685968\n","Iteration 1738, loss = 0.00685559\n","Iteration 1739, loss = 0.00685151\n","Iteration 1740, loss = 0.00684743\n","Iteration 1741, loss = 0.00684335\n","Iteration 1742, loss = 0.00683929\n","Iteration 1743, loss = 0.00683522\n","Iteration 1744, loss = 0.00683116\n","Iteration 1745, loss = 0.00682711\n","Iteration 1746, loss = 0.00682306\n","Iteration 1747, loss = 0.00681902\n","Iteration 1748, loss = 0.00681498\n","Iteration 1749, loss = 0.00681094\n","Iteration 1750, loss = 0.00680691\n","Iteration 1751, loss = 0.00680289\n","Iteration 1752, loss = 0.00679887\n","Iteration 1753, loss = 0.00679485\n","Iteration 1754, loss = 0.00679084\n","Iteration 1755, loss = 0.00678684\n","Iteration 1756, loss = 0.00678284\n","Iteration 1757, loss = 0.00677884\n","Iteration 1758, loss = 0.00677485\n","Iteration 1759, loss = 0.00677086\n","Iteration 1760, loss = 0.00676688\n","Iteration 1761, loss = 0.00676290\n","Iteration 1762, loss = 0.00675893\n","Iteration 1763, loss = 0.00675496\n","Iteration 1764, loss = 0.00675100\n","Iteration 1765, loss = 0.00674704\n","Iteration 1766, loss = 0.00674309\n","Iteration 1767, loss = 0.00673914\n","Iteration 1768, loss = 0.00673520\n","Iteration 1769, loss = 0.00673126\n","Iteration 1770, loss = 0.00672732\n","Iteration 1771, loss = 0.00672339\n","Iteration 1772, loss = 0.00671947\n","Iteration 1773, loss = 0.00671555\n","Iteration 1774, loss = 0.00671163\n","Iteration 1775, loss = 0.00670772\n","Iteration 1776, loss = 0.00670382\n","Iteration 1777, loss = 0.00669991\n","Iteration 1778, loss = 0.00669602\n","Iteration 1779, loss = 0.00669212\n","Iteration 1780, loss = 0.00668824\n","Iteration 1781, loss = 0.00668435\n","Iteration 1782, loss = 0.00668047\n","Iteration 1783, loss = 0.00667660\n","Iteration 1784, loss = 0.00667273\n","Iteration 1785, loss = 0.00666886\n","Iteration 1786, loss = 0.00666500\n","Iteration 1787, loss = 0.00666115\n","Iteration 1788, loss = 0.00665729\n","Iteration 1789, loss = 0.00665345\n","Iteration 1790, loss = 0.00664960\n","Iteration 1791, loss = 0.00664577\n","Iteration 1792, loss = 0.00664193\n","Iteration 1793, loss = 0.00663810\n","Iteration 1794, loss = 0.00663428\n","Iteration 1795, loss = 0.00663046\n","Iteration 1796, loss = 0.00662664\n","Iteration 1797, loss = 0.00662283\n","Iteration 1798, loss = 0.00661902\n","Iteration 1799, loss = 0.00661522\n","Iteration 1800, loss = 0.00661142\n","Iteration 1801, loss = 0.00660763\n","Iteration 1802, loss = 0.00660384\n","Iteration 1803, loss = 0.00660006\n","Iteration 1804, loss = 0.00659628\n","Iteration 1805, loss = 0.00659250\n","Iteration 1806, loss = 0.00658873\n","Iteration 1807, loss = 0.00658496\n","Iteration 1808, loss = 0.00658120\n","Iteration 1809, loss = 0.00657744\n","Iteration 1810, loss = 0.00657369\n","Iteration 1811, loss = 0.00656994\n","Iteration 1812, loss = 0.00656619\n","Iteration 1813, loss = 0.00656245\n","Iteration 1814, loss = 0.00655872\n","Iteration 1815, loss = 0.00655498\n","Iteration 1816, loss = 0.00655126\n","Iteration 1817, loss = 0.00654753\n","Iteration 1818, loss = 0.00654381\n","Iteration 1819, loss = 0.00654010\n","Iteration 1820, loss = 0.00653639\n","Iteration 1821, loss = 0.00653268\n","Iteration 1822, loss = 0.00652898\n","Iteration 1823, loss = 0.00652528\n","Iteration 1824, loss = 0.00652159\n","Iteration 1825, loss = 0.00651790\n","Iteration 1826, loss = 0.00651421\n","Iteration 1827, loss = 0.00651053\n","Iteration 1828, loss = 0.00650685\n","Iteration 1829, loss = 0.00650318\n","Iteration 1830, loss = 0.00649951\n","Iteration 1831, loss = 0.00649585\n","Iteration 1832, loss = 0.00649219\n","Iteration 1833, loss = 0.00648853\n","Iteration 1834, loss = 0.00648488\n","Iteration 1835, loss = 0.00648123\n","Iteration 1836, loss = 0.00647759\n","Iteration 1837, loss = 0.00647395\n","Iteration 1838, loss = 0.00647032\n","Iteration 1839, loss = 0.00646669\n","Iteration 1840, loss = 0.00646306\n","Iteration 1841, loss = 0.00645944\n","Iteration 1842, loss = 0.00645582\n","Iteration 1843, loss = 0.00645221\n","Iteration 1844, loss = 0.00644859\n","Iteration 1845, loss = 0.00644499\n","Iteration 1846, loss = 0.00644139\n","Iteration 1847, loss = 0.00643779\n","Iteration 1848, loss = 0.00643420\n","Iteration 1849, loss = 0.00643061\n","Iteration 1850, loss = 0.00642702\n","Iteration 1851, loss = 0.00642344\n","Iteration 1852, loss = 0.00641986\n","Iteration 1853, loss = 0.00641629\n","Iteration 1854, loss = 0.00641272\n","Iteration 1855, loss = 0.00640915\n","Iteration 1856, loss = 0.00640559\n","Iteration 1857, loss = 0.00640204\n","Iteration 1858, loss = 0.00639848\n","Iteration 1859, loss = 0.00639493\n","Iteration 1860, loss = 0.00639139\n","Iteration 1861, loss = 0.00638785\n","Iteration 1862, loss = 0.00638431\n","Iteration 1863, loss = 0.00638078\n","Iteration 1864, loss = 0.00637725\n","Iteration 1865, loss = 0.00637372\n","Iteration 1866, loss = 0.00637020\n","Iteration 1867, loss = 0.00636668\n","Iteration 1868, loss = 0.00636317\n","Iteration 1869, loss = 0.00635966\n","Iteration 1870, loss = 0.00635615\n","Iteration 1871, loss = 0.00635265\n","Iteration 1872, loss = 0.00634916\n","Iteration 1873, loss = 0.00634566\n","Iteration 1874, loss = 0.00634217\n","Iteration 1875, loss = 0.00633869\n","Iteration 1876, loss = 0.00633520\n","Iteration 1877, loss = 0.00633173\n","Iteration 1878, loss = 0.00632825\n","Iteration 1879, loss = 0.00632478\n","Iteration 1880, loss = 0.00632132\n","Iteration 1881, loss = 0.00631785\n","Iteration 1882, loss = 0.00631439\n","Iteration 1883, loss = 0.00631094\n","Iteration 1884, loss = 0.00630749\n","Iteration 1885, loss = 0.00630404\n","Iteration 1886, loss = 0.00630060\n","Iteration 1887, loss = 0.00629716\n","Iteration 1888, loss = 0.00629372\n","Iteration 1889, loss = 0.00629029\n","Iteration 1890, loss = 0.00628686\n","Iteration 1891, loss = 0.00628344\n","Iteration 1892, loss = 0.00628002\n","Iteration 1893, loss = 0.00627660\n","Iteration 1894, loss = 0.00627319\n","Iteration 1895, loss = 0.00626978\n","Iteration 1896, loss = 0.00626637\n","Iteration 1897, loss = 0.00626297\n","Iteration 1898, loss = 0.00625957\n","Iteration 1899, loss = 0.00625618\n","Iteration 1900, loss = 0.00625279\n","Iteration 1901, loss = 0.00624940\n","Iteration 1902, loss = 0.00624602\n","Iteration 1903, loss = 0.00624264\n","Iteration 1904, loss = 0.00623926\n","Iteration 1905, loss = 0.00623589\n","Iteration 1906, loss = 0.00623253\n","Iteration 1907, loss = 0.00622916\n","Iteration 1908, loss = 0.00622580\n","Iteration 1909, loss = 0.00622244\n","Iteration 1910, loss = 0.00621909\n","Iteration 1911, loss = 0.00621574\n","Iteration 1912, loss = 0.00621239\n","Iteration 1913, loss = 0.00620905\n","Iteration 1914, loss = 0.00620571\n","Iteration 1915, loss = 0.00620238\n","Iteration 1916, loss = 0.00619905\n","Iteration 1917, loss = 0.00619572\n","Iteration 1918, loss = 0.00619240\n","Iteration 1919, loss = 0.00618908\n","Iteration 1920, loss = 0.00618576\n","Iteration 1921, loss = 0.00618245\n","Iteration 1922, loss = 0.00617914\n","Iteration 1923, loss = 0.00617583\n","Iteration 1924, loss = 0.00617253\n","Iteration 1925, loss = 0.00616923\n","Iteration 1926, loss = 0.00616593\n","Iteration 1927, loss = 0.00616264\n","Iteration 1928, loss = 0.00615936\n","Iteration 1929, loss = 0.00615607\n","Iteration 1930, loss = 0.00615279\n","Iteration 1931, loss = 0.00614951\n","Iteration 1932, loss = 0.00614624\n","Iteration 1933, loss = 0.00614297\n","Iteration 1934, loss = 0.00613970\n","Iteration 1935, loss = 0.00613644\n","Iteration 1936, loss = 0.00613318\n","Iteration 1937, loss = 0.00612992\n","Iteration 1938, loss = 0.00612667\n","Iteration 1939, loss = 0.00612342\n","Iteration 1940, loss = 0.00612018\n","Iteration 1941, loss = 0.00611694\n","Iteration 1942, loss = 0.00611370\n","Iteration 1943, loss = 0.00611046\n","Iteration 1944, loss = 0.00610723\n","Iteration 1945, loss = 0.00610400\n","Iteration 1946, loss = 0.00610078\n","Iteration 1947, loss = 0.00609756\n","Iteration 1948, loss = 0.00609434\n","Iteration 1949, loss = 0.00609113\n","Iteration 1950, loss = 0.00608792\n","Iteration 1951, loss = 0.00608471\n","Iteration 1952, loss = 0.00608150\n","Iteration 1953, loss = 0.00607830\n","Iteration 1954, loss = 0.00607511\n","Iteration 1955, loss = 0.00607191\n","Iteration 1956, loss = 0.00606872\n","Iteration 1957, loss = 0.00606554\n","Iteration 1958, loss = 0.00606235\n","Iteration 1959, loss = 0.00605917\n","Iteration 1960, loss = 0.00605600\n","Iteration 1961, loss = 0.00605282\n","Iteration 1962, loss = 0.00604966\n","Iteration 1963, loss = 0.00604649\n","Iteration 1964, loss = 0.00604333\n","Iteration 1965, loss = 0.00604017\n","Iteration 1966, loss = 0.00603701\n","Iteration 1967, loss = 0.00603386\n","Iteration 1968, loss = 0.00603071\n","Iteration 1969, loss = 0.00602756\n","Iteration 1970, loss = 0.00602442\n","Iteration 1971, loss = 0.00602128\n","Iteration 1972, loss = 0.00601814\n","Iteration 1973, loss = 0.00601501\n","Iteration 1974, loss = 0.00601188\n","Iteration 1975, loss = 0.00600876\n","Iteration 1976, loss = 0.00600563\n","Iteration 1977, loss = 0.00600251\n","Iteration 1978, loss = 0.00599940\n","Iteration 1979, loss = 0.00599629\n","Iteration 1980, loss = 0.00599318\n","Iteration 1981, loss = 0.00599007\n","Iteration 1982, loss = 0.00598697\n","Iteration 1983, loss = 0.00598387\n","Iteration 1984, loss = 0.00598077\n","Iteration 1985, loss = 0.00597768\n","Iteration 1986, loss = 0.00597459\n","Iteration 1987, loss = 0.00597150\n","Iteration 1988, loss = 0.00596842\n","Iteration 1989, loss = 0.00596534\n","Iteration 1990, loss = 0.00596226\n","Iteration 1991, loss = 0.00595919\n","Iteration 1992, loss = 0.00595612\n","Iteration 1993, loss = 0.00595305\n","Iteration 1994, loss = 0.00594999\n","Iteration 1995, loss = 0.00594692\n","Iteration 1996, loss = 0.00594387\n","Iteration 1997, loss = 0.00594081\n","Iteration 1998, loss = 0.00593776\n","Iteration 1999, loss = 0.00593471\n","Iteration 2000, loss = 0.00593167\n","Iteration 2001, loss = 0.00592863\n","Iteration 2002, loss = 0.00592559\n","Iteration 2003, loss = 0.00592255\n","Iteration 2004, loss = 0.00591952\n","Iteration 2005, loss = 0.00591649\n","Iteration 2006, loss = 0.00591347\n","Iteration 2007, loss = 0.00591044\n","Iteration 2008, loss = 0.00590743\n","Iteration 2009, loss = 0.00590441\n","Iteration 2010, loss = 0.00590140\n","Iteration 2011, loss = 0.00589839\n","Iteration 2012, loss = 0.00589538\n","Iteration 2013, loss = 0.00589238\n","Iteration 2014, loss = 0.00588937\n","Iteration 2015, loss = 0.00588638\n","Iteration 2016, loss = 0.00588338\n","Iteration 2017, loss = 0.00588039\n","Iteration 2018, loss = 0.00587740\n","Iteration 2019, loss = 0.00587442\n","Iteration 2020, loss = 0.00587144\n","Iteration 2021, loss = 0.00586846\n","Iteration 2022, loss = 0.00586548\n","Iteration 2023, loss = 0.00586251\n","Iteration 2024, loss = 0.00585954\n","Iteration 2025, loss = 0.00585657\n","Iteration 2026, loss = 0.00585361\n","Iteration 2027, loss = 0.00585065\n","Iteration 2028, loss = 0.00584769\n","Iteration 2029, loss = 0.00584474\n","Iteration 2030, loss = 0.00584179\n","Iteration 2031, loss = 0.00583884\n","Iteration 2032, loss = 0.00583589\n","Iteration 2033, loss = 0.00583295\n","Iteration 2034, loss = 0.00583001\n","Iteration 2035, loss = 0.00582708\n","Iteration 2036, loss = 0.00582414\n","Iteration 2037, loss = 0.00582121\n","Iteration 2038, loss = 0.00581829\n","Iteration 2039, loss = 0.00581536\n","Iteration 2040, loss = 0.00581244\n","Iteration 2041, loss = 0.00580952\n","Iteration 2042, loss = 0.00580661\n","Iteration 2043, loss = 0.00580370\n","Iteration 2044, loss = 0.00580079\n","Iteration 2045, loss = 0.00579788\n","Iteration 2046, loss = 0.00579498\n","Iteration 2047, loss = 0.00579208\n","Iteration 2048, loss = 0.00578918\n","Iteration 2049, loss = 0.00578629\n","Iteration 2050, loss = 0.00578339\n","Iteration 2051, loss = 0.00578051\n","Iteration 2052, loss = 0.00577762\n","Iteration 2053, loss = 0.00577474\n","Iteration 2054, loss = 0.00577186\n","Iteration 2055, loss = 0.00576898\n","Iteration 2056, loss = 0.00576611\n","Iteration 2057, loss = 0.00576324\n","Iteration 2058, loss = 0.00576037\n","Iteration 2059, loss = 0.00575750\n","Iteration 2060, loss = 0.00575464\n","Iteration 2061, loss = 0.00575178\n","Iteration 2062, loss = 0.00574893\n","Iteration 2063, loss = 0.00574607\n","Iteration 2064, loss = 0.00574322\n","Iteration 2065, loss = 0.00574038\n","Iteration 2066, loss = 0.00573753\n","Iteration 2067, loss = 0.00573469\n","Iteration 2068, loss = 0.00573185\n","Iteration 2069, loss = 0.00572901\n","Iteration 2070, loss = 0.00572618\n","Iteration 2071, loss = 0.00572335\n","Iteration 2072, loss = 0.00572052\n","Iteration 2073, loss = 0.00571770\n","Iteration 2074, loss = 0.00571488\n","Iteration 2075, loss = 0.00571206\n","Iteration 2076, loss = 0.00570924\n","Iteration 2077, loss = 0.00570643\n","Iteration 2078, loss = 0.00570362\n","Iteration 2079, loss = 0.00570081\n","Iteration 2080, loss = 0.00569801\n","Iteration 2081, loss = 0.00569520\n","Iteration 2082, loss = 0.00569241\n","Iteration 2083, loss = 0.00568961\n","Iteration 2084, loss = 0.00568682\n","Iteration 2085, loss = 0.00568403\n","Iteration 2086, loss = 0.00568124\n","Iteration 2087, loss = 0.00567845\n","Iteration 2088, loss = 0.00567567\n","Iteration 2089, loss = 0.00567289\n","Iteration 2090, loss = 0.00567011\n","Iteration 2091, loss = 0.00566734\n","Iteration 2092, loss = 0.00566457\n","Iteration 2093, loss = 0.00566180\n","Iteration 2094, loss = 0.00565903\n","Iteration 2095, loss = 0.00565627\n","Iteration 2096, loss = 0.00565351\n","Iteration 2097, loss = 0.00565075\n","Iteration 2098, loss = 0.00564800\n","Iteration 2099, loss = 0.00564525\n","Iteration 2100, loss = 0.00564250\n","Iteration 2101, loss = 0.00563975\n","Iteration 2102, loss = 0.00563701\n","Iteration 2103, loss = 0.00563427\n","Iteration 2104, loss = 0.00563153\n","Iteration 2105, loss = 0.00562879\n","Iteration 2106, loss = 0.00562606\n","Iteration 2107, loss = 0.00562333\n","Iteration 2108, loss = 0.00562060\n","Iteration 2109, loss = 0.00561788\n","Iteration 2110, loss = 0.00561516\n","Iteration 2111, loss = 0.00561244\n","Iteration 2112, loss = 0.00560972\n","Iteration 2113, loss = 0.00560701\n","Iteration 2114, loss = 0.00560430\n","Iteration 2115, loss = 0.00560159\n","Iteration 2116, loss = 0.00559888\n","Iteration 2117, loss = 0.00559618\n","Iteration 2118, loss = 0.00559348\n","Iteration 2119, loss = 0.00559078\n","Iteration 2120, loss = 0.00558809\n","Iteration 2121, loss = 0.00558539\n","Iteration 2122, loss = 0.00558270\n","Iteration 2123, loss = 0.00558002\n","Iteration 2124, loss = 0.00557733\n","Iteration 2125, loss = 0.00557465\n","Iteration 2126, loss = 0.00557197\n","Iteration 2127, loss = 0.00556929\n","Iteration 2128, loss = 0.00556662\n","Iteration 2129, loss = 0.00556395\n","Iteration 2130, loss = 0.00556128\n","Iteration 2131, loss = 0.00555861\n","Iteration 2132, loss = 0.00555595\n","Iteration 2133, loss = 0.00555329\n","Iteration 2134, loss = 0.00555063\n","Iteration 2135, loss = 0.00554798\n","Iteration 2136, loss = 0.00554532\n","Iteration 2137, loss = 0.00554267\n","Iteration 2138, loss = 0.00554002\n","Iteration 2139, loss = 0.00553738\n","Iteration 2140, loss = 0.00553474\n","Iteration 2141, loss = 0.00553210\n","Iteration 2142, loss = 0.00552946\n","Iteration 2143, loss = 0.00552682\n","Iteration 2144, loss = 0.00552419\n","Iteration 2145, loss = 0.00552156\n","Iteration 2146, loss = 0.00551893\n","Iteration 2147, loss = 0.00551631\n","Iteration 2148, loss = 0.00551369\n","Iteration 2149, loss = 0.00551107\n","Iteration 2150, loss = 0.00550845\n","Iteration 2151, loss = 0.00550583\n","Iteration 2152, loss = 0.00550322\n","Iteration 2153, loss = 0.00550061\n","Iteration 2154, loss = 0.00549801\n","Iteration 2155, loss = 0.00549540\n","Iteration 2156, loss = 0.00549280\n","Iteration 2157, loss = 0.00549020\n","Iteration 2158, loss = 0.00548760\n","Iteration 2159, loss = 0.00548501\n","Iteration 2160, loss = 0.00548242\n","Iteration 2161, loss = 0.00547983\n","Iteration 2162, loss = 0.00547724\n","Iteration 2163, loss = 0.00547466\n","Iteration 2164, loss = 0.00547207\n","Iteration 2165, loss = 0.00546950\n","Iteration 2166, loss = 0.00546692\n","Iteration 2167, loss = 0.00546434\n","Iteration 2168, loss = 0.00546177\n","Iteration 2169, loss = 0.00545920\n","Iteration 2170, loss = 0.00545664\n","Iteration 2171, loss = 0.00545407\n","Iteration 2172, loss = 0.00545151\n","Iteration 2173, loss = 0.00544895\n","Iteration 2174, loss = 0.00544639\n","Iteration 2175, loss = 0.00544384\n","Iteration 2176, loss = 0.00544129\n","Iteration 2177, loss = 0.00543874\n","Iteration 2178, loss = 0.00543619\n","Iteration 2179, loss = 0.00543364\n","Iteration 2180, loss = 0.00543110\n","Iteration 2181, loss = 0.00542856\n","Iteration 2182, loss = 0.00542602\n","Iteration 2183, loss = 0.00542349\n","Iteration 2184, loss = 0.00542096\n","Iteration 2185, loss = 0.00541843\n","Iteration 2186, loss = 0.00541590\n","Iteration 2187, loss = 0.00541337\n","Iteration 2188, loss = 0.00541085\n","Iteration 2189, loss = 0.00540833\n","Iteration 2190, loss = 0.00540581\n","Iteration 2191, loss = 0.00540329\n","Iteration 2192, loss = 0.00540078\n","Iteration 2193, loss = 0.00539827\n","Iteration 2194, loss = 0.00539576\n","Iteration 2195, loss = 0.00539326\n","Iteration 2196, loss = 0.00539075\n","Iteration 2197, loss = 0.00538825\n","Iteration 2198, loss = 0.00538575\n","Iteration 2199, loss = 0.00538325\n","Iteration 2200, loss = 0.00538076\n","Iteration 2201, loss = 0.00537827\n","Iteration 2202, loss = 0.00537578\n","Iteration 2203, loss = 0.00537329\n","Iteration 2204, loss = 0.00537081\n","Iteration 2205, loss = 0.00536832\n","Iteration 2206, loss = 0.00536584\n","Iteration 2207, loss = 0.00536337\n","Iteration 2208, loss = 0.00536089\n","Iteration 2209, loss = 0.00535842\n","Iteration 2210, loss = 0.00535595\n","Iteration 2211, loss = 0.00535348\n","Iteration 2212, loss = 0.00535101\n","Iteration 2213, loss = 0.00534855\n","Iteration 2214, loss = 0.00534609\n","Iteration 2215, loss = 0.00534363\n","Iteration 2216, loss = 0.00534117\n","Iteration 2217, loss = 0.00533872\n","Iteration 2218, loss = 0.00533626\n","Iteration 2219, loss = 0.00533381\n","Iteration 2220, loss = 0.00533137\n","Iteration 2221, loss = 0.00532892\n","Iteration 2222, loss = 0.00532648\n","Iteration 2223, loss = 0.00532404\n","Iteration 2224, loss = 0.00532160\n","Iteration 2225, loss = 0.00531916\n","Iteration 2226, loss = 0.00531673\n","Iteration 2227, loss = 0.00531430\n","Iteration 2228, loss = 0.00531187\n","Iteration 2229, loss = 0.00530944\n","Iteration 2230, loss = 0.00530702\n","Iteration 2231, loss = 0.00530459\n","Iteration 2232, loss = 0.00530217\n","Iteration 2233, loss = 0.00529975\n","Iteration 2234, loss = 0.00529734\n","Iteration 2235, loss = 0.00529493\n","Iteration 2236, loss = 0.00529251\n","Iteration 2237, loss = 0.00529010\n","Iteration 2238, loss = 0.00528770\n","Iteration 2239, loss = 0.00528529\n","Iteration 2240, loss = 0.00528289\n","Iteration 2241, loss = 0.00528049\n","Iteration 2242, loss = 0.00527809\n","Iteration 2243, loss = 0.00527570\n","Iteration 2244, loss = 0.00527330\n","Iteration 2245, loss = 0.00527091\n","Iteration 2246, loss = 0.00526853\n","Iteration 2247, loss = 0.00526614\n","Iteration 2248, loss = 0.00526375\n","Iteration 2249, loss = 0.00526137\n","Iteration 2250, loss = 0.00525899\n","Iteration 2251, loss = 0.00525661\n","Iteration 2252, loss = 0.00525424\n","Iteration 2253, loss = 0.00525187\n","Iteration 2254, loss = 0.00524949\n","Iteration 2255, loss = 0.00524712\n","Iteration 2256, loss = 0.00524476\n","Iteration 2257, loss = 0.00524239\n","Iteration 2258, loss = 0.00524003\n","Iteration 2259, loss = 0.00523767\n","Iteration 2260, loss = 0.00523531\n","Iteration 2261, loss = 0.00523296\n","Iteration 2262, loss = 0.00523060\n","Iteration 2263, loss = 0.00522825\n","Iteration 2264, loss = 0.00522590\n","Iteration 2265, loss = 0.00522356\n","Iteration 2266, loss = 0.00522121\n","Iteration 2267, loss = 0.00521887\n","Iteration 2268, loss = 0.00521653\n","Iteration 2269, loss = 0.00521419\n","Iteration 2270, loss = 0.00521185\n","Iteration 2271, loss = 0.00520952\n","Iteration 2272, loss = 0.00520719\n","Iteration 2273, loss = 0.00520486\n","Iteration 2274, loss = 0.00520253\n","Iteration 2275, loss = 0.00520020\n","Iteration 2276, loss = 0.00519788\n","Iteration 2277, loss = 0.00519556\n","Iteration 2278, loss = 0.00519324\n","Iteration 2279, loss = 0.00519092\n","Iteration 2280, loss = 0.00518860\n","Iteration 2281, loss = 0.00518629\n","Iteration 2282, loss = 0.00518398\n","Iteration 2283, loss = 0.00518167\n","Iteration 2284, loss = 0.00517937\n","Iteration 2285, loss = 0.00517706\n","Iteration 2286, loss = 0.00517476\n","Iteration 2287, loss = 0.00517246\n","Iteration 2288, loss = 0.00517016\n","Iteration 2289, loss = 0.00516786\n","Iteration 2290, loss = 0.00516557\n","Iteration 2291, loss = 0.00516328\n","Iteration 2292, loss = 0.00516099\n","Iteration 2293, loss = 0.00515870\n","Iteration 2294, loss = 0.00515641\n","Iteration 2295, loss = 0.00515413\n","Iteration 2296, loss = 0.00515185\n","Iteration 2297, loss = 0.00514957\n","Iteration 2298, loss = 0.00514729\n","Iteration 2299, loss = 0.00514502\n","Iteration 2300, loss = 0.00514274\n","Iteration 2301, loss = 0.00514047\n","Iteration 2302, loss = 0.00513820\n","Iteration 2303, loss = 0.00513594\n","Iteration 2304, loss = 0.00513367\n","Iteration 2305, loss = 0.00513141\n","Iteration 2306, loss = 0.00512915\n","Iteration 2307, loss = 0.00512689\n","Iteration 2308, loss = 0.00512463\n","Iteration 2309, loss = 0.00512238\n","Iteration 2310, loss = 0.00512012\n","Iteration 2311, loss = 0.00511787\n","Iteration 2312, loss = 0.00511562\n","Iteration 2313, loss = 0.00511338\n","Iteration 2314, loss = 0.00511113\n","Iteration 2315, loss = 0.00510889\n","Iteration 2316, loss = 0.00510665\n","Iteration 2317, loss = 0.00510441\n","Iteration 2318, loss = 0.00510218\n","Iteration 2319, loss = 0.00509994\n","Iteration 2320, loss = 0.00509771\n","Iteration 2321, loss = 0.00509548\n","Iteration 2322, loss = 0.00509325\n","Iteration 2323, loss = 0.00509102\n","Iteration 2324, loss = 0.00508880\n","Iteration 2325, loss = 0.00508658\n","Iteration 2326, loss = 0.00508435\n","Iteration 2327, loss = 0.00508214\n","Iteration 2328, loss = 0.00507992\n","Iteration 2329, loss = 0.00507770\n","Iteration 2330, loss = 0.00507549\n","Iteration 2331, loss = 0.00507328\n","Iteration 2332, loss = 0.00507107\n","Iteration 2333, loss = 0.00506887\n","Iteration 2334, loss = 0.00506666\n","Iteration 2335, loss = 0.00506446\n","Iteration 2336, loss = 0.00506226\n","Iteration 2337, loss = 0.00506006\n","Iteration 2338, loss = 0.00505786\n","Iteration 2339, loss = 0.00505567\n","Iteration 2340, loss = 0.00505347\n","Iteration 2341, loss = 0.00505128\n","Iteration 2342, loss = 0.00504909\n","Iteration 2343, loss = 0.00504691\n","Iteration 2344, loss = 0.00504472\n","Iteration 2345, loss = 0.00504254\n","Iteration 2346, loss = 0.00504036\n","Iteration 2347, loss = 0.00503818\n","Iteration 2348, loss = 0.00503600\n","Iteration 2349, loss = 0.00503383\n","Iteration 2350, loss = 0.00503165\n","Iteration 2351, loss = 0.00502948\n","Iteration 2352, loss = 0.00502731\n","Iteration 2353, loss = 0.00502514\n","Iteration 2354, loss = 0.00502298\n","Iteration 2355, loss = 0.00502081\n","Iteration 2356, loss = 0.00501865\n","Iteration 2357, loss = 0.00501649\n","Iteration 2358, loss = 0.00501433\n","Iteration 2359, loss = 0.00501218\n","Iteration 2360, loss = 0.00501002\n","Iteration 2361, loss = 0.00500787\n","Iteration 2362, loss = 0.00500572\n","Iteration 2363, loss = 0.00500357\n","Iteration 2364, loss = 0.00500142\n","Iteration 2365, loss = 0.00499928\n","Iteration 2366, loss = 0.00499714\n","Iteration 2367, loss = 0.00499499\n","Iteration 2368, loss = 0.00499286\n","Iteration 2369, loss = 0.00499072\n","Iteration 2370, loss = 0.00498858\n","Iteration 2371, loss = 0.00498645\n","Iteration 2372, loss = 0.00498432\n","Iteration 2373, loss = 0.00498219\n","Iteration 2374, loss = 0.00498006\n","Iteration 2375, loss = 0.00497793\n","Iteration 2376, loss = 0.00497581\n","Iteration 2377, loss = 0.00497369\n","Iteration 2378, loss = 0.00497157\n","Iteration 2379, loss = 0.00496945\n","Iteration 2380, loss = 0.00496733\n","Iteration 2381, loss = 0.00496522\n","Iteration 2382, loss = 0.00496310\n","Iteration 2383, loss = 0.00496099\n","Iteration 2384, loss = 0.00495888\n","Iteration 2385, loss = 0.00495678\n","Iteration 2386, loss = 0.00495467\n","Iteration 2387, loss = 0.00495257\n","Iteration 2388, loss = 0.00495046\n","Iteration 2389, loss = 0.00494836\n","Iteration 2390, loss = 0.00494626\n","Iteration 2391, loss = 0.00494417\n","Iteration 2392, loss = 0.00494207\n","Iteration 2393, loss = 0.00493998\n","Iteration 2394, loss = 0.00493789\n","Iteration 2395, loss = 0.00493580\n","Iteration 2396, loss = 0.00493371\n","Iteration 2397, loss = 0.00493163\n","Iteration 2398, loss = 0.00492954\n","Iteration 2399, loss = 0.00492746\n","Iteration 2400, loss = 0.00492538\n","Iteration 2401, loss = 0.00492330\n","Iteration 2402, loss = 0.00492123\n","Iteration 2403, loss = 0.00491915\n","Iteration 2404, loss = 0.00491708\n","Iteration 2405, loss = 0.00491501\n","Iteration 2406, loss = 0.00491294\n","Iteration 2407, loss = 0.00491087\n","Iteration 2408, loss = 0.00490880\n","Iteration 2409, loss = 0.00490674\n","Iteration 2410, loss = 0.00490468\n","Iteration 2411, loss = 0.00490262\n","Iteration 2412, loss = 0.00490056\n","Iteration 2413, loss = 0.00489850\n","Iteration 2414, loss = 0.00489645\n","Iteration 2415, loss = 0.00489439\n","Iteration 2416, loss = 0.00489234\n","Iteration 2417, loss = 0.00489029\n","Iteration 2418, loss = 0.00488824\n","Iteration 2419, loss = 0.00488620\n","Iteration 2420, loss = 0.00488415\n","Iteration 2421, loss = 0.00488211\n","Iteration 2422, loss = 0.00488007\n","Iteration 2423, loss = 0.00487803\n","Iteration 2424, loss = 0.00487599\n","Iteration 2425, loss = 0.00487396\n","Iteration 2426, loss = 0.00487192\n","Iteration 2427, loss = 0.00486989\n","Iteration 2428, loss = 0.00486786\n","Iteration 2429, loss = 0.00486583\n","Iteration 2430, loss = 0.00486381\n","Iteration 2431, loss = 0.00486178\n","Iteration 2432, loss = 0.00485976\n","Iteration 2433, loss = 0.00485773\n","Iteration 2434, loss = 0.00485571\n","Iteration 2435, loss = 0.00485370\n","Iteration 2436, loss = 0.00485168\n","Iteration 2437, loss = 0.00484966\n","Iteration 2438, loss = 0.00484765\n","Iteration 2439, loss = 0.00484564\n","Iteration 2440, loss = 0.00484363\n","Iteration 2441, loss = 0.00484162\n","Iteration 2442, loss = 0.00483962\n","Iteration 2443, loss = 0.00483761\n","Iteration 2444, loss = 0.00483561\n","Iteration 2445, loss = 0.00483361\n","Iteration 2446, loss = 0.00483161\n","Iteration 2447, loss = 0.00482961\n","Iteration 2448, loss = 0.00482761\n","Iteration 2449, loss = 0.00482562\n","Iteration 2450, loss = 0.00482363\n","Iteration 2451, loss = 0.00482164\n","Iteration 2452, loss = 0.00481965\n","Iteration 2453, loss = 0.00481766\n","Iteration 2454, loss = 0.00481567\n","Iteration 2455, loss = 0.00481369\n","Iteration 2456, loss = 0.00481171\n","Iteration 2457, loss = 0.00480973\n","Iteration 2458, loss = 0.00480775\n","Iteration 2459, loss = 0.00480577\n","Iteration 2460, loss = 0.00480379\n","Iteration 2461, loss = 0.00480182\n","Iteration 2462, loss = 0.00479985\n","Iteration 2463, loss = 0.00479788\n","Iteration 2464, loss = 0.00479591\n","Iteration 2465, loss = 0.00479394\n","Iteration 2466, loss = 0.00479197\n","Iteration 2467, loss = 0.00479001\n","Iteration 2468, loss = 0.00478805\n","Iteration 2469, loss = 0.00478609\n","Iteration 2470, loss = 0.00478413\n","Iteration 2471, loss = 0.00478217\n","Iteration 2472, loss = 0.00478021\n","Iteration 2473, loss = 0.00477826\n","Iteration 2474, loss = 0.00477631\n","Iteration 2475, loss = 0.00477435\n","Iteration 2476, loss = 0.00477241\n","Iteration 2477, loss = 0.00477046\n","Iteration 2478, loss = 0.00476851\n","Iteration 2479, loss = 0.00476657\n","Iteration 2480, loss = 0.00476462\n","Iteration 2481, loss = 0.00476268\n","Iteration 2482, loss = 0.00476074\n","Iteration 2483, loss = 0.00475881\n","Iteration 2484, loss = 0.00475687\n","Iteration 2485, loss = 0.00475493\n","Iteration 2486, loss = 0.00475300\n","Iteration 2487, loss = 0.00475107\n","Iteration 2488, loss = 0.00474914\n","Iteration 2489, loss = 0.00474721\n","Iteration 2490, loss = 0.00474529\n","Iteration 2491, loss = 0.00474336\n","Iteration 2492, loss = 0.00474144\n","Iteration 2493, loss = 0.00473952\n","Iteration 2494, loss = 0.00473760\n","Iteration 2495, loss = 0.00473568\n","Iteration 2496, loss = 0.00473376\n","Iteration 2497, loss = 0.00473184\n","Iteration 2498, loss = 0.00472993\n","Iteration 2499, loss = 0.00472802\n","Iteration 2500, loss = 0.00472611\n","Iteration 2501, loss = 0.00472420\n","Iteration 2502, loss = 0.00472229\n","Iteration 2503, loss = 0.00472039\n","Iteration 2504, loss = 0.00471848\n","Iteration 2505, loss = 0.00471658\n","Iteration 2506, loss = 0.00471468\n","Iteration 2507, loss = 0.00471278\n","Iteration 2508, loss = 0.00471088\n","Iteration 2509, loss = 0.00470898\n","Iteration 2510, loss = 0.00470709\n","Iteration 2511, loss = 0.00470520\n","Iteration 2512, loss = 0.00470330\n","Iteration 2513, loss = 0.00470141\n","Iteration 2514, loss = 0.00469953\n","Iteration 2515, loss = 0.00469764\n","Iteration 2516, loss = 0.00469575\n","Iteration 2517, loss = 0.00469387\n","Iteration 2518, loss = 0.00469199\n","Iteration 2519, loss = 0.00469011\n","Iteration 2520, loss = 0.00468823\n","Iteration 2521, loss = 0.00468635\n","Iteration 2522, loss = 0.00468447\n","Iteration 2523, loss = 0.00468260\n","Iteration 2524, loss = 0.00468073\n","Iteration 2525, loss = 0.00467886\n","Iteration 2526, loss = 0.00467699\n","Iteration 2527, loss = 0.00467512\n","Iteration 2528, loss = 0.00467325\n","Iteration 2529, loss = 0.00467139\n","Iteration 2530, loss = 0.00466952\n","Iteration 2531, loss = 0.00466766\n","Iteration 2532, loss = 0.00466580\n","Iteration 2533, loss = 0.00466394\n","Iteration 2534, loss = 0.00466208\n","Iteration 2535, loss = 0.00466023\n","Iteration 2536, loss = 0.00465837\n","Iteration 2537, loss = 0.00465652\n","Iteration 2538, loss = 0.00465467\n","Iteration 2539, loss = 0.00465282\n","Iteration 2540, loss = 0.00465097\n","Iteration 2541, loss = 0.00464912\n","Iteration 2542, loss = 0.00464728\n","Iteration 2543, loss = 0.00464543\n","Iteration 2544, loss = 0.00464359\n","Iteration 2545, loss = 0.00464175\n","Iteration 2546, loss = 0.00463991\n","Iteration 2547, loss = 0.00463807\n","Iteration 2548, loss = 0.00463624\n","Iteration 2549, loss = 0.00463440\n","Iteration 2550, loss = 0.00463257\n","Iteration 2551, loss = 0.00463074\n","Iteration 2552, loss = 0.00462891\n","Iteration 2553, loss = 0.00462708\n","Iteration 2554, loss = 0.00462525\n","Iteration 2555, loss = 0.00462342\n","Iteration 2556, loss = 0.00462160\n","Iteration 2557, loss = 0.00461978\n","Iteration 2558, loss = 0.00461795\n","Iteration 2559, loss = 0.00461613\n","Iteration 2560, loss = 0.00461432\n","Iteration 2561, loss = 0.00461250\n","Iteration 2562, loss = 0.00461068\n","Iteration 2563, loss = 0.00460887\n","Iteration 2564, loss = 0.00460706\n","Iteration 2565, loss = 0.00460524\n","Iteration 2566, loss = 0.00460344\n","Iteration 2567, loss = 0.00460163\n","Iteration 2568, loss = 0.00459982\n","Iteration 2569, loss = 0.00459802\n","Iteration 2570, loss = 0.00459621\n","Iteration 2571, loss = 0.00459441\n","Iteration 2572, loss = 0.00459261\n","Iteration 2573, loss = 0.00459081\n","Iteration 2574, loss = 0.00458901\n","Iteration 2575, loss = 0.00458721\n","Iteration 2576, loss = 0.00458542\n","Iteration 2577, loss = 0.00458363\n","Iteration 2578, loss = 0.00458183\n","Iteration 2579, loss = 0.00458004\n","Iteration 2580, loss = 0.00457825\n","Iteration 2581, loss = 0.00457647\n","Iteration 2582, loss = 0.00457468\n","Iteration 2583, loss = 0.00457289\n","Iteration 2584, loss = 0.00457111\n","Iteration 2585, loss = 0.00456933\n","Iteration 2586, loss = 0.00456755\n","Iteration 2587, loss = 0.00456577\n","Iteration 2588, loss = 0.00456399\n","Iteration 2589, loss = 0.00456221\n","Iteration 2590, loss = 0.00456044\n","Iteration 2591, loss = 0.00455867\n","Iteration 2592, loss = 0.00455689\n","Iteration 2593, loss = 0.00455512\n","Iteration 2594, loss = 0.00455335\n","Iteration 2595, loss = 0.00455159\n","Iteration 2596, loss = 0.00454982\n","Iteration 2597, loss = 0.00454805\n","Iteration 2598, loss = 0.00454629\n","Iteration 2599, loss = 0.00454453\n","Iteration 2600, loss = 0.00454277\n","Iteration 2601, loss = 0.00454101\n","Iteration 2602, loss = 0.00453925\n","Iteration 2603, loss = 0.00453749\n","Iteration 2604, loss = 0.00453574\n","Iteration 2605, loss = 0.00453398\n","Iteration 2606, loss = 0.00453223\n","Iteration 2607, loss = 0.00453048\n","Iteration 2608, loss = 0.00452873\n","Iteration 2609, loss = 0.00452698\n","Iteration 2610, loss = 0.00452524\n","Iteration 2611, loss = 0.00452349\n","Iteration 2612, loss = 0.00452175\n","Iteration 2613, loss = 0.00452000\n","Iteration 2614, loss = 0.00451826\n","Iteration 2615, loss = 0.00451652\n","Iteration 2616, loss = 0.00451479\n","Iteration 2617, loss = 0.00451305\n","Iteration 2618, loss = 0.00451131\n","Iteration 2619, loss = 0.00450958\n","Iteration 2620, loss = 0.00450785\n","Iteration 2621, loss = 0.00450611\n","Iteration 2622, loss = 0.00450438\n","Iteration 2623, loss = 0.00450265\n","Iteration 2624, loss = 0.00450093\n","Iteration 2625, loss = 0.00449920\n","Iteration 2626, loss = 0.00449748\n","Iteration 2627, loss = 0.00449575\n","Iteration 2628, loss = 0.00449403\n","Iteration 2629, loss = 0.00449231\n","Iteration 2630, loss = 0.00449059\n","Iteration 2631, loss = 0.00448887\n","Iteration 2632, loss = 0.00448716\n","Iteration 2633, loss = 0.00448544\n","Iteration 2634, loss = 0.00448373\n","Iteration 2635, loss = 0.00448202\n","Iteration 2636, loss = 0.00448030\n","Iteration 2637, loss = 0.00447859\n","Iteration 2638, loss = 0.00447689\n","Iteration 2639, loss = 0.00447518\n","Iteration 2640, loss = 0.00447347\n","Iteration 2641, loss = 0.00447177\n","Iteration 2642, loss = 0.00447007\n","Iteration 2643, loss = 0.00446836\n","Iteration 2644, loss = 0.00446666\n","Iteration 2645, loss = 0.00446496\n","Iteration 2646, loss = 0.00446327\n","Iteration 2647, loss = 0.00446157\n","Iteration 2648, loss = 0.00445988\n","Iteration 2649, loss = 0.00445818\n","Iteration 2650, loss = 0.00445649\n","Iteration 2651, loss = 0.00445480\n","Iteration 2652, loss = 0.00445311\n","Iteration 2653, loss = 0.00445142\n","Iteration 2654, loss = 0.00444973\n","Iteration 2655, loss = 0.00444805\n","Iteration 2656, loss = 0.00444636\n","Iteration 2657, loss = 0.00444468\n","Iteration 2658, loss = 0.00444300\n","Iteration 2659, loss = 0.00444132\n","Iteration 2660, loss = 0.00443964\n","Iteration 2661, loss = 0.00443796\n","Iteration 2662, loss = 0.00443628\n","Iteration 2663, loss = 0.00443461\n","Iteration 2664, loss = 0.00443293\n","Iteration 2665, loss = 0.00443126\n","Iteration 2666, loss = 0.00442959\n","Iteration 2667, loss = 0.00442792\n","Iteration 2668, loss = 0.00442625\n","Iteration 2669, loss = 0.00442458\n","Iteration 2670, loss = 0.00442292\n","Iteration 2671, loss = 0.00442125\n","Iteration 2672, loss = 0.00441959\n","Iteration 2673, loss = 0.00441793\n","Iteration 2674, loss = 0.00441627\n","Iteration 2675, loss = 0.00441461\n","Iteration 2676, loss = 0.00441295\n","Iteration 2677, loss = 0.00441129\n","Iteration 2678, loss = 0.00440964\n","Iteration 2679, loss = 0.00440798\n","Iteration 2680, loss = 0.00440633\n","Iteration 2681, loss = 0.00440468\n","Iteration 2682, loss = 0.00440303\n","Iteration 2683, loss = 0.00440138\n","Iteration 2684, loss = 0.00439973\n","Iteration 2685, loss = 0.00439808\n","Iteration 2686, loss = 0.00439644\n","Iteration 2687, loss = 0.00439479\n","Iteration 2688, loss = 0.00439315\n","Iteration 2689, loss = 0.00439151\n","Iteration 2690, loss = 0.00438987\n","Iteration 2691, loss = 0.00438823\n","Iteration 2692, loss = 0.00438659\n","Iteration 2693, loss = 0.00438495\n","Iteration 2694, loss = 0.00438332\n","Iteration 2695, loss = 0.00438168\n","Iteration 2696, loss = 0.00438005\n","Iteration 2697, loss = 0.00437842\n","Iteration 2698, loss = 0.00437679\n","Iteration 2699, loss = 0.00437516\n","Iteration 2700, loss = 0.00437353\n","Iteration 2701, loss = 0.00437190\n","Iteration 2702, loss = 0.00437028\n","Iteration 2703, loss = 0.00436865\n","Iteration 2704, loss = 0.00436703\n","Iteration 2705, loss = 0.00436541\n","Iteration 2706, loss = 0.00436379\n","Iteration 2707, loss = 0.00436217\n","Iteration 2708, loss = 0.00436055\n","Iteration 2709, loss = 0.00435894\n","Iteration 2710, loss = 0.00435732\n","Iteration 2711, loss = 0.00435571\n","Iteration 2712, loss = 0.00435409\n","Iteration 2713, loss = 0.00435248\n","Iteration 2714, loss = 0.00435087\n","Iteration 2715, loss = 0.00434926\n","Iteration 2716, loss = 0.00434765\n","Iteration 2717, loss = 0.00434605\n","Iteration 2718, loss = 0.00434444\n","Iteration 2719, loss = 0.00434284\n","Iteration 2720, loss = 0.00434123\n","Iteration 2721, loss = 0.00433963\n","Iteration 2722, loss = 0.00433803\n","Iteration 2723, loss = 0.00433643\n","Iteration 2724, loss = 0.00433483\n","Iteration 2725, loss = 0.00433324\n","Iteration 2726, loss = 0.00433164\n","Iteration 2727, loss = 0.00433005\n","Iteration 2728, loss = 0.00432845\n","Iteration 2729, loss = 0.00432686\n","Iteration 2730, loss = 0.00432527\n","Iteration 2731, loss = 0.00432368\n","Iteration 2732, loss = 0.00432209\n","Iteration 2733, loss = 0.00432050\n","Iteration 2734, loss = 0.00431892\n","Iteration 2735, loss = 0.00431733\n","Iteration 2736, loss = 0.00431575\n","Iteration 2737, loss = 0.00431417\n","Iteration 2738, loss = 0.00431259\n","Iteration 2739, loss = 0.00431101\n","Iteration 2740, loss = 0.00430943\n","Iteration 2741, loss = 0.00430785\n","Iteration 2742, loss = 0.00430627\n","Iteration 2743, loss = 0.00430470\n","Iteration 2744, loss = 0.00430312\n","Iteration 2745, loss = 0.00430155\n","Iteration 2746, loss = 0.00429998\n","Iteration 2747, loss = 0.00429841\n","Iteration 2748, loss = 0.00429684\n","Iteration 2749, loss = 0.00429527\n","Iteration 2750, loss = 0.00429370\n","Iteration 2751, loss = 0.00429214\n","Iteration 2752, loss = 0.00429057\n","Iteration 2753, loss = 0.00428901\n","Iteration 2754, loss = 0.00428745\n","Iteration 2755, loss = 0.00428589\n","Iteration 2756, loss = 0.00428433\n","Iteration 2757, loss = 0.00428277\n","Iteration 2758, loss = 0.00428121\n","Iteration 2759, loss = 0.00427965\n","Iteration 2760, loss = 0.00427810\n","Iteration 2761, loss = 0.00427655\n","Iteration 2762, loss = 0.00427499\n","Iteration 2763, loss = 0.00427344\n","Iteration 2764, loss = 0.00427189\n","Iteration 2765, loss = 0.00427034\n","Iteration 2766, loss = 0.00426879\n","Iteration 2767, loss = 0.00426725\n","Iteration 2768, loss = 0.00426570\n","Iteration 2769, loss = 0.00426416\n","Iteration 2770, loss = 0.00426261\n","Iteration 2771, loss = 0.00426107\n","Iteration 2772, loss = 0.00425953\n","Iteration 2773, loss = 0.00425799\n","Iteration 2774, loss = 0.00425645\n","Iteration 2775, loss = 0.00425491\n","Iteration 2776, loss = 0.00425337\n","Iteration 2777, loss = 0.00425184\n","Iteration 2778, loss = 0.00425031\n","Iteration 2779, loss = 0.00424877\n","Iteration 2780, loss = 0.00424724\n","Iteration 2781, loss = 0.00424571\n","Iteration 2782, loss = 0.00424418\n","Iteration 2783, loss = 0.00424265\n","Iteration 2784, loss = 0.00424112\n","Iteration 2785, loss = 0.00423960\n","Iteration 2786, loss = 0.00423807\n","Iteration 2787, loss = 0.00423655\n","Iteration 2788, loss = 0.00423502\n","Iteration 2789, loss = 0.00423350\n","Iteration 2790, loss = 0.00423198\n","Iteration 2791, loss = 0.00423046\n","Iteration 2792, loss = 0.00422894\n","Iteration 2793, loss = 0.00422743\n","Iteration 2794, loss = 0.00422591\n","Iteration 2795, loss = 0.00422440\n","Iteration 2796, loss = 0.00422288\n","Iteration 2797, loss = 0.00422137\n","Iteration 2798, loss = 0.00421986\n","Iteration 2799, loss = 0.00421835\n","Iteration 2800, loss = 0.00421684\n","Iteration 2801, loss = 0.00421533\n","Iteration 2802, loss = 0.00421382\n","Iteration 2803, loss = 0.00421232\n","Iteration 2804, loss = 0.00421081\n","Iteration 2805, loss = 0.00420931\n","Iteration 2806, loss = 0.00420781\n","Iteration 2807, loss = 0.00420630\n","Iteration 2808, loss = 0.00420480\n","Iteration 2809, loss = 0.00420330\n","Iteration 2810, loss = 0.00420181\n","Iteration 2811, loss = 0.00420031\n","Iteration 2812, loss = 0.00419881\n","Iteration 2813, loss = 0.00419732\n","Iteration 2814, loss = 0.00419582\n","Iteration 2815, loss = 0.00419433\n","Iteration 2816, loss = 0.00419284\n","Iteration 2817, loss = 0.00419135\n","Iteration 2818, loss = 0.00418986\n","Iteration 2819, loss = 0.00418837\n","Iteration 2820, loss = 0.00418688\n","Iteration 2821, loss = 0.00418540\n","Iteration 2822, loss = 0.00418391\n","Iteration 2823, loss = 0.00418243\n","Iteration 2824, loss = 0.00418095\n","Iteration 2825, loss = 0.00417946\n","Iteration 2826, loss = 0.00417798\n","Iteration 2827, loss = 0.00417650\n","Iteration 2828, loss = 0.00417502\n","Iteration 2829, loss = 0.00417355\n","Iteration 2830, loss = 0.00417207\n","Iteration 2831, loss = 0.00417060\n","Iteration 2832, loss = 0.00416912\n","Iteration 2833, loss = 0.00416765\n","Iteration 2834, loss = 0.00416618\n","Iteration 2835, loss = 0.00416471\n","Iteration 2836, loss = 0.00416324\n","Iteration 2837, loss = 0.00416177\n","Iteration 2838, loss = 0.00416030\n","Iteration 2839, loss = 0.00415883\n","Iteration 2840, loss = 0.00415737\n","Iteration 2841, loss = 0.00415590\n","Iteration 2842, loss = 0.00415444\n","Iteration 2843, loss = 0.00415298\n","Iteration 2844, loss = 0.00415152\n","Iteration 2845, loss = 0.00415006\n","Iteration 2846, loss = 0.00414860\n","Iteration 2847, loss = 0.00414714\n","Iteration 2848, loss = 0.00414568\n","Iteration 2849, loss = 0.00414422\n","Iteration 2850, loss = 0.00414277\n","Iteration 2851, loss = 0.00414132\n","Iteration 2852, loss = 0.00413986\n","Iteration 2853, loss = 0.00413841\n","Iteration 2854, loss = 0.00413696\n","Iteration 2855, loss = 0.00413551\n","Iteration 2856, loss = 0.00413406\n","Iteration 2857, loss = 0.00413262\n","Iteration 2858, loss = 0.00413117\n","Iteration 2859, loss = 0.00412972\n","Iteration 2860, loss = 0.00412828\n","Iteration 2861, loss = 0.00412684\n","Iteration 2862, loss = 0.00412539\n","Iteration 2863, loss = 0.00412395\n","Iteration 2864, loss = 0.00412251\n","Iteration 2865, loss = 0.00412107\n","Iteration 2866, loss = 0.00411963\n","Iteration 2867, loss = 0.00411820\n","Iteration 2868, loss = 0.00411676\n","Iteration 2869, loss = 0.00411533\n","Iteration 2870, loss = 0.00411389\n","Iteration 2871, loss = 0.00411246\n","Iteration 2872, loss = 0.00411103\n","Iteration 2873, loss = 0.00410960\n","Iteration 2874, loss = 0.00410817\n","Iteration 2875, loss = 0.00410674\n","Iteration 2876, loss = 0.00410531\n","Iteration 2877, loss = 0.00410388\n","Iteration 2878, loss = 0.00410246\n","Iteration 2879, loss = 0.00410103\n","Iteration 2880, loss = 0.00409961\n","Iteration 2881, loss = 0.00409819\n","Iteration 2882, loss = 0.00409677\n","Iteration 2883, loss = 0.00409535\n","Iteration 2884, loss = 0.00409393\n","Iteration 2885, loss = 0.00409251\n","Iteration 2886, loss = 0.00409109\n","Iteration 2887, loss = 0.00408967\n","Iteration 2888, loss = 0.00408826\n","Iteration 2889, loss = 0.00408684\n","Iteration 2890, loss = 0.00408543\n","Iteration 2891, loss = 0.00408402\n","Iteration 2892, loss = 0.00408261\n","Iteration 2893, loss = 0.00408120\n","Iteration 2894, loss = 0.00407979\n","Iteration 2895, loss = 0.00407838\n","Iteration 2896, loss = 0.00407697\n","Iteration 2897, loss = 0.00407556\n","Iteration 2898, loss = 0.00407416\n","Iteration 2899, loss = 0.00407275\n","Iteration 2900, loss = 0.00407135\n","Iteration 2901, loss = 0.00406995\n","Iteration 2902, loss = 0.00406855\n","Iteration 2903, loss = 0.00406715\n","Iteration 2904, loss = 0.00406575\n","Iteration 2905, loss = 0.00406435\n","Iteration 2906, loss = 0.00406295\n","Iteration 2907, loss = 0.00406156\n","Iteration 2908, loss = 0.00406016\n","Iteration 2909, loss = 0.00405877\n","Iteration 2910, loss = 0.00405737\n","Iteration 2911, loss = 0.00405598\n","Iteration 2912, loss = 0.00405459\n","Iteration 2913, loss = 0.00405320\n","Iteration 2914, loss = 0.00405181\n","Iteration 2915, loss = 0.00405042\n","Iteration 2916, loss = 0.00404903\n","Iteration 2917, loss = 0.00404765\n","Iteration 2918, loss = 0.00404626\n","Iteration 2919, loss = 0.00404488\n","Iteration 2920, loss = 0.00404349\n","Iteration 2921, loss = 0.00404211\n","Iteration 2922, loss = 0.00404073\n","Iteration 2923, loss = 0.00403935\n","Iteration 2924, loss = 0.00403797\n","Iteration 2925, loss = 0.00403659\n","Iteration 2926, loss = 0.00403521\n","Iteration 2927, loss = 0.00403384\n","Iteration 2928, loss = 0.00403246\n","Iteration 2929, loss = 0.00403109\n","Iteration 2930, loss = 0.00402971\n","Iteration 2931, loss = 0.00402834\n","Iteration 2932, loss = 0.00402697\n","Iteration 2933, loss = 0.00402560\n","Iteration 2934, loss = 0.00402423\n","Iteration 2935, loss = 0.00402286\n","Iteration 2936, loss = 0.00402149\n","Iteration 2937, loss = 0.00402013\n","Iteration 2938, loss = 0.00401876\n","Iteration 2939, loss = 0.00401739\n","Iteration 2940, loss = 0.00401603\n","Iteration 2941, loss = 0.00401467\n","Iteration 2942, loss = 0.00401331\n","Iteration 2943, loss = 0.00401194\n","Iteration 2944, loss = 0.00401058\n","Iteration 2945, loss = 0.00400923\n","Iteration 2946, loss = 0.00400787\n","Iteration 2947, loss = 0.00400651\n","Iteration 2948, loss = 0.00400515\n","Iteration 2949, loss = 0.00400380\n","Iteration 2950, loss = 0.00400244\n","Iteration 2951, loss = 0.00400109\n","Iteration 2952, loss = 0.00399974\n","Iteration 2953, loss = 0.00399839\n","Iteration 2954, loss = 0.00399704\n","Iteration 2955, loss = 0.00399569\n","Iteration 2956, loss = 0.00399434\n","Iteration 2957, loss = 0.00399299\n","Iteration 2958, loss = 0.00399164\n","Iteration 2959, loss = 0.00399030\n","Iteration 2960, loss = 0.00398895\n","Iteration 2961, loss = 0.00398761\n","Iteration 2962, loss = 0.00398626\n","Iteration 2963, loss = 0.00398492\n","Iteration 2964, loss = 0.00398358\n","Iteration 2965, loss = 0.00398224\n","Iteration 2966, loss = 0.00398090\n","Iteration 2967, loss = 0.00397956\n","Iteration 2968, loss = 0.00397823\n","Iteration 2969, loss = 0.00397689\n","Iteration 2970, loss = 0.00397555\n","Iteration 2971, loss = 0.00397422\n","Iteration 2972, loss = 0.00397289\n","Iteration 2973, loss = 0.00397155\n","Iteration 2974, loss = 0.00397022\n","Iteration 2975, loss = 0.00396889\n","Iteration 2976, loss = 0.00396756\n","Iteration 2977, loss = 0.00396623\n","Iteration 2978, loss = 0.00396490\n","Iteration 2979, loss = 0.00396358\n","Iteration 2980, loss = 0.00396225\n","Iteration 2981, loss = 0.00396092\n","Iteration 2982, loss = 0.00395960\n","Iteration 2983, loss = 0.00395828\n","Iteration 2984, loss = 0.00395695\n","Iteration 2985, loss = 0.00395563\n","Iteration 2986, loss = 0.00395431\n","Iteration 2987, loss = 0.00395299\n","Iteration 2988, loss = 0.00395167\n","Iteration 2989, loss = 0.00395035\n","Iteration 2990, loss = 0.00394904\n","Iteration 2991, loss = 0.00394772\n","Iteration 2992, loss = 0.00394641\n","Iteration 2993, loss = 0.00394509\n","Iteration 2994, loss = 0.00394378\n","Iteration 2995, loss = 0.00394247\n","Iteration 2996, loss = 0.00394115\n","Iteration 2997, loss = 0.00393984\n","Iteration 2998, loss = 0.00393853\n","Iteration 2999, loss = 0.00393722\n","Iteration 3000, loss = 0.00393592\n","Iteration 3001, loss = 0.00393461\n","Iteration 3002, loss = 0.00393330\n","Iteration 3003, loss = 0.00393200\n","Iteration 3004, loss = 0.00393069\n","Iteration 3005, loss = 0.00392939\n","Iteration 3006, loss = 0.00392809\n","Iteration 3007, loss = 0.00392679\n","Iteration 3008, loss = 0.00392548\n","Iteration 3009, loss = 0.00392418\n","Iteration 3010, loss = 0.00392289\n","Iteration 3011, loss = 0.00392159\n","Iteration 3012, loss = 0.00392029\n","Iteration 3013, loss = 0.00391899\n","Iteration 3014, loss = 0.00391770\n","Iteration 3015, loss = 0.00391640\n","Iteration 3016, loss = 0.00391511\n","Iteration 3017, loss = 0.00391382\n","Iteration 3018, loss = 0.00391253\n","Iteration 3019, loss = 0.00391123\n","Iteration 3020, loss = 0.00390994\n","Iteration 3021, loss = 0.00390866\n","Iteration 3022, loss = 0.00390737\n","Iteration 3023, loss = 0.00390608\n","Iteration 3024, loss = 0.00390479\n","Iteration 3025, loss = 0.00390351\n","Iteration 3026, loss = 0.00390222\n","Iteration 3027, loss = 0.00390094\n","Iteration 3028, loss = 0.00389966\n","Iteration 3029, loss = 0.00389837\n","Iteration 3030, loss = 0.00389709\n","Iteration 3031, loss = 0.00389581\n","Iteration 3032, loss = 0.00389453\n","Iteration 3033, loss = 0.00389325\n","Iteration 3034, loss = 0.00389198\n","Iteration 3035, loss = 0.00389070\n","Iteration 3036, loss = 0.00388942\n","Iteration 3037, loss = 0.00388815\n","Iteration 3038, loss = 0.00388687\n","Iteration 3039, loss = 0.00388560\n","Iteration 3040, loss = 0.00388433\n","Iteration 3041, loss = 0.00388306\n","Iteration 3042, loss = 0.00388179\n","Iteration 3043, loss = 0.00388052\n","Iteration 3044, loss = 0.00387925\n","Iteration 3045, loss = 0.00387798\n","Iteration 3046, loss = 0.00387671\n","Iteration 3047, loss = 0.00387544\n","Iteration 3048, loss = 0.00387418\n","Iteration 3049, loss = 0.00387291\n","Iteration 3050, loss = 0.00387165\n","Iteration 3051, loss = 0.00387039\n","Iteration 3052, loss = 0.00386912\n","Iteration 3053, loss = 0.00386786\n","Iteration 3054, loss = 0.00386660\n","Iteration 3055, loss = 0.00386534\n","Iteration 3056, loss = 0.00386408\n","Iteration 3057, loss = 0.00386283\n","Iteration 3058, loss = 0.00386157\n","Iteration 3059, loss = 0.00386031\n","Iteration 3060, loss = 0.00385906\n","Iteration 3061, loss = 0.00385780\n","Iteration 3062, loss = 0.00385655\n","Iteration 3063, loss = 0.00385530\n","Iteration 3064, loss = 0.00385404\n","Iteration 3065, loss = 0.00385279\n","Iteration 3066, loss = 0.00385154\n","Iteration 3067, loss = 0.00385029\n","Iteration 3068, loss = 0.00384904\n","Iteration 3069, loss = 0.00384780\n","Iteration 3070, loss = 0.00384655\n","Iteration 3071, loss = 0.00384530\n","Iteration 3072, loss = 0.00384406\n","Iteration 3073, loss = 0.00384281\n","Iteration 3074, loss = 0.00384157\n","Iteration 3075, loss = 0.00384033\n","Iteration 3076, loss = 0.00383909\n","Iteration 3077, loss = 0.00383785\n","Iteration 3078, loss = 0.00383661\n","Iteration 3079, loss = 0.00383537\n","Iteration 3080, loss = 0.00383413\n","Iteration 3081, loss = 0.00383289\n","Iteration 3082, loss = 0.00383165\n","Iteration 3083, loss = 0.00383042\n","Iteration 3084, loss = 0.00382918\n","Iteration 3085, loss = 0.00382795\n","Iteration 3086, loss = 0.00382671\n","Iteration 3087, loss = 0.00382548\n","Iteration 3088, loss = 0.00382425\n","Iteration 3089, loss = 0.00382302\n","Iteration 3090, loss = 0.00382179\n","Iteration 3091, loss = 0.00382056\n","Iteration 3092, loss = 0.00381933\n","Iteration 3093, loss = 0.00381810\n","Iteration 3094, loss = 0.00381687\n","Iteration 3095, loss = 0.00381565\n","Iteration 3096, loss = 0.00381442\n","Iteration 3097, loss = 0.00381320\n","Iteration 3098, loss = 0.00381198\n","Iteration 3099, loss = 0.00381075\n","Iteration 3100, loss = 0.00380953\n","Iteration 3101, loss = 0.00380831\n","Iteration 3102, loss = 0.00380709\n","Iteration 3103, loss = 0.00380587\n","Iteration 3104, loss = 0.00380465\n","Iteration 3105, loss = 0.00380343\n","Iteration 3106, loss = 0.00380222\n","Iteration 3107, loss = 0.00380100\n","Iteration 3108, loss = 0.00379978\n","Iteration 3109, loss = 0.00379857\n","Iteration 3110, loss = 0.00379735\n","Iteration 3111, loss = 0.00379614\n","Iteration 3112, loss = 0.00379493\n","Iteration 3113, loss = 0.00379372\n","Iteration 3114, loss = 0.00379251\n","Iteration 3115, loss = 0.00379130\n","Iteration 3116, loss = 0.00379009\n","Iteration 3117, loss = 0.00378888\n","Iteration 3118, loss = 0.00378767\n","Iteration 3119, loss = 0.00378647\n","Iteration 3120, loss = 0.00378526\n","Iteration 3121, loss = 0.00378406\n","Iteration 3122, loss = 0.00378285\n","Iteration 3123, loss = 0.00378165\n","Iteration 3124, loss = 0.00378044\n","Iteration 3125, loss = 0.00377924\n","Iteration 3126, loss = 0.00377804\n","Iteration 3127, loss = 0.00377684\n","Iteration 3128, loss = 0.00377564\n","Iteration 3129, loss = 0.00377444\n","Iteration 3130, loss = 0.00377325\n","Iteration 3131, loss = 0.00377205\n","Iteration 3132, loss = 0.00377085\n","Iteration 3133, loss = 0.00376966\n","Iteration 3134, loss = 0.00376846\n","Iteration 3135, loss = 0.00376727\n","Iteration 3136, loss = 0.00376607\n","Iteration 3137, loss = 0.00376488\n","Iteration 3138, loss = 0.00376369\n","Iteration 3139, loss = 0.00376250\n","Iteration 3140, loss = 0.00376131\n","Iteration 3141, loss = 0.00376012\n","Iteration 3142, loss = 0.00375893\n","Iteration 3143, loss = 0.00375775\n","Iteration 3144, loss = 0.00375656\n","Iteration 3145, loss = 0.00375537\n","Iteration 3146, loss = 0.00375419\n","Iteration 3147, loss = 0.00375300\n","Iteration 3148, loss = 0.00375182\n","Iteration 3149, loss = 0.00375064\n","Iteration 3150, loss = 0.00374945\n","Iteration 3151, loss = 0.00374827\n","Iteration 3152, loss = 0.00374709\n","Iteration 3153, loss = 0.00374591\n","Iteration 3154, loss = 0.00374473\n","Iteration 3155, loss = 0.00374355\n","Iteration 3156, loss = 0.00374238\n","Iteration 3157, loss = 0.00374120\n","Iteration 3158, loss = 0.00374002\n","Iteration 3159, loss = 0.00373885\n","Iteration 3160, loss = 0.00373767\n","Iteration 3161, loss = 0.00373650\n","Iteration 3162, loss = 0.00373533\n","Iteration 3163, loss = 0.00373416\n","Iteration 3164, loss = 0.00373298\n","Iteration 3165, loss = 0.00373181\n","Iteration 3166, loss = 0.00373064\n","Iteration 3167, loss = 0.00372947\n","Iteration 3168, loss = 0.00372831\n","Iteration 3169, loss = 0.00372714\n","Iteration 3170, loss = 0.00372597\n","Iteration 3171, loss = 0.00372481\n","Iteration 3172, loss = 0.00372364\n","Iteration 3173, loss = 0.00372248\n","Iteration 3174, loss = 0.00372131\n","Iteration 3175, loss = 0.00372015\n","Iteration 3176, loss = 0.00371899\n","Iteration 3177, loss = 0.00371783\n","Iteration 3178, loss = 0.00371667\n","Iteration 3179, loss = 0.00371551\n","Iteration 3180, loss = 0.00371435\n","Iteration 3181, loss = 0.00371319\n","Iteration 3182, loss = 0.00371203\n","Iteration 3183, loss = 0.00371087\n","Iteration 3184, loss = 0.00370972\n","Iteration 3185, loss = 0.00370856\n","Iteration 3186, loss = 0.00370741\n","Iteration 3187, loss = 0.00370625\n","Iteration 3188, loss = 0.00370510\n","Iteration 3189, loss = 0.00370395\n","Iteration 3190, loss = 0.00370280\n","Iteration 3191, loss = 0.00370164\n","Iteration 3192, loss = 0.00370049\n","Iteration 3193, loss = 0.00369935\n","Iteration 3194, loss = 0.00369820\n","Iteration 3195, loss = 0.00369705\n","Iteration 3196, loss = 0.00369590\n","Iteration 3197, loss = 0.00369476\n","Iteration 3198, loss = 0.00369361\n","Iteration 3199, loss = 0.00369246\n","Iteration 3200, loss = 0.00369132\n","Iteration 3201, loss = 0.00369018\n","Iteration 3202, loss = 0.00368903\n","Iteration 3203, loss = 0.00368789\n","Iteration 3204, loss = 0.00368675\n","Iteration 3205, loss = 0.00368561\n","Iteration 3206, loss = 0.00368447\n","Iteration 3207, loss = 0.00368333\n","Iteration 3208, loss = 0.00368219\n","Iteration 3209, loss = 0.00368106\n","Iteration 3210, loss = 0.00367992\n","Iteration 3211, loss = 0.00367878\n","Iteration 3212, loss = 0.00367765\n","Iteration 3213, loss = 0.00367651\n","Iteration 3214, loss = 0.00367538\n","Iteration 3215, loss = 0.00367425\n","Iteration 3216, loss = 0.00367311\n","Iteration 3217, loss = 0.00367198\n","Iteration 3218, loss = 0.00367085\n","Iteration 3219, loss = 0.00366972\n","Iteration 3220, loss = 0.00366859\n","Iteration 3221, loss = 0.00366746\n","Iteration 3222, loss = 0.00366633\n","Iteration 3223, loss = 0.00366521\n","Iteration 3224, loss = 0.00366408\n","Iteration 3225, loss = 0.00366295\n","Iteration 3226, loss = 0.00366183\n","Iteration 3227, loss = 0.00366070\n","Iteration 3228, loss = 0.00365958\n","Iteration 3229, loss = 0.00365846\n","Iteration 3230, loss = 0.00365733\n","Iteration 3231, loss = 0.00365621\n","Iteration 3232, loss = 0.00365509\n","Iteration 3233, loss = 0.00365397\n","Iteration 3234, loss = 0.00365285\n","Iteration 3235, loss = 0.00365173\n","Iteration 3236, loss = 0.00365062\n","Iteration 3237, loss = 0.00364950\n","Iteration 3238, loss = 0.00364838\n","Iteration 3239, loss = 0.00364727\n","Iteration 3240, loss = 0.00364615\n","Iteration 3241, loss = 0.00364504\n","Iteration 3242, loss = 0.00364392\n","Iteration 3243, loss = 0.00364281\n","Iteration 3244, loss = 0.00364170\n","Iteration 3245, loss = 0.00364059\n","Iteration 3246, loss = 0.00363947\n","Iteration 3247, loss = 0.00363836\n","Iteration 3248, loss = 0.00363726\n","Iteration 3249, loss = 0.00363615\n","Iteration 3250, loss = 0.00363504\n","Iteration 3251, loss = 0.00363393\n","Iteration 3252, loss = 0.00363282\n","Iteration 3253, loss = 0.00363172\n","Iteration 3254, loss = 0.00363061\n","Iteration 3255, loss = 0.00362951\n","Iteration 3256, loss = 0.00362840\n","Iteration 3257, loss = 0.00362730\n","Iteration 3258, loss = 0.00362620\n","Iteration 3259, loss = 0.00362510\n","Iteration 3260, loss = 0.00362400\n","Iteration 3261, loss = 0.00362290\n","Iteration 3262, loss = 0.00362180\n","Iteration 3263, loss = 0.00362070\n","Iteration 3264, loss = 0.00361960\n","Iteration 3265, loss = 0.00361850\n","Iteration 3266, loss = 0.00361740\n","Iteration 3267, loss = 0.00361631\n","Iteration 3268, loss = 0.00361521\n","Iteration 3269, loss = 0.00361412\n","Iteration 3270, loss = 0.00361302\n","Iteration 3271, loss = 0.00361193\n","Iteration 3272, loss = 0.00361084\n","Iteration 3273, loss = 0.00360975\n","Iteration 3274, loss = 0.00360866\n","Iteration 3275, loss = 0.00360756\n","Iteration 3276, loss = 0.00360647\n","Iteration 3277, loss = 0.00360539\n","Iteration 3278, loss = 0.00360430\n","Iteration 3279, loss = 0.00360321\n","Iteration 3280, loss = 0.00360212\n","Iteration 3281, loss = 0.00360104\n","Iteration 3282, loss = 0.00359995\n","Iteration 3283, loss = 0.00359886\n","Iteration 3284, loss = 0.00359778\n","Iteration 3285, loss = 0.00359670\n","Iteration 3286, loss = 0.00359561\n","Iteration 3287, loss = 0.00359453\n","Iteration 3288, loss = 0.00359345\n","Iteration 3289, loss = 0.00359237\n","Iteration 3290, loss = 0.00359129\n","Iteration 3291, loss = 0.00359021\n","Iteration 3292, loss = 0.00358913\n","Iteration 3293, loss = 0.00358805\n","Iteration 3294, loss = 0.00358697\n","Iteration 3295, loss = 0.00358590\n","Iteration 3296, loss = 0.00358482\n","Iteration 3297, loss = 0.00358374\n","Iteration 3298, loss = 0.00358267\n","Iteration 3299, loss = 0.00358159\n","Iteration 3300, loss = 0.00358052\n","Iteration 3301, loss = 0.00357945\n","Iteration 3302, loss = 0.00357838\n","Iteration 3303, loss = 0.00357730\n","Iteration 3304, loss = 0.00357623\n","Iteration 3305, loss = 0.00357516\n","Iteration 3306, loss = 0.00357409\n","Iteration 3307, loss = 0.00357302\n","Iteration 3308, loss = 0.00357196\n","Iteration 3309, loss = 0.00357089\n","Iteration 3310, loss = 0.00356982\n","Iteration 3311, loss = 0.00356876\n","Iteration 3312, loss = 0.00356769\n","Iteration 3313, loss = 0.00356663\n","Iteration 3314, loss = 0.00356556\n","Iteration 3315, loss = 0.00356450\n","Iteration 3316, loss = 0.00356343\n","Iteration 3317, loss = 0.00356237\n","Iteration 3318, loss = 0.00356131\n","Iteration 3319, loss = 0.00356025\n","Iteration 3320, loss = 0.00355919\n","Iteration 3321, loss = 0.00355813\n","Iteration 3322, loss = 0.00355707\n","Iteration 3323, loss = 0.00355601\n","Iteration 3324, loss = 0.00355496\n","Iteration 3325, loss = 0.00355390\n","Iteration 3326, loss = 0.00355284\n","Iteration 3327, loss = 0.00355179\n","Iteration 3328, loss = 0.00355073\n","Iteration 3329, loss = 0.00354968\n","Iteration 3330, loss = 0.00354862\n","Iteration 3331, loss = 0.00354757\n","Iteration 3332, loss = 0.00354652\n","Iteration 3333, loss = 0.00354547\n","Iteration 3334, loss = 0.00354442\n","Iteration 3335, loss = 0.00354337\n","Iteration 3336, loss = 0.00354232\n","Iteration 3337, loss = 0.00354127\n","Iteration 3338, loss = 0.00354022\n","Iteration 3339, loss = 0.00353917\n","Iteration 3340, loss = 0.00353812\n","Iteration 3341, loss = 0.00353708\n","Iteration 3342, loss = 0.00353603\n","Iteration 3343, loss = 0.00353499\n","Iteration 3344, loss = 0.00353394\n","Iteration 3345, loss = 0.00353290\n","Iteration 3346, loss = 0.00353185\n","Iteration 3347, loss = 0.00353081\n","Iteration 3348, loss = 0.00352977\n","Iteration 3349, loss = 0.00352873\n","Iteration 3350, loss = 0.00352769\n","Iteration 3351, loss = 0.00352665\n","Iteration 3352, loss = 0.00352561\n","Iteration 3353, loss = 0.00352457\n","Iteration 3354, loss = 0.00352353\n","Iteration 3355, loss = 0.00352249\n","Iteration 3356, loss = 0.00352146\n","Iteration 3357, loss = 0.00352042\n","Iteration 3358, loss = 0.00351939\n","Iteration 3359, loss = 0.00351835\n","Iteration 3360, loss = 0.00351732\n","Iteration 3361, loss = 0.00351628\n","Iteration 3362, loss = 0.00351525\n","Iteration 3363, loss = 0.00351422\n","Iteration 3364, loss = 0.00351319\n","Iteration 3365, loss = 0.00351215\n","Iteration 3366, loss = 0.00351112\n","Iteration 3367, loss = 0.00351009\n","Iteration 3368, loss = 0.00350906\n","Iteration 3369, loss = 0.00350804\n","Iteration 3370, loss = 0.00350701\n","Iteration 3371, loss = 0.00350598\n","Iteration 3372, loss = 0.00350495\n","Iteration 3373, loss = 0.00350393\n","Iteration 3374, loss = 0.00350290\n","Iteration 3375, loss = 0.00350188\n","Iteration 3376, loss = 0.00350085\n","Iteration 3377, loss = 0.00349983\n","Iteration 3378, loss = 0.00349881\n","Iteration 3379, loss = 0.00349778\n","Iteration 3380, loss = 0.00349676\n","Iteration 3381, loss = 0.00349574\n","Iteration 3382, loss = 0.00349472\n","Iteration 3383, loss = 0.00349370\n","Iteration 3384, loss = 0.00349268\n","Iteration 3385, loss = 0.00349166\n","Iteration 3386, loss = 0.00349065\n","Iteration 3387, loss = 0.00348963\n","Iteration 3388, loss = 0.00348861\n","Iteration 3389, loss = 0.00348760\n","Iteration 3390, loss = 0.00348658\n","Iteration 3391, loss = 0.00348557\n","Iteration 3392, loss = 0.00348455\n","Iteration 3393, loss = 0.00348354\n","Iteration 3394, loss = 0.00348253\n","Iteration 3395, loss = 0.00348151\n","Iteration 3396, loss = 0.00348050\n","Iteration 3397, loss = 0.00347949\n","Iteration 3398, loss = 0.00347848\n","Iteration 3399, loss = 0.00347747\n","Iteration 3400, loss = 0.00347646\n","Iteration 3401, loss = 0.00347545\n","Iteration 3402, loss = 0.00347444\n","Iteration 3403, loss = 0.00347344\n","Iteration 3404, loss = 0.00347243\n","Iteration 3405, loss = 0.00347142\n","Iteration 3406, loss = 0.00347042\n","Iteration 3407, loss = 0.00346941\n","Iteration 3408, loss = 0.00346841\n","Iteration 3409, loss = 0.00346740\n","Iteration 3410, loss = 0.00346640\n","Iteration 3411, loss = 0.00346540\n","Iteration 3412, loss = 0.00346440\n","Iteration 3413, loss = 0.00346340\n","Iteration 3414, loss = 0.00346240\n","Iteration 3415, loss = 0.00346140\n","Iteration 3416, loss = 0.00346040\n","Iteration 3417, loss = 0.00345940\n","Iteration 3418, loss = 0.00345840\n","Iteration 3419, loss = 0.00345740\n","Iteration 3420, loss = 0.00345640\n","Iteration 3421, loss = 0.00345541\n","Iteration 3422, loss = 0.00345441\n","Iteration 3423, loss = 0.00345342\n","Iteration 3424, loss = 0.00345242\n","Iteration 3425, loss = 0.00345143\n","Iteration 3426, loss = 0.00345043\n","Iteration 3427, loss = 0.00344944\n","Iteration 3428, loss = 0.00344845\n","Iteration 3429, loss = 0.00344746\n","Iteration 3430, loss = 0.00344647\n","Iteration 3431, loss = 0.00344548\n","Iteration 3432, loss = 0.00344449\n","Iteration 3433, loss = 0.00344350\n","Iteration 3434, loss = 0.00344251\n","Iteration 3435, loss = 0.00344152\n","Iteration 3436, loss = 0.00344053\n","Iteration 3437, loss = 0.00343955\n","Iteration 3438, loss = 0.00343856\n","Iteration 3439, loss = 0.00343757\n","Iteration 3440, loss = 0.00343659\n","Iteration 3441, loss = 0.00343560\n","Iteration 3442, loss = 0.00343462\n","Iteration 3443, loss = 0.00343364\n","Iteration 3444, loss = 0.00343265\n","Iteration 3445, loss = 0.00343167\n","Iteration 3446, loss = 0.00343069\n","Iteration 3447, loss = 0.00342971\n","Iteration 3448, loss = 0.00342873\n","Iteration 3449, loss = 0.00342775\n","Iteration 3450, loss = 0.00342677\n","Iteration 3451, loss = 0.00342579\n","Iteration 3452, loss = 0.00342481\n","Iteration 3453, loss = 0.00342384\n","Iteration 3454, loss = 0.00342286\n","Iteration 3455, loss = 0.00342188\n","Iteration 3456, loss = 0.00342091\n","Iteration 3457, loss = 0.00341993\n","Iteration 3458, loss = 0.00341896\n","Iteration 3459, loss = 0.00341798\n","Iteration 3460, loss = 0.00341701\n","Iteration 3461, loss = 0.00341604\n","Iteration 3462, loss = 0.00341506\n","Iteration 3463, loss = 0.00341409\n","Iteration 3464, loss = 0.00341312\n","Iteration 3465, loss = 0.00341215\n","Iteration 3466, loss = 0.00341118\n","Iteration 3467, loss = 0.00341021\n","Iteration 3468, loss = 0.00340924\n","Iteration 3469, loss = 0.00340827\n","Iteration 3470, loss = 0.00340731\n","Iteration 3471, loss = 0.00340634\n","Iteration 3472, loss = 0.00340537\n","Iteration 3473, loss = 0.00340441\n","Iteration 3474, loss = 0.00340344\n","Iteration 3475, loss = 0.00340248\n","Iteration 3476, loss = 0.00340151\n","Iteration 3477, loss = 0.00340055\n","Iteration 3478, loss = 0.00339959\n","Iteration 3479, loss = 0.00339862\n","Iteration 3480, loss = 0.00339766\n","Iteration 3481, loss = 0.00339670\n","Iteration 3482, loss = 0.00339574\n","Iteration 3483, loss = 0.00339478\n","Iteration 3484, loss = 0.00339382\n","Iteration 3485, loss = 0.00339286\n","Iteration 3486, loss = 0.00339190\n","Iteration 3487, loss = 0.00339095\n","Iteration 3488, loss = 0.00338999\n","Iteration 3489, loss = 0.00338903\n","Iteration 3490, loss = 0.00338808\n","Iteration 3491, loss = 0.00338712\n","Iteration 3492, loss = 0.00338617\n","Iteration 3493, loss = 0.00338521\n","Iteration 3494, loss = 0.00338426\n","Iteration 3495, loss = 0.00338330\n","Iteration 3496, loss = 0.00338235\n","Iteration 3497, loss = 0.00338140\n","Iteration 3498, loss = 0.00338045\n","Iteration 3499, loss = 0.00337950\n","Iteration 3500, loss = 0.00337855\n","Iteration 3501, loss = 0.00337760\n","Iteration 3502, loss = 0.00337665\n","Iteration 3503, loss = 0.00337570\n","Iteration 3504, loss = 0.00337475\n","Iteration 3505, loss = 0.00337380\n","Iteration 3506, loss = 0.00337285\n","Iteration 3507, loss = 0.00337191\n","Iteration 3508, loss = 0.00337096\n","Iteration 3509, loss = 0.00337002\n","Iteration 3510, loss = 0.00336907\n","Iteration 3511, loss = 0.00336813\n","Iteration 3512, loss = 0.00336718\n","Iteration 3513, loss = 0.00336624\n","Iteration 3514, loss = 0.00336530\n","Iteration 3515, loss = 0.00336435\n","Iteration 3516, loss = 0.00336341\n","Iteration 3517, loss = 0.00336247\n","Iteration 3518, loss = 0.00336153\n","Iteration 3519, loss = 0.00336059\n","Iteration 3520, loss = 0.00335965\n","Iteration 3521, loss = 0.00335871\n","Iteration 3522, loss = 0.00335777\n","Iteration 3523, loss = 0.00335684\n","Iteration 3524, loss = 0.00335590\n","Iteration 3525, loss = 0.00335496\n","Iteration 3526, loss = 0.00335403\n","Iteration 3527, loss = 0.00335309\n","Iteration 3528, loss = 0.00335216\n","Iteration 3529, loss = 0.00335122\n","Iteration 3530, loss = 0.00335029\n","Iteration 3531, loss = 0.00334936\n","Iteration 3532, loss = 0.00334842\n","Iteration 3533, loss = 0.00334749\n","Iteration 3534, loss = 0.00334656\n","Iteration 3535, loss = 0.00334563\n","Iteration 3536, loss = 0.00334470\n","Iteration 3537, loss = 0.00334377\n","Iteration 3538, loss = 0.00334284\n","Iteration 3539, loss = 0.00334191\n","Iteration 3540, loss = 0.00334098\n","Iteration 3541, loss = 0.00334005\n","Iteration 3542, loss = 0.00333912\n","Iteration 3543, loss = 0.00333820\n","Iteration 3544, loss = 0.00333727\n","Iteration 3545, loss = 0.00333635\n","Iteration 3546, loss = 0.00333542\n","Iteration 3547, loss = 0.00333450\n","Iteration 3548, loss = 0.00333357\n","Iteration 3549, loss = 0.00333265\n","Iteration 3550, loss = 0.00333172\n","Iteration 3551, loss = 0.00333080\n","Iteration 3552, loss = 0.00332988\n","Iteration 3553, loss = 0.00332896\n","Iteration 3554, loss = 0.00332804\n","Iteration 3555, loss = 0.00332712\n","Iteration 3556, loss = 0.00332620\n","Iteration 3557, loss = 0.00332528\n","Iteration 3558, loss = 0.00332436\n","Iteration 3559, loss = 0.00332344\n","Iteration 3560, loss = 0.00332252\n","Iteration 3561, loss = 0.00332161\n","Iteration 3562, loss = 0.00332069\n","Iteration 3563, loss = 0.00331977\n","Iteration 3564, loss = 0.00331886\n","Iteration 3565, loss = 0.00331794\n","Iteration 3566, loss = 0.00331703\n","Iteration 3567, loss = 0.00331612\n","Iteration 3568, loss = 0.00331520\n","Iteration 3569, loss = 0.00331429\n","Iteration 3570, loss = 0.00331338\n","Iteration 3571, loss = 0.00331246\n","Iteration 3572, loss = 0.00331155\n","Iteration 3573, loss = 0.00331064\n","Iteration 3574, loss = 0.00330973\n","Iteration 3575, loss = 0.00330882\n","Iteration 3576, loss = 0.00330791\n","Iteration 3577, loss = 0.00330700\n","Iteration 3578, loss = 0.00330610\n","Iteration 3579, loss = 0.00330519\n","Iteration 3580, loss = 0.00330428\n","Iteration 3581, loss = 0.00330337\n","Iteration 3582, loss = 0.00330247\n","Iteration 3583, loss = 0.00330156\n","Iteration 3584, loss = 0.00330066\n","Iteration 3585, loss = 0.00329975\n","Iteration 3586, loss = 0.00329885\n","Iteration 3587, loss = 0.00329795\n","Iteration 3588, loss = 0.00329704\n","Iteration 3589, loss = 0.00329614\n","Iteration 3590, loss = 0.00329524\n","Iteration 3591, loss = 0.00329434\n","Iteration 3592, loss = 0.00329344\n","Iteration 3593, loss = 0.00329254\n","Iteration 3594, loss = 0.00329164\n","Iteration 3595, loss = 0.00329074\n","Iteration 3596, loss = 0.00328984\n","Iteration 3597, loss = 0.00328894\n","Iteration 3598, loss = 0.00328804\n","Iteration 3599, loss = 0.00328715\n","Iteration 3600, loss = 0.00328625\n","Iteration 3601, loss = 0.00328535\n","Iteration 3602, loss = 0.00328446\n","Iteration 3603, loss = 0.00328356\n","Iteration 3604, loss = 0.00328267\n","Iteration 3605, loss = 0.00328177\n","Iteration 3606, loss = 0.00328088\n","Iteration 3607, loss = 0.00327999\n","Iteration 3608, loss = 0.00327909\n","Iteration 3609, loss = 0.00327820\n","Iteration 3610, loss = 0.00327731\n","Iteration 3611, loss = 0.00327642\n","Iteration 3612, loss = 0.00327553\n","Iteration 3613, loss = 0.00327464\n","Iteration 3614, loss = 0.00327375\n","Iteration 3615, loss = 0.00327286\n","Iteration 3616, loss = 0.00327197\n","Iteration 3617, loss = 0.00327108\n","Iteration 3618, loss = 0.00327019\n","Iteration 3619, loss = 0.00326931\n","Iteration 3620, loss = 0.00326842\n","Iteration 3621, loss = 0.00326754\n","Iteration 3622, loss = 0.00326665\n","Iteration 3623, loss = 0.00326576\n","Iteration 3624, loss = 0.00326488\n","Iteration 3625, loss = 0.00326400\n","Iteration 3626, loss = 0.00326311\n","Iteration 3627, loss = 0.00326223\n","Iteration 3628, loss = 0.00326135\n","Iteration 3629, loss = 0.00326047\n","Iteration 3630, loss = 0.00325958\n","Iteration 3631, loss = 0.00325870\n","Iteration 3632, loss = 0.00325782\n","Iteration 3633, loss = 0.00325694\n","Iteration 3634, loss = 0.00325606\n","Iteration 3635, loss = 0.00325518\n","Iteration 3636, loss = 0.00325431\n","Iteration 3637, loss = 0.00325343\n","Iteration 3638, loss = 0.00325255\n","Iteration 3639, loss = 0.00325167\n","Iteration 3640, loss = 0.00325080\n","Iteration 3641, loss = 0.00324992\n","Iteration 3642, loss = 0.00324904\n","Iteration 3643, loss = 0.00324817\n","Iteration 3644, loss = 0.00324730\n","Iteration 3645, loss = 0.00324642\n","Iteration 3646, loss = 0.00324555\n","Iteration 3647, loss = 0.00324467\n","Iteration 3648, loss = 0.00324380\n","Iteration 3649, loss = 0.00324293\n","Iteration 3650, loss = 0.00324206\n","Iteration 3651, loss = 0.00324119\n","Iteration 3652, loss = 0.00324032\n","Iteration 3653, loss = 0.00323945\n","Iteration 3654, loss = 0.00323858\n","Iteration 3655, loss = 0.00323771\n","Iteration 3656, loss = 0.00323684\n","Iteration 3657, loss = 0.00323597\n","Iteration 3658, loss = 0.00323510\n","Iteration 3659, loss = 0.00323424\n","Iteration 3660, loss = 0.00323337\n","Iteration 3661, loss = 0.00323250\n","Iteration 3662, loss = 0.00323164\n","Iteration 3663, loss = 0.00323077\n","Iteration 3664, loss = 0.00322991\n","Iteration 3665, loss = 0.00322904\n","Iteration 3666, loss = 0.00322818\n","Iteration 3667, loss = 0.00322732\n","Iteration 3668, loss = 0.00322646\n","Iteration 3669, loss = 0.00322559\n","Iteration 3670, loss = 0.00322473\n","Iteration 3671, loss = 0.00322387\n","Iteration 3672, loss = 0.00322301\n","Iteration 3673, loss = 0.00322215\n","Iteration 3674, loss = 0.00322129\n","Iteration 3675, loss = 0.00322043\n","Iteration 3676, loss = 0.00321957\n","Iteration 3677, loss = 0.00321871\n","Iteration 3678, loss = 0.00321785\n","Iteration 3679, loss = 0.00321700\n","Iteration 3680, loss = 0.00321614\n","Iteration 3681, loss = 0.00321528\n","Iteration 3682, loss = 0.00321443\n","Iteration 3683, loss = 0.00321357\n","Iteration 3684, loss = 0.00321272\n","Iteration 3685, loss = 0.00321186\n","Iteration 3686, loss = 0.00321101\n","Iteration 3687, loss = 0.00321016\n","Iteration 3688, loss = 0.00320930\n","Iteration 3689, loss = 0.00320845\n","Iteration 3690, loss = 0.00320760\n","Iteration 3691, loss = 0.00320675\n","Iteration 3692, loss = 0.00320589\n","Iteration 3693, loss = 0.00320504\n","Iteration 3694, loss = 0.00320419\n","Iteration 3695, loss = 0.00320334\n","Iteration 3696, loss = 0.00320249\n","Iteration 3697, loss = 0.00320165\n","Iteration 3698, loss = 0.00320080\n","Iteration 3699, loss = 0.00319995\n","Iteration 3700, loss = 0.00319910\n","Iteration 3701, loss = 0.00319826\n","Iteration 3702, loss = 0.00319741\n","Iteration 3703, loss = 0.00319656\n","Iteration 3704, loss = 0.00319572\n","Iteration 3705, loss = 0.00319487\n","Iteration 3706, loss = 0.00319403\n","Iteration 3707, loss = 0.00319318\n","Iteration 3708, loss = 0.00319234\n","Iteration 3709, loss = 0.00319150\n","Iteration 3710, loss = 0.00319065\n","Iteration 3711, loss = 0.00318981\n","Iteration 3712, loss = 0.00318897\n","Iteration 3713, loss = 0.00318813\n","Iteration 3714, loss = 0.00318729\n","Iteration 3715, loss = 0.00318645\n","Iteration 3716, loss = 0.00318561\n","Iteration 3717, loss = 0.00318477\n","Iteration 3718, loss = 0.00318393\n","Iteration 3719, loss = 0.00318309\n","Iteration 3720, loss = 0.00318225\n","Iteration 3721, loss = 0.00318142\n","Iteration 3722, loss = 0.00318058\n","Iteration 3723, loss = 0.00317974\n","Iteration 3724, loss = 0.00317891\n","Iteration 3725, loss = 0.00317807\n","Iteration 3726, loss = 0.00317724\n","Iteration 3727, loss = 0.00317640\n","Iteration 3728, loss = 0.00317557\n","Iteration 3729, loss = 0.00317473\n","Iteration 3730, loss = 0.00317390\n","Iteration 3731, loss = 0.00317307\n","Iteration 3732, loss = 0.00317223\n","Iteration 3733, loss = 0.00317140\n","Iteration 3734, loss = 0.00317057\n","Iteration 3735, loss = 0.00316974\n","Iteration 3736, loss = 0.00316891\n","Iteration 3737, loss = 0.00316808\n","Iteration 3738, loss = 0.00316725\n","Iteration 3739, loss = 0.00316642\n","Iteration 3740, loss = 0.00316559\n","Iteration 3741, loss = 0.00316476\n","Iteration 3742, loss = 0.00316394\n","Iteration 3743, loss = 0.00316311\n","Iteration 3744, loss = 0.00316228\n","Iteration 3745, loss = 0.00316145\n","Iteration 3746, loss = 0.00316063\n","Iteration 3747, loss = 0.00315980\n","Iteration 3748, loss = 0.00315898\n","Iteration 3749, loss = 0.00315815\n","Iteration 3750, loss = 0.00315733\n","Iteration 3751, loss = 0.00315651\n","Iteration 3752, loss = 0.00315568\n","Iteration 3753, loss = 0.00315486\n","Iteration 3754, loss = 0.00315404\n","Iteration 3755, loss = 0.00315321\n","Iteration 3756, loss = 0.00315239\n","Iteration 3757, loss = 0.00315157\n","Iteration 3758, loss = 0.00315075\n","Iteration 3759, loss = 0.00314993\n","Iteration 3760, loss = 0.00314911\n","Iteration 3761, loss = 0.00314829\n","Iteration 3762, loss = 0.00314747\n","Iteration 3763, loss = 0.00314666\n","Iteration 3764, loss = 0.00314584\n","Iteration 3765, loss = 0.00314502\n","Iteration 3766, loss = 0.00314420\n","Iteration 3767, loss = 0.00314339\n","Iteration 3768, loss = 0.00314257\n","Iteration 3769, loss = 0.00314175\n","Iteration 3770, loss = 0.00314094\n","Iteration 3771, loss = 0.00314012\n","Iteration 3772, loss = 0.00313931\n","Iteration 3773, loss = 0.00313850\n","Iteration 3774, loss = 0.00313768\n","Iteration 3775, loss = 0.00313687\n","Iteration 3776, loss = 0.00313606\n","Iteration 3777, loss = 0.00313525\n","Iteration 3778, loss = 0.00313443\n","Iteration 3779, loss = 0.00313362\n","Iteration 3780, loss = 0.00313281\n","Iteration 3781, loss = 0.00313200\n","Iteration 3782, loss = 0.00313119\n","Iteration 3783, loss = 0.00313038\n","Iteration 3784, loss = 0.00312957\n","Iteration 3785, loss = 0.00312877\n","Iteration 3786, loss = 0.00312796\n","Iteration 3787, loss = 0.00312715\n","Iteration 3788, loss = 0.00312634\n","Iteration 3789, loss = 0.00312554\n","Iteration 3790, loss = 0.00312473\n","Iteration 3791, loss = 0.00312392\n","Iteration 3792, loss = 0.00312312\n","Iteration 3793, loss = 0.00312231\n","Iteration 3794, loss = 0.00312151\n","Iteration 3795, loss = 0.00312070\n","Iteration 3796, loss = 0.00311990\n","Iteration 3797, loss = 0.00311910\n","Iteration 3798, loss = 0.00311829\n","Iteration 3799, loss = 0.00311749\n","Iteration 3800, loss = 0.00311669\n","Iteration 3801, loss = 0.00311589\n","Iteration 3802, loss = 0.00311509\n","Iteration 3803, loss = 0.00311429\n","Iteration 3804, loss = 0.00311349\n","Iteration 3805, loss = 0.00311269\n","Iteration 3806, loss = 0.00311189\n","Iteration 3807, loss = 0.00311109\n","Iteration 3808, loss = 0.00311029\n","Iteration 3809, loss = 0.00310949\n","Iteration 3810, loss = 0.00310869\n","Iteration 3811, loss = 0.00310790\n","Iteration 3812, loss = 0.00310710\n","Iteration 3813, loss = 0.00310630\n","Iteration 3814, loss = 0.00310551\n","Iteration 3815, loss = 0.00310471\n","Iteration 3816, loss = 0.00310392\n","Iteration 3817, loss = 0.00310312\n","Iteration 3818, loss = 0.00310233\n","Iteration 3819, loss = 0.00310154\n","Iteration 3820, loss = 0.00310074\n","Iteration 3821, loss = 0.00309995\n","Iteration 3822, loss = 0.00309916\n","Iteration 3823, loss = 0.00309836\n","Iteration 3824, loss = 0.00309757\n","Iteration 3825, loss = 0.00309678\n","Iteration 3826, loss = 0.00309599\n","Iteration 3827, loss = 0.00309520\n","Iteration 3828, loss = 0.00309441\n","Iteration 3829, loss = 0.00309362\n","Iteration 3830, loss = 0.00309283\n","Iteration 3831, loss = 0.00309204\n","Iteration 3832, loss = 0.00309126\n","Iteration 3833, loss = 0.00309047\n","Iteration 3834, loss = 0.00308968\n","Iteration 3835, loss = 0.00308889\n","Iteration 3836, loss = 0.00308811\n","Iteration 3837, loss = 0.00308732\n","Iteration 3838, loss = 0.00308654\n","Iteration 3839, loss = 0.00308575\n","Iteration 3840, loss = 0.00308497\n","Iteration 3841, loss = 0.00308418\n","Iteration 3842, loss = 0.00308340\n","Iteration 3843, loss = 0.00308261\n","Iteration 3844, loss = 0.00308183\n","Iteration 3845, loss = 0.00308105\n","Iteration 3846, loss = 0.00308027\n","Iteration 3847, loss = 0.00307948\n","Iteration 3848, loss = 0.00307870\n","Iteration 3849, loss = 0.00307792\n","Iteration 3850, loss = 0.00307714\n","Iteration 3851, loss = 0.00307636\n","Iteration 3852, loss = 0.00307558\n","Iteration 3853, loss = 0.00307480\n","Iteration 3854, loss = 0.00307402\n","Iteration 3855, loss = 0.00307324\n","Iteration 3856, loss = 0.00307247\n","Iteration 3857, loss = 0.00307169\n","Iteration 3858, loss = 0.00307091\n","Iteration 3859, loss = 0.00307013\n","Iteration 3860, loss = 0.00306936\n","Iteration 3861, loss = 0.00306858\n","Iteration 3862, loss = 0.00306781\n","Iteration 3863, loss = 0.00306703\n","Iteration 3864, loss = 0.00306626\n","Iteration 3865, loss = 0.00306548\n","Iteration 3866, loss = 0.00306471\n","Iteration 3867, loss = 0.00306393\n","Iteration 3868, loss = 0.00306316\n","Iteration 3869, loss = 0.00306239\n","Iteration 3870, loss = 0.00306162\n","Iteration 3871, loss = 0.00306084\n","Iteration 3872, loss = 0.00306007\n","Iteration 3873, loss = 0.00305930\n","Iteration 3874, loss = 0.00305853\n","Iteration 3875, loss = 0.00305776\n","Iteration 3876, loss = 0.00305699\n","Iteration 3877, loss = 0.00305622\n","Iteration 3878, loss = 0.00305545\n","Iteration 3879, loss = 0.00305468\n","Iteration 3880, loss = 0.00305392\n","Iteration 3881, loss = 0.00305315\n","Iteration 3882, loss = 0.00305238\n","Iteration 3883, loss = 0.00305161\n","Iteration 3884, loss = 0.00305085\n","Iteration 3885, loss = 0.00305008\n","Iteration 3886, loss = 0.00304932\n","Iteration 3887, loss = 0.00304855\n","Iteration 3888, loss = 0.00304778\n","Iteration 3889, loss = 0.00304702\n","Iteration 3890, loss = 0.00304626\n","Iteration 3891, loss = 0.00304549\n","Iteration 3892, loss = 0.00304473\n","Iteration 3893, loss = 0.00304397\n","Iteration 3894, loss = 0.00304320\n","Iteration 3895, loss = 0.00304244\n","Iteration 3896, loss = 0.00304168\n","Iteration 3897, loss = 0.00304092\n","Iteration 3898, loss = 0.00304016\n","Iteration 3899, loss = 0.00303940\n","Iteration 3900, loss = 0.00303864\n","Iteration 3901, loss = 0.00303788\n","Iteration 3902, loss = 0.00303712\n","Iteration 3903, loss = 0.00303636\n","Iteration 3904, loss = 0.00303560\n","Iteration 3905, loss = 0.00303484\n","Iteration 3906, loss = 0.00303409\n","Iteration 3907, loss = 0.00303333\n","Iteration 3908, loss = 0.00303257\n","Iteration 3909, loss = 0.00303181\n","Iteration 3910, loss = 0.00303106\n","Iteration 3911, loss = 0.00303030\n","Iteration 3912, loss = 0.00302955\n","Iteration 3913, loss = 0.00302879\n","Iteration 3914, loss = 0.00302804\n","Iteration 3915, loss = 0.00302728\n","Iteration 3916, loss = 0.00302653\n","Iteration 3917, loss = 0.00302578\n","Iteration 3918, loss = 0.00302502\n","Iteration 3919, loss = 0.00302427\n","Iteration 3920, loss = 0.00302352\n","Iteration 3921, loss = 0.00302277\n","Iteration 3922, loss = 0.00302202\n","Iteration 3923, loss = 0.00302127\n","Iteration 3924, loss = 0.00302052\n","Iteration 3925, loss = 0.00301977\n","Iteration 3926, loss = 0.00301902\n","Iteration 3927, loss = 0.00301827\n","Iteration 3928, loss = 0.00301752\n","Iteration 3929, loss = 0.00301677\n","Iteration 3930, loss = 0.00301602\n","Iteration 3931, loss = 0.00301527\n","Iteration 3932, loss = 0.00301453\n","Iteration 3933, loss = 0.00301378\n","Iteration 3934, loss = 0.00301303\n","Iteration 3935, loss = 0.00301229\n","Iteration 3936, loss = 0.00301154\n","Iteration 3937, loss = 0.00301079\n","Iteration 3938, loss = 0.00301005\n","Iteration 3939, loss = 0.00300930\n","Iteration 3940, loss = 0.00300856\n","Iteration 3941, loss = 0.00300782\n","Iteration 3942, loss = 0.00300707\n","Iteration 3943, loss = 0.00300633\n","Iteration 3944, loss = 0.00300559\n","Iteration 3945, loss = 0.00300485\n","Iteration 3946, loss = 0.00300410\n","Iteration 3947, loss = 0.00300336\n","Iteration 3948, loss = 0.00300262\n","Iteration 3949, loss = 0.00300188\n","Iteration 3950, loss = 0.00300114\n","Iteration 3951, loss = 0.00300040\n","Iteration 3952, loss = 0.00299966\n","Iteration 3953, loss = 0.00299892\n","Iteration 3954, loss = 0.00299818\n","Iteration 3955, loss = 0.00299744\n","Iteration 3956, loss = 0.00299671\n","Iteration 3957, loss = 0.00299597\n","Iteration 3958, loss = 0.00299523\n","Iteration 3959, loss = 0.00299449\n","Iteration 3960, loss = 0.00299376\n","Iteration 3961, loss = 0.00299302\n","Iteration 3962, loss = 0.00299229\n","Iteration 3963, loss = 0.00299155\n","Iteration 3964, loss = 0.00299082\n","Iteration 3965, loss = 0.00299008\n","Iteration 3966, loss = 0.00298935\n","Iteration 3967, loss = 0.00298861\n","Iteration 3968, loss = 0.00298788\n","Iteration 3969, loss = 0.00298715\n","Iteration 3970, loss = 0.00298641\n","Iteration 3971, loss = 0.00298568\n","Iteration 3972, loss = 0.00298495\n","Iteration 3973, loss = 0.00298422\n","Iteration 3974, loss = 0.00298349\n","Iteration 3975, loss = 0.00298276\n","Iteration 3976, loss = 0.00298203\n","Iteration 3977, loss = 0.00298130\n","Iteration 3978, loss = 0.00298057\n","Iteration 3979, loss = 0.00297984\n","Iteration 3980, loss = 0.00297911\n","Iteration 3981, loss = 0.00297838\n","Iteration 3982, loss = 0.00297765\n","Iteration 3983, loss = 0.00297692\n","Iteration 3984, loss = 0.00297620\n","Iteration 3985, loss = 0.00297547\n","Iteration 3986, loss = 0.00297474\n","Iteration 3987, loss = 0.00297402\n","Iteration 3988, loss = 0.00297329\n","Iteration 3989, loss = 0.00297256\n","Iteration 3990, loss = 0.00297184\n","Iteration 3991, loss = 0.00297111\n","Iteration 3992, loss = 0.00297039\n","Iteration 3993, loss = 0.00296967\n","Iteration 3994, loss = 0.00296894\n","Iteration 3995, loss = 0.00296822\n","Iteration 3996, loss = 0.00296750\n","Iteration 3997, loss = 0.00296677\n","Iteration 3998, loss = 0.00296605\n","Iteration 3999, loss = 0.00296533\n","Iteration 4000, loss = 0.00296461\n","Iteration 4001, loss = 0.00296389\n","Iteration 4002, loss = 0.00296317\n","Iteration 4003, loss = 0.00296245\n","Iteration 4004, loss = 0.00296173\n","Iteration 4005, loss = 0.00296101\n","Iteration 4006, loss = 0.00296029\n","Iteration 4007, loss = 0.00295957\n","Iteration 4008, loss = 0.00295885\n","Iteration 4009, loss = 0.00295813\n","Iteration 4010, loss = 0.00295742\n","Iteration 4011, loss = 0.00295670\n","Iteration 4012, loss = 0.00295598\n","Iteration 4013, loss = 0.00295526\n","Iteration 4014, loss = 0.00295455\n","Iteration 4015, loss = 0.00295383\n","Iteration 4016, loss = 0.00295312\n","Iteration 4017, loss = 0.00295240\n","Iteration 4018, loss = 0.00295169\n","Iteration 4019, loss = 0.00295097\n","Iteration 4020, loss = 0.00295026\n","Iteration 4021, loss = 0.00294955\n","Iteration 4022, loss = 0.00294883\n","Iteration 4023, loss = 0.00294812\n","Iteration 4024, loss = 0.00294741\n","Iteration 4025, loss = 0.00294669\n","Iteration 4026, loss = 0.00294598\n","Iteration 4027, loss = 0.00294527\n","Iteration 4028, loss = 0.00294456\n","Iteration 4029, loss = 0.00294385\n","Iteration 4030, loss = 0.00294314\n","Iteration 4031, loss = 0.00294243\n","Iteration 4032, loss = 0.00294172\n","Iteration 4033, loss = 0.00294101\n","Iteration 4034, loss = 0.00294030\n","Iteration 4035, loss = 0.00293959\n","Iteration 4036, loss = 0.00293888\n","Iteration 4037, loss = 0.00293818\n","Iteration 4038, loss = 0.00293747\n","Iteration 4039, loss = 0.00293676\n","Iteration 4040, loss = 0.00293606\n","Iteration 4041, loss = 0.00293535\n","Iteration 4042, loss = 0.00293464\n","Iteration 4043, loss = 0.00293394\n","Iteration 4044, loss = 0.00293323\n","Iteration 4045, loss = 0.00293253\n","Iteration 4046, loss = 0.00293182\n","Iteration 4047, loss = 0.00293112\n","Iteration 4048, loss = 0.00293041\n","Iteration 4049, loss = 0.00292971\n","Iteration 4050, loss = 0.00292901\n","Iteration 4051, loss = 0.00292831\n","Iteration 4052, loss = 0.00292760\n","Iteration 4053, loss = 0.00292690\n","Iteration 4054, loss = 0.00292620\n","Iteration 4055, loss = 0.00292550\n","Iteration 4056, loss = 0.00292480\n","Iteration 4057, loss = 0.00292410\n","Iteration 4058, loss = 0.00292340\n","Iteration 4059, loss = 0.00292270\n","Iteration 4060, loss = 0.00292200\n","Iteration 4061, loss = 0.00292130\n","Iteration 4062, loss = 0.00292060\n","Iteration 4063, loss = 0.00291990\n","Iteration 4064, loss = 0.00291920\n","Iteration 4065, loss = 0.00291850\n","Iteration 4066, loss = 0.00291781\n","Iteration 4067, loss = 0.00291711\n","Iteration 4068, loss = 0.00291641\n","Iteration 4069, loss = 0.00291572\n","Iteration 4070, loss = 0.00291502\n","Iteration 4071, loss = 0.00291433\n","Iteration 4072, loss = 0.00291363\n","Iteration 4073, loss = 0.00291293\n","Iteration 4074, loss = 0.00291224\n","Iteration 4075, loss = 0.00291155\n","Iteration 4076, loss = 0.00291085\n","Iteration 4077, loss = 0.00291016\n","Iteration 4078, loss = 0.00290947\n","Iteration 4079, loss = 0.00290877\n","Iteration 4080, loss = 0.00290808\n","Iteration 4081, loss = 0.00290739\n","Iteration 4082, loss = 0.00290670\n","Iteration 4083, loss = 0.00290600\n","Iteration 4084, loss = 0.00290531\n","Iteration 4085, loss = 0.00290462\n","Iteration 4086, loss = 0.00290393\n","Iteration 4087, loss = 0.00290324\n","Iteration 4088, loss = 0.00290255\n","Iteration 4089, loss = 0.00290186\n","Iteration 4090, loss = 0.00290117\n","Iteration 4091, loss = 0.00290049\n","Iteration 4092, loss = 0.00289980\n","Iteration 4093, loss = 0.00289911\n","Iteration 4094, loss = 0.00289842\n","Iteration 4095, loss = 0.00289773\n","Iteration 4096, loss = 0.00289705\n","Iteration 4097, loss = 0.00289636\n","Iteration 4098, loss = 0.00289567\n","Iteration 4099, loss = 0.00289499\n","Iteration 4100, loss = 0.00289430\n","Iteration 4101, loss = 0.00289362\n","Iteration 4102, loss = 0.00289293\n","Iteration 4103, loss = 0.00289225\n","Iteration 4104, loss = 0.00289156\n","Iteration 4105, loss = 0.00289088\n","Iteration 4106, loss = 0.00289020\n","Iteration 4107, loss = 0.00288951\n","Iteration 4108, loss = 0.00288883\n","Iteration 4109, loss = 0.00288815\n","Iteration 4110, loss = 0.00288747\n","Iteration 4111, loss = 0.00288678\n","Iteration 4112, loss = 0.00288610\n","Iteration 4113, loss = 0.00288542\n","Iteration 4114, loss = 0.00288474\n","Iteration 4115, loss = 0.00288406\n","Iteration 4116, loss = 0.00288338\n","Iteration 4117, loss = 0.00288270\n","Iteration 4118, loss = 0.00288202\n","Iteration 4119, loss = 0.00288134\n","Iteration 4120, loss = 0.00288066\n","Iteration 4121, loss = 0.00287999\n","Iteration 4122, loss = 0.00287931\n","Iteration 4123, loss = 0.00287863\n","Iteration 4124, loss = 0.00287795\n","Iteration 4125, loss = 0.00287728\n","Iteration 4126, loss = 0.00287660\n","Iteration 4127, loss = 0.00287592\n","Iteration 4128, loss = 0.00287525\n","Iteration 4129, loss = 0.00287457\n","Iteration 4130, loss = 0.00287390\n","Iteration 4131, loss = 0.00287322\n","Iteration 4132, loss = 0.00287255\n","Iteration 4133, loss = 0.00287187\n","Iteration 4134, loss = 0.00287120\n","Iteration 4135, loss = 0.00287052\n","Iteration 4136, loss = 0.00286985\n","Iteration 4137, loss = 0.00286918\n","Iteration 4138, loss = 0.00286851\n","Iteration 4139, loss = 0.00286783\n","Iteration 4140, loss = 0.00286716\n","Iteration 4141, loss = 0.00286649\n","Iteration 4142, loss = 0.00286582\n","Iteration 4143, loss = 0.00286515\n","Iteration 4144, loss = 0.00286448\n","Iteration 4145, loss = 0.00286381\n","Iteration 4146, loss = 0.00286314\n","Iteration 4147, loss = 0.00286247\n","Iteration 4148, loss = 0.00286180\n","Iteration 4149, loss = 0.00286113\n","Iteration 4150, loss = 0.00286046\n","Iteration 4151, loss = 0.00285979\n","Iteration 4152, loss = 0.00285912\n","Iteration 4153, loss = 0.00285846\n","Iteration 4154, loss = 0.00285779\n","Iteration 4155, loss = 0.00285712\n","Iteration 4156, loss = 0.00285646\n","Iteration 4157, loss = 0.00285579\n","Iteration 4158, loss = 0.00285512\n","Iteration 4159, loss = 0.00285446\n","Iteration 4160, loss = 0.00285379\n","Iteration 4161, loss = 0.00285313\n","Iteration 4162, loss = 0.00285246\n","Iteration 4163, loss = 0.00285180\n","Iteration 4164, loss = 0.00285113\n","Iteration 4165, loss = 0.00285047\n","Iteration 4166, loss = 0.00284981\n","Iteration 4167, loss = 0.00284914\n","Iteration 4168, loss = 0.00284848\n","Iteration 4169, loss = 0.00284782\n","Iteration 4170, loss = 0.00284716\n","Iteration 4171, loss = 0.00284650\n","Iteration 4172, loss = 0.00284583\n","Iteration 4173, loss = 0.00284517\n","Iteration 4174, loss = 0.00284451\n","Iteration 4175, loss = 0.00284385\n","Iteration 4176, loss = 0.00284319\n","Iteration 4177, loss = 0.00284253\n","Iteration 4178, loss = 0.00284187\n","Iteration 4179, loss = 0.00284121\n","Iteration 4180, loss = 0.00284055\n","Iteration 4181, loss = 0.00283990\n","Iteration 4182, loss = 0.00283924\n","Iteration 4183, loss = 0.00283858\n","Iteration 4184, loss = 0.00283792\n","Iteration 4185, loss = 0.00283727\n","Iteration 4186, loss = 0.00283661\n","Iteration 4187, loss = 0.00283595\n","Iteration 4188, loss = 0.00283530\n","Iteration 4189, loss = 0.00283464\n","Iteration 4190, loss = 0.00283398\n","Iteration 4191, loss = 0.00283333\n","Iteration 4192, loss = 0.00283267\n","Iteration 4193, loss = 0.00283202\n","Iteration 4194, loss = 0.00283137\n","Iteration 4195, loss = 0.00283071\n","Iteration 4196, loss = 0.00283006\n","Iteration 4197, loss = 0.00282940\n","Iteration 4198, loss = 0.00282875\n","Iteration 4199, loss = 0.00282810\n","Iteration 4200, loss = 0.00282745\n","Iteration 4201, loss = 0.00282680\n","Iteration 4202, loss = 0.00282614\n","Iteration 4203, loss = 0.00282549\n","Iteration 4204, loss = 0.00282484\n","Iteration 4205, loss = 0.00282419\n","Iteration 4206, loss = 0.00282354\n","Iteration 4207, loss = 0.00282289\n","Iteration 4208, loss = 0.00282224\n","Iteration 4209, loss = 0.00282159\n","Iteration 4210, loss = 0.00282094\n","Iteration 4211, loss = 0.00282029\n","Iteration 4212, loss = 0.00281964\n","Iteration 4213, loss = 0.00281900\n","Iteration 4214, loss = 0.00281835\n","Iteration 4215, loss = 0.00281770\n","Iteration 4216, loss = 0.00281705\n","Iteration 4217, loss = 0.00281641\n","Iteration 4218, loss = 0.00281576\n","Iteration 4219, loss = 0.00281511\n","Iteration 4220, loss = 0.00281447\n","Iteration 4221, loss = 0.00281382\n","Iteration 4222, loss = 0.00281318\n","Iteration 4223, loss = 0.00281253\n","Iteration 4224, loss = 0.00281189\n","Iteration 4225, loss = 0.00281124\n","Iteration 4226, loss = 0.00281060\n","Iteration 4227, loss = 0.00280995\n","Iteration 4228, loss = 0.00280931\n","Iteration 4229, loss = 0.00280867\n","Iteration 4230, loss = 0.00280803\n","Iteration 4231, loss = 0.00280738\n","Iteration 4232, loss = 0.00280674\n","Iteration 4233, loss = 0.00280610\n","Iteration 4234, loss = 0.00280546\n","Iteration 4235, loss = 0.00280482\n","Iteration 4236, loss = 0.00280418\n","Iteration 4237, loss = 0.00280353\n","Iteration 4238, loss = 0.00280289\n","Iteration 4239, loss = 0.00280225\n","Iteration 4240, loss = 0.00280161\n","Iteration 4241, loss = 0.00280098\n","Iteration 4242, loss = 0.00280034\n","Iteration 4243, loss = 0.00279970\n","Iteration 4244, loss = 0.00279906\n","Iteration 4245, loss = 0.00279842\n","Iteration 4246, loss = 0.00279778\n","Iteration 4247, loss = 0.00279715\n","Iteration 4248, loss = 0.00279651\n","Iteration 4249, loss = 0.00279587\n","Iteration 4250, loss = 0.00279523\n","Iteration 4251, loss = 0.00279460\n","Iteration 4252, loss = 0.00279396\n","Iteration 4253, loss = 0.00279333\n","Iteration 4254, loss = 0.00279269\n","Iteration 4255, loss = 0.00279206\n","Iteration 4256, loss = 0.00279142\n","Iteration 4257, loss = 0.00279079\n","Iteration 4258, loss = 0.00279015\n","Iteration 4259, loss = 0.00278952\n","Iteration 4260, loss = 0.00278889\n","Iteration 4261, loss = 0.00278825\n","Iteration 4262, loss = 0.00278762\n","Iteration 4263, loss = 0.00278699\n","Iteration 4264, loss = 0.00278635\n","Iteration 4265, loss = 0.00278572\n","Iteration 4266, loss = 0.00278509\n","Iteration 4267, loss = 0.00278446\n","Iteration 4268, loss = 0.00278383\n","Iteration 4269, loss = 0.00278320\n","Iteration 4270, loss = 0.00278257\n","Iteration 4271, loss = 0.00278194\n","Iteration 4272, loss = 0.00278131\n","Iteration 4273, loss = 0.00278068\n","Iteration 4274, loss = 0.00278005\n","Iteration 4275, loss = 0.00277942\n","Iteration 4276, loss = 0.00277879\n","Iteration 4277, loss = 0.00277816\n","Iteration 4278, loss = 0.00277753\n","Iteration 4279, loss = 0.00277691\n","Iteration 4280, loss = 0.00277628\n","Iteration 4281, loss = 0.00277565\n","Iteration 4282, loss = 0.00277503\n","Iteration 4283, loss = 0.00277440\n","Iteration 4284, loss = 0.00277377\n","Iteration 4285, loss = 0.00277315\n","Iteration 4286, loss = 0.00277252\n","Iteration 4287, loss = 0.00277190\n","Iteration 4288, loss = 0.00277127\n","Iteration 4289, loss = 0.00277065\n","Iteration 4290, loss = 0.00277002\n","Iteration 4291, loss = 0.00276940\n","Iteration 4292, loss = 0.00276877\n","Iteration 4293, loss = 0.00276815\n","Iteration 4294, loss = 0.00276753\n","Iteration 4295, loss = 0.00276690\n","Iteration 4296, loss = 0.00276628\n","Iteration 4297, loss = 0.00276566\n","Iteration 4298, loss = 0.00276504\n","Iteration 4299, loss = 0.00276442\n","Iteration 4300, loss = 0.00276379\n","Iteration 4301, loss = 0.00276317\n","Iteration 4302, loss = 0.00276255\n","Iteration 4303, loss = 0.00276193\n","Iteration 4304, loss = 0.00276131\n","Iteration 4305, loss = 0.00276069\n","Iteration 4306, loss = 0.00276007\n","Iteration 4307, loss = 0.00275945\n","Iteration 4308, loss = 0.00275883\n","Iteration 4309, loss = 0.00275821\n","Iteration 4310, loss = 0.00275760\n","Iteration 4311, loss = 0.00275698\n","Iteration 4312, loss = 0.00275636\n","Iteration 4313, loss = 0.00275574\n","Iteration 4314, loss = 0.00275513\n","Iteration 4315, loss = 0.00275451\n","Iteration 4316, loss = 0.00275389\n","Iteration 4317, loss = 0.00275328\n","Iteration 4318, loss = 0.00275266\n","Iteration 4319, loss = 0.00275204\n","Iteration 4320, loss = 0.00275143\n","Iteration 4321, loss = 0.00275081\n","Iteration 4322, loss = 0.00275020\n","Iteration 4323, loss = 0.00274958\n","Iteration 4324, loss = 0.00274897\n","Iteration 4325, loss = 0.00274835\n","Iteration 4326, loss = 0.00274774\n","Iteration 4327, loss = 0.00274713\n","Iteration 4328, loss = 0.00274651\n","Iteration 4329, loss = 0.00274590\n","Iteration 4330, loss = 0.00274529\n","Iteration 4331, loss = 0.00274468\n","Iteration 4332, loss = 0.00274406\n","Iteration 4333, loss = 0.00274345\n","Iteration 4334, loss = 0.00274284\n","Iteration 4335, loss = 0.00274223\n","Iteration 4336, loss = 0.00274162\n","Iteration 4337, loss = 0.00274101\n","Iteration 4338, loss = 0.00274040\n","Iteration 4339, loss = 0.00273979\n","Iteration 4340, loss = 0.00273918\n","Iteration 4341, loss = 0.00273857\n","Iteration 4342, loss = 0.00273796\n","Iteration 4343, loss = 0.00273735\n","Iteration 4344, loss = 0.00273674\n","Iteration 4345, loss = 0.00273614\n","Iteration 4346, loss = 0.00273553\n","Iteration 4347, loss = 0.00273492\n","Iteration 4348, loss = 0.00273431\n","Iteration 4349, loss = 0.00273371\n","Iteration 4350, loss = 0.00273310\n","Iteration 4351, loss = 0.00273249\n","Iteration 4352, loss = 0.00273189\n","Iteration 4353, loss = 0.00273128\n","Iteration 4354, loss = 0.00273067\n","Iteration 4355, loss = 0.00273007\n","Iteration 4356, loss = 0.00272946\n","Iteration 4357, loss = 0.00272886\n","Iteration 4358, loss = 0.00272825\n","Iteration 4359, loss = 0.00272765\n","Iteration 4360, loss = 0.00272705\n","Iteration 4361, loss = 0.00272644\n","Iteration 4362, loss = 0.00272584\n","Iteration 4363, loss = 0.00272524\n","Iteration 4364, loss = 0.00272463\n","Iteration 4365, loss = 0.00272403\n","Iteration 4366, loss = 0.00272343\n","Iteration 4367, loss = 0.00272283\n","Iteration 4368, loss = 0.00272223\n","Iteration 4369, loss = 0.00272162\n","Iteration 4370, loss = 0.00272102\n","Iteration 4371, loss = 0.00272042\n","Iteration 4372, loss = 0.00271982\n","Iteration 4373, loss = 0.00271922\n","Iteration 4374, loss = 0.00271862\n","Iteration 4375, loss = 0.00271802\n","Iteration 4376, loss = 0.00271742\n","Iteration 4377, loss = 0.00271682\n","Iteration 4378, loss = 0.00271622\n","Iteration 4379, loss = 0.00271563\n","Iteration 4380, loss = 0.00271503\n","Iteration 4381, loss = 0.00271443\n","Iteration 4382, loss = 0.00271383\n","Iteration 4383, loss = 0.00271323\n","Iteration 4384, loss = 0.00271264\n","Iteration 4385, loss = 0.00271204\n","Iteration 4386, loss = 0.00271144\n","Iteration 4387, loss = 0.00271085\n","Iteration 4388, loss = 0.00271025\n","Iteration 4389, loss = 0.00270966\n","Iteration 4390, loss = 0.00270906\n","Iteration 4391, loss = 0.00270847\n","Iteration 4392, loss = 0.00270787\n","Iteration 4393, loss = 0.00270728\n","Iteration 4394, loss = 0.00270668\n","Iteration 4395, loss = 0.00270609\n","Iteration 4396, loss = 0.00270549\n","Iteration 4397, loss = 0.00270490\n","Iteration 4398, loss = 0.00270431\n","Iteration 4399, loss = 0.00270371\n","Iteration 4400, loss = 0.00270312\n","Iteration 4401, loss = 0.00270253\n","Iteration 4402, loss = 0.00270194\n","Iteration 4403, loss = 0.00270134\n","Iteration 4404, loss = 0.00270075\n","Iteration 4405, loss = 0.00270016\n","Iteration 4406, loss = 0.00269957\n","Iteration 4407, loss = 0.00269898\n","Iteration 4408, loss = 0.00269839\n","Iteration 4409, loss = 0.00269780\n","Iteration 4410, loss = 0.00269721\n","Iteration 4411, loss = 0.00269662\n","Iteration 4412, loss = 0.00269603\n","Iteration 4413, loss = 0.00269544\n","Iteration 4414, loss = 0.00269485\n","Iteration 4415, loss = 0.00269426\n","Iteration 4416, loss = 0.00269368\n","Iteration 4417, loss = 0.00269309\n","Iteration 4418, loss = 0.00269250\n","Iteration 4419, loss = 0.00269191\n","Iteration 4420, loss = 0.00269133\n","Iteration 4421, loss = 0.00269074\n","Iteration 4422, loss = 0.00269015\n","Iteration 4423, loss = 0.00268957\n","Iteration 4424, loss = 0.00268898\n","Iteration 4425, loss = 0.00268839\n","Iteration 4426, loss = 0.00268781\n","Iteration 4427, loss = 0.00268722\n","Iteration 4428, loss = 0.00268664\n","Iteration 4429, loss = 0.00268605\n","Iteration 4430, loss = 0.00268547\n","Iteration 4431, loss = 0.00268489\n","Iteration 4432, loss = 0.00268430\n","Iteration 4433, loss = 0.00268372\n","Iteration 4434, loss = 0.00268313\n","Iteration 4435, loss = 0.00268255\n","Iteration 4436, loss = 0.00268197\n","Iteration 4437, loss = 0.00268139\n","Iteration 4438, loss = 0.00268080\n","Iteration 4439, loss = 0.00268022\n","Iteration 4440, loss = 0.00267964\n","Iteration 4441, loss = 0.00267906\n","Iteration 4442, loss = 0.00267848\n","Iteration 4443, loss = 0.00267790\n","Iteration 4444, loss = 0.00267732\n","Iteration 4445, loss = 0.00267674\n","Iteration 4446, loss = 0.00267616\n","Iteration 4447, loss = 0.00267558\n","Iteration 4448, loss = 0.00267500\n","Iteration 4449, loss = 0.00267442\n","Iteration 4450, loss = 0.00267384\n","Iteration 4451, loss = 0.00267326\n","Iteration 4452, loss = 0.00267268\n","Iteration 4453, loss = 0.00267210\n","Iteration 4454, loss = 0.00267152\n","Iteration 4455, loss = 0.00267095\n","Iteration 4456, loss = 0.00267037\n","Iteration 4457, loss = 0.00266979\n","Iteration 4458, loss = 0.00266921\n","Iteration 4459, loss = 0.00266864\n","Iteration 4460, loss = 0.00266806\n","Iteration 4461, loss = 0.00266749\n","Iteration 4462, loss = 0.00266691\n","Iteration 4463, loss = 0.00266633\n","Iteration 4464, loss = 0.00266576\n","Iteration 4465, loss = 0.00266518\n","Iteration 4466, loss = 0.00266461\n","Iteration 4467, loss = 0.00266403\n","Iteration 4468, loss = 0.00266346\n","Iteration 4469, loss = 0.00266289\n","Iteration 4470, loss = 0.00266231\n","Iteration 4471, loss = 0.00266174\n","Iteration 4472, loss = 0.00266117\n","Iteration 4473, loss = 0.00266059\n","Iteration 4474, loss = 0.00266002\n","Iteration 4475, loss = 0.00265945\n","Iteration 4476, loss = 0.00265888\n","Iteration 4477, loss = 0.00265830\n","Iteration 4478, loss = 0.00265773\n","Iteration 4479, loss = 0.00265716\n","Iteration 4480, loss = 0.00265659\n","Iteration 4481, loss = 0.00265602\n","Iteration 4482, loss = 0.00265545\n","Iteration 4483, loss = 0.00265488\n","Iteration 4484, loss = 0.00265431\n","Iteration 4485, loss = 0.00265374\n","Iteration 4486, loss = 0.00265317\n","Iteration 4487, loss = 0.00265260\n","Iteration 4488, loss = 0.00265203\n","Iteration 4489, loss = 0.00265146\n","Iteration 4490, loss = 0.00265089\n","Iteration 4491, loss = 0.00265033\n","Iteration 4492, loss = 0.00264976\n","Iteration 4493, loss = 0.00264919\n","Iteration 4494, loss = 0.00264862\n","Iteration 4495, loss = 0.00264806\n","Iteration 4496, loss = 0.00264749\n","Iteration 4497, loss = 0.00264692\n","Iteration 4498, loss = 0.00264636\n","Iteration 4499, loss = 0.00264579\n","Iteration 4500, loss = 0.00264522\n","Iteration 4501, loss = 0.00264466\n","Iteration 4502, loss = 0.00264409\n","Iteration 4503, loss = 0.00264353\n","Iteration 4504, loss = 0.00264296\n","Iteration 4505, loss = 0.00264240\n","Iteration 4506, loss = 0.00264183\n","Iteration 4507, loss = 0.00264127\n","Iteration 4508, loss = 0.00264071\n","Iteration 4509, loss = 0.00264014\n","Iteration 4510, loss = 0.00263958\n","Iteration 4511, loss = 0.00263902\n","Iteration 4512, loss = 0.00263845\n","Iteration 4513, loss = 0.00263789\n","Iteration 4514, loss = 0.00263733\n","Iteration 4515, loss = 0.00263677\n","Iteration 4516, loss = 0.00263621\n","Iteration 4517, loss = 0.00263564\n","Iteration 4518, loss = 0.00263508\n","Iteration 4519, loss = 0.00263452\n","Iteration 4520, loss = 0.00263396\n","Iteration 4521, loss = 0.00263340\n","Iteration 4522, loss = 0.00263284\n","Iteration 4523, loss = 0.00263228\n","Iteration 4524, loss = 0.00263172\n","Iteration 4525, loss = 0.00263116\n","Iteration 4526, loss = 0.00263060\n","Iteration 4527, loss = 0.00263004\n","Iteration 4528, loss = 0.00262948\n","Iteration 4529, loss = 0.00262893\n","Iteration 4530, loss = 0.00262837\n","Iteration 4531, loss = 0.00262781\n","Iteration 4532, loss = 0.00262725\n","Iteration 4533, loss = 0.00262669\n","Iteration 4534, loss = 0.00262614\n","Iteration 4535, loss = 0.00262558\n","Iteration 4536, loss = 0.00262502\n","Iteration 4537, loss = 0.00262447\n","Iteration 4538, loss = 0.00262391\n","Iteration 4539, loss = 0.00262336\n","Iteration 4540, loss = 0.00262280\n","Iteration 4541, loss = 0.00262224\n","Iteration 4542, loss = 0.00262169\n","Iteration 4543, loss = 0.00262113\n","Iteration 4544, loss = 0.00262058\n","Iteration 4545, loss = 0.00262003\n","Iteration 4546, loss = 0.00261947\n","Iteration 4547, loss = 0.00261892\n","Iteration 4548, loss = 0.00261836\n","Iteration 4549, loss = 0.00261781\n","Iteration 4550, loss = 0.00261726\n","Iteration 4551, loss = 0.00261670\n","Iteration 4552, loss = 0.00261615\n","Iteration 4553, loss = 0.00261560\n","Iteration 4554, loss = 0.00261505\n","Iteration 4555, loss = 0.00261450\n","Iteration 4556, loss = 0.00261394\n","Iteration 4557, loss = 0.00261339\n","Iteration 4558, loss = 0.00261284\n","Iteration 4559, loss = 0.00261229\n","Iteration 4560, loss = 0.00261174\n","Iteration 4561, loss = 0.00261119\n","Iteration 4562, loss = 0.00261064\n","Iteration 4563, loss = 0.00261009\n","Iteration 4564, loss = 0.00260954\n","Iteration 4565, loss = 0.00260899\n","Iteration 4566, loss = 0.00260844\n","Iteration 4567, loss = 0.00260789\n","Iteration 4568, loss = 0.00260734\n","Iteration 4569, loss = 0.00260680\n","Iteration 4570, loss = 0.00260625\n","Iteration 4571, loss = 0.00260570\n","Iteration 4572, loss = 0.00260515\n","Iteration 4573, loss = 0.00260460\n","Iteration 4574, loss = 0.00260406\n","Iteration 4575, loss = 0.00260351\n","Iteration 4576, loss = 0.00260296\n","Iteration 4577, loss = 0.00260242\n","Iteration 4578, loss = 0.00260187\n","Iteration 4579, loss = 0.00260133\n","Iteration 4580, loss = 0.00260078\n","Iteration 4581, loss = 0.00260023\n","Iteration 4582, loss = 0.00259969\n","Iteration 4583, loss = 0.00259914\n","Iteration 4584, loss = 0.00259860\n","Iteration 4585, loss = 0.00259806\n","Iteration 4586, loss = 0.00259751\n","Iteration 4587, loss = 0.00259697\n","Iteration 4588, loss = 0.00259642\n","Iteration 4589, loss = 0.00259588\n","Iteration 4590, loss = 0.00259534\n","Iteration 4591, loss = 0.00259479\n","Iteration 4592, loss = 0.00259425\n","Iteration 4593, loss = 0.00259371\n","Iteration 4594, loss = 0.00259317\n","Iteration 4595, loss = 0.00259262\n","Iteration 4596, loss = 0.00259208\n","Iteration 4597, loss = 0.00259154\n","Iteration 4598, loss = 0.00259100\n","Iteration 4599, loss = 0.00259046\n","Iteration 4600, loss = 0.00258992\n","Iteration 4601, loss = 0.00258938\n","Iteration 4602, loss = 0.00258884\n","Iteration 4603, loss = 0.00258830\n","Iteration 4604, loss = 0.00258776\n","Iteration 4605, loss = 0.00258722\n","Iteration 4606, loss = 0.00258668\n","Iteration 4607, loss = 0.00258614\n","Iteration 4608, loss = 0.00258560\n","Iteration 4609, loss = 0.00258506\n","Iteration 4610, loss = 0.00258452\n","Iteration 4611, loss = 0.00258398\n","Iteration 4612, loss = 0.00258345\n","Iteration 4613, loss = 0.00258291\n","Iteration 4614, loss = 0.00258237\n","Iteration 4615, loss = 0.00258183\n","Iteration 4616, loss = 0.00258130\n","Iteration 4617, loss = 0.00258076\n","Iteration 4618, loss = 0.00258022\n","Iteration 4619, loss = 0.00257969\n","Iteration 4620, loss = 0.00257915\n","Iteration 4621, loss = 0.00257862\n","Iteration 4622, loss = 0.00257808\n","Iteration 4623, loss = 0.00257755\n","Iteration 4624, loss = 0.00257701\n","Iteration 4625, loss = 0.00257648\n","Iteration 4626, loss = 0.00257594\n","Iteration 4627, loss = 0.00257541\n","Iteration 4628, loss = 0.00257487\n","Iteration 4629, loss = 0.00257434\n","Iteration 4630, loss = 0.00257381\n","Iteration 4631, loss = 0.00257327\n","Iteration 4632, loss = 0.00257274\n","Iteration 4633, loss = 0.00257221\n","Iteration 4634, loss = 0.00257167\n","Iteration 4635, loss = 0.00257114\n","Iteration 4636, loss = 0.00257061\n","Iteration 4637, loss = 0.00257008\n","Iteration 4638, loss = 0.00256954\n","Iteration 4639, loss = 0.00256901\n","Iteration 4640, loss = 0.00256848\n","Iteration 4641, loss = 0.00256795\n","Iteration 4642, loss = 0.00256742\n","Iteration 4643, loss = 0.00256689\n","Iteration 4644, loss = 0.00256636\n","Iteration 4645, loss = 0.00256583\n","Iteration 4646, loss = 0.00256530\n","Iteration 4647, loss = 0.00256477\n","Iteration 4648, loss = 0.00256424\n","Iteration 4649, loss = 0.00256371\n","Iteration 4650, loss = 0.00256318\n","Iteration 4651, loss = 0.00256265\n","Iteration 4652, loss = 0.00256213\n","Iteration 4653, loss = 0.00256160\n","Iteration 4654, loss = 0.00256107\n","Iteration 4655, loss = 0.00256054\n","Iteration 4656, loss = 0.00256001\n","Iteration 4657, loss = 0.00255949\n","Iteration 4658, loss = 0.00255896\n","Iteration 4659, loss = 0.00255843\n","Iteration 4660, loss = 0.00255791\n","Iteration 4661, loss = 0.00255738\n","Iteration 4662, loss = 0.00255685\n","Iteration 4663, loss = 0.00255633\n","Iteration 4664, loss = 0.00255580\n","Iteration 4665, loss = 0.00255528\n","Iteration 4666, loss = 0.00255475\n","Iteration 4667, loss = 0.00255423\n","Iteration 4668, loss = 0.00255370\n","Iteration 4669, loss = 0.00255318\n","Iteration 4670, loss = 0.00255265\n","Iteration 4671, loss = 0.00255213\n","Iteration 4672, loss = 0.00255161\n","Iteration 4673, loss = 0.00255108\n","Iteration 4674, loss = 0.00255056\n","Iteration 4675, loss = 0.00255004\n","Iteration 4676, loss = 0.00254951\n","Iteration 4677, loss = 0.00254899\n","Iteration 4678, loss = 0.00254847\n","Iteration 4679, loss = 0.00254795\n","Iteration 4680, loss = 0.00254742\n","Iteration 4681, loss = 0.00254690\n","Iteration 4682, loss = 0.00254638\n","Iteration 4683, loss = 0.00254586\n","Iteration 4684, loss = 0.00254534\n","Iteration 4685, loss = 0.00254482\n","Iteration 4686, loss = 0.00254430\n","Iteration 4687, loss = 0.00254378\n","Iteration 4688, loss = 0.00254326\n","Iteration 4689, loss = 0.00254274\n","Iteration 4690, loss = 0.00254222\n","Iteration 4691, loss = 0.00254170\n","Iteration 4692, loss = 0.00254118\n","Iteration 4693, loss = 0.00254066\n","Iteration 4694, loss = 0.00254014\n","Iteration 4695, loss = 0.00253962\n","Iteration 4696, loss = 0.00253910\n","Iteration 4697, loss = 0.00253859\n","Iteration 4698, loss = 0.00253807\n","Iteration 4699, loss = 0.00253755\n","Iteration 4700, loss = 0.00253703\n","Iteration 4701, loss = 0.00253652\n","Iteration 4702, loss = 0.00253600\n","Iteration 4703, loss = 0.00253548\n","Iteration 4704, loss = 0.00253497\n","Iteration 4705, loss = 0.00253445\n","Iteration 4706, loss = 0.00253393\n","Iteration 4707, loss = 0.00253342\n","Iteration 4708, loss = 0.00253290\n","Iteration 4709, loss = 0.00253239\n","Iteration 4710, loss = 0.00253187\n","Iteration 4711, loss = 0.00253136\n","Iteration 4712, loss = 0.00253084\n","Iteration 4713, loss = 0.00253033\n","Iteration 4714, loss = 0.00252981\n","Iteration 4715, loss = 0.00252930\n","Iteration 4716, loss = 0.00252879\n","Iteration 4717, loss = 0.00252827\n","Iteration 4718, loss = 0.00252776\n","Iteration 4719, loss = 0.00252725\n","Iteration 4720, loss = 0.00252673\n","Iteration 4721, loss = 0.00252622\n","Iteration 4722, loss = 0.00252571\n","Iteration 4723, loss = 0.00252519\n","Iteration 4724, loss = 0.00252468\n","Iteration 4725, loss = 0.00252417\n","Iteration 4726, loss = 0.00252366\n","Iteration 4727, loss = 0.00252315\n","Iteration 4728, loss = 0.00252264\n","Iteration 4729, loss = 0.00252213\n","Iteration 4730, loss = 0.00252162\n","Iteration 4731, loss = 0.00252111\n","Iteration 4732, loss = 0.00252060\n","Iteration 4733, loss = 0.00252009\n","Iteration 4734, loss = 0.00251958\n","Iteration 4735, loss = 0.00251907\n","Iteration 4736, loss = 0.00251856\n","Iteration 4737, loss = 0.00251805\n","Iteration 4738, loss = 0.00251754\n","Iteration 4739, loss = 0.00251703\n","Iteration 4740, loss = 0.00251652\n","Iteration 4741, loss = 0.00251601\n","Iteration 4742, loss = 0.00251551\n","Iteration 4743, loss = 0.00251500\n","Iteration 4744, loss = 0.00251449\n","Iteration 4745, loss = 0.00251398\n","Iteration 4746, loss = 0.00251348\n","Iteration 4747, loss = 0.00251297\n","Iteration 4748, loss = 0.00251246\n","Iteration 4749, loss = 0.00251196\n","Iteration 4750, loss = 0.00251145\n","Iteration 4751, loss = 0.00251094\n","Iteration 4752, loss = 0.00251044\n","Iteration 4753, loss = 0.00250993\n","Iteration 4754, loss = 0.00250943\n","Iteration 4755, loss = 0.00250892\n","Iteration 4756, loss = 0.00250842\n","Iteration 4757, loss = 0.00250791\n","Iteration 4758, loss = 0.00250741\n","Iteration 4759, loss = 0.00250690\n","Iteration 4760, loss = 0.00250640\n","Iteration 4761, loss = 0.00250590\n","Iteration 4762, loss = 0.00250539\n","Iteration 4763, loss = 0.00250489\n","Iteration 4764, loss = 0.00250438\n","Iteration 4765, loss = 0.00250388\n","Iteration 4766, loss = 0.00250338\n","Iteration 4767, loss = 0.00250288\n","Iteration 4768, loss = 0.00250237\n","Iteration 4769, loss = 0.00250187\n","Iteration 4770, loss = 0.00250137\n","Iteration 4771, loss = 0.00250087\n","Iteration 4772, loss = 0.00250037\n","Iteration 4773, loss = 0.00249987\n","Iteration 4774, loss = 0.00249937\n","Iteration 4775, loss = 0.00249886\n","Iteration 4776, loss = 0.00249836\n","Iteration 4777, loss = 0.00249786\n","Iteration 4778, loss = 0.00249736\n","Iteration 4779, loss = 0.00249686\n","Iteration 4780, loss = 0.00249636\n","Iteration 4781, loss = 0.00249586\n","Iteration 4782, loss = 0.00249536\n","Iteration 4783, loss = 0.00249487\n","Iteration 4784, loss = 0.00249437\n","Iteration 4785, loss = 0.00249387\n","Iteration 4786, loss = 0.00249337\n","Iteration 4787, loss = 0.00249287\n","Iteration 4788, loss = 0.00249237\n","Iteration 4789, loss = 0.00249188\n","Iteration 4790, loss = 0.00249138\n","Iteration 4791, loss = 0.00249088\n","Iteration 4792, loss = 0.00249038\n","Iteration 4793, loss = 0.00248989\n","Iteration 4794, loss = 0.00248939\n","Iteration 4795, loss = 0.00248889\n","Iteration 4796, loss = 0.00248840\n","Iteration 4797, loss = 0.00248790\n","Iteration 4798, loss = 0.00248741\n","Iteration 4799, loss = 0.00248691\n","Iteration 4800, loss = 0.00248641\n","Iteration 4801, loss = 0.00248592\n","Iteration 4802, loss = 0.00248542\n","Iteration 4803, loss = 0.00248493\n","Iteration 4804, loss = 0.00248443\n","Iteration 4805, loss = 0.00248394\n","Iteration 4806, loss = 0.00248345\n","Iteration 4807, loss = 0.00248295\n","Iteration 4808, loss = 0.00248246\n","Iteration 4809, loss = 0.00248196\n","Iteration 4810, loss = 0.00248147\n","Iteration 4811, loss = 0.00248098\n","Iteration 4812, loss = 0.00248049\n","Iteration 4813, loss = 0.00247999\n","Iteration 4814, loss = 0.00247950\n","Iteration 4815, loss = 0.00247901\n","Iteration 4816, loss = 0.00247852\n","Iteration 4817, loss = 0.00247802\n","Iteration 4818, loss = 0.00247753\n","Iteration 4819, loss = 0.00247704\n","Iteration 4820, loss = 0.00247655\n","Iteration 4821, loss = 0.00247606\n","Iteration 4822, loss = 0.00247557\n","Iteration 4823, loss = 0.00247508\n","Iteration 4824, loss = 0.00247459\n","Iteration 4825, loss = 0.00247410\n","Iteration 4826, loss = 0.00247361\n","Iteration 4827, loss = 0.00247312\n","Iteration 4828, loss = 0.00247263\n","Iteration 4829, loss = 0.00247214\n","Iteration 4830, loss = 0.00247165\n","Iteration 4831, loss = 0.00247116\n","Iteration 4832, loss = 0.00247067\n","Iteration 4833, loss = 0.00247018\n","Iteration 4834, loss = 0.00246969\n","Iteration 4835, loss = 0.00246921\n","Iteration 4836, loss = 0.00246872\n","Iteration 4837, loss = 0.00246823\n","Iteration 4838, loss = 0.00246774\n","Iteration 4839, loss = 0.00246726\n","Iteration 4840, loss = 0.00246677\n","Iteration 4841, loss = 0.00246628\n","Iteration 4842, loss = 0.00246580\n","Iteration 4843, loss = 0.00246531\n","Iteration 4844, loss = 0.00246482\n","Iteration 4845, loss = 0.00246434\n","Iteration 4846, loss = 0.00246385\n","Iteration 4847, loss = 0.00246337\n","Iteration 4848, loss = 0.00246288\n","Iteration 4849, loss = 0.00246240\n","Iteration 4850, loss = 0.00246191\n","Iteration 4851, loss = 0.00246143\n","Iteration 4852, loss = 0.00246094\n","Iteration 4853, loss = 0.00246046\n","Iteration 4854, loss = 0.00245997\n","Iteration 4855, loss = 0.00245949\n","Iteration 4856, loss = 0.00245900\n","Iteration 4857, loss = 0.00245852\n","Iteration 4858, loss = 0.00245804\n","Iteration 4859, loss = 0.00245755\n","Iteration 4860, loss = 0.00245707\n","Iteration 4861, loss = 0.00245659\n","Iteration 4862, loss = 0.00245611\n","Iteration 4863, loss = 0.00245562\n","Iteration 4864, loss = 0.00245514\n","Iteration 4865, loss = 0.00245466\n","Iteration 4866, loss = 0.00245418\n","Iteration 4867, loss = 0.00245370\n","Iteration 4868, loss = 0.00245322\n","Iteration 4869, loss = 0.00245273\n","Iteration 4870, loss = 0.00245225\n","Iteration 4871, loss = 0.00245177\n","Iteration 4872, loss = 0.00245129\n","Iteration 4873, loss = 0.00245081\n","Iteration 4874, loss = 0.00245033\n","Iteration 4875, loss = 0.00244985\n","Iteration 4876, loss = 0.00244937\n","Iteration 4877, loss = 0.00244889\n","Iteration 4878, loss = 0.00244841\n","Iteration 4879, loss = 0.00244793\n","Iteration 4880, loss = 0.00244746\n","Iteration 4881, loss = 0.00244698\n","Iteration 4882, loss = 0.00244650\n","Iteration 4883, loss = 0.00244602\n","Iteration 4884, loss = 0.00244554\n","Iteration 4885, loss = 0.00244506\n","Iteration 4886, loss = 0.00244459\n","Iteration 4887, loss = 0.00244411\n","Iteration 4888, loss = 0.00244363\n","Iteration 4889, loss = 0.00244315\n","Iteration 4890, loss = 0.00244268\n","Iteration 4891, loss = 0.00244220\n","Iteration 4892, loss = 0.00244172\n","Iteration 4893, loss = 0.00244125\n","Iteration 4894, loss = 0.00244077\n","Iteration 4895, loss = 0.00244030\n","Iteration 4896, loss = 0.00243982\n","Iteration 4897, loss = 0.00243935\n","Iteration 4898, loss = 0.00243887\n","Iteration 4899, loss = 0.00243840\n","Iteration 4900, loss = 0.00243792\n","Iteration 4901, loss = 0.00243745\n","Iteration 4902, loss = 0.00243697\n","Iteration 4903, loss = 0.00243650\n","Iteration 4904, loss = 0.00243602\n","Iteration 4905, loss = 0.00243555\n","Iteration 4906, loss = 0.00243508\n","Iteration 4907, loss = 0.00243460\n","Iteration 4908, loss = 0.00243413\n","Iteration 4909, loss = 0.00243366\n","Iteration 4910, loss = 0.00243318\n","Iteration 4911, loss = 0.00243271\n","Iteration 4912, loss = 0.00243224\n","Iteration 4913, loss = 0.00243177\n","Iteration 4914, loss = 0.00243129\n","Iteration 4915, loss = 0.00243082\n","Iteration 4916, loss = 0.00243035\n","Iteration 4917, loss = 0.00242988\n","Iteration 4918, loss = 0.00242941\n","Iteration 4919, loss = 0.00242894\n","Iteration 4920, loss = 0.00242847\n","Iteration 4921, loss = 0.00242799\n","Iteration 4922, loss = 0.00242752\n","Iteration 4923, loss = 0.00242705\n","Iteration 4924, loss = 0.00242658\n","Iteration 4925, loss = 0.00242611\n","Iteration 4926, loss = 0.00242564\n","Iteration 4927, loss = 0.00242517\n","Iteration 4928, loss = 0.00242470\n","Iteration 4929, loss = 0.00242424\n","Iteration 4930, loss = 0.00242377\n","Iteration 4931, loss = 0.00242330\n","Iteration 4932, loss = 0.00242283\n","Iteration 4933, loss = 0.00242236\n","Iteration 4934, loss = 0.00242189\n","Iteration 4935, loss = 0.00242143\n","Iteration 4936, loss = 0.00242096\n","Iteration 4937, loss = 0.00242049\n","Iteration 4938, loss = 0.00242002\n","Iteration 4939, loss = 0.00241956\n","Iteration 4940, loss = 0.00241909\n","Iteration 4941, loss = 0.00241862\n","Iteration 4942, loss = 0.00241815\n","Iteration 4943, loss = 0.00241769\n","Iteration 4944, loss = 0.00241722\n","Iteration 4945, loss = 0.00241676\n","Iteration 4946, loss = 0.00241629\n","Iteration 4947, loss = 0.00241582\n","Iteration 4948, loss = 0.00241536\n","Iteration 4949, loss = 0.00241489\n","Iteration 4950, loss = 0.00241443\n","Iteration 4951, loss = 0.00241396\n","Iteration 4952, loss = 0.00241350\n","Iteration 4953, loss = 0.00241304\n","Iteration 4954, loss = 0.00241257\n","Iteration 4955, loss = 0.00241211\n","Iteration 4956, loss = 0.00241164\n","Iteration 4957, loss = 0.00241118\n","Iteration 4958, loss = 0.00241072\n","Iteration 4959, loss = 0.00241025\n","Iteration 4960, loss = 0.00240979\n","Iteration 4961, loss = 0.00240933\n","Iteration 4962, loss = 0.00240886\n","Iteration 4963, loss = 0.00240840\n","Iteration 4964, loss = 0.00240794\n","Iteration 4965, loss = 0.00240748\n","Iteration 4966, loss = 0.00240701\n","Iteration 4967, loss = 0.00240655\n","Iteration 4968, loss = 0.00240609\n","Iteration 4969, loss = 0.00240563\n","Iteration 4970, loss = 0.00240517\n","Iteration 4971, loss = 0.00240471\n","Iteration 4972, loss = 0.00240425\n","Iteration 4973, loss = 0.00240379\n","Iteration 4974, loss = 0.00240333\n","Iteration 4975, loss = 0.00240287\n","Iteration 4976, loss = 0.00240241\n","Iteration 4977, loss = 0.00240195\n","Iteration 4978, loss = 0.00240149\n","Iteration 4979, loss = 0.00240103\n","Iteration 4980, loss = 0.00240057\n","Iteration 4981, loss = 0.00240011\n","Iteration 4982, loss = 0.00239965\n","Iteration 4983, loss = 0.00239919\n","Iteration 4984, loss = 0.00239873\n","Iteration 4985, loss = 0.00239827\n","Iteration 4986, loss = 0.00239782\n","Iteration 4987, loss = 0.00239736\n","Iteration 4988, loss = 0.00239690\n","Iteration 4989, loss = 0.00239644\n","Iteration 4990, loss = 0.00239598\n","Iteration 4991, loss = 0.00239553\n","Iteration 4992, loss = 0.00239507\n","Iteration 4993, loss = 0.00239461\n","Iteration 4994, loss = 0.00239416\n","Iteration 4995, loss = 0.00239370\n","Iteration 4996, loss = 0.00239324\n","Iteration 4997, loss = 0.00239279\n","Iteration 4998, loss = 0.00239233\n","Iteration 4999, loss = 0.00239188\n","Iteration 5000, loss = 0.00239142\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=4, learning_rate_init=0.01, max_iter=5000,\n","              random_state=123, solver='sgd', tol=1e-07, verbose=1)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=4, learning_rate_init=0.01, max_iter=5000,\n","              random_state=123, solver=&#x27;sgd&#x27;, tol=1e-07, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=4, learning_rate_init=0.01, max_iter=5000,\n","              random_state=123, solver=&#x27;sgd&#x27;, tol=1e-07, verbose=1)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"ovB9O5nIUlmH"},"source":["###Plotting Loss vs Epochs graph"]},{"cell_type":"code","metadata":{"id":"cooked-liberal","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1750021763502,"user_tz":-330,"elapsed":286,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}},"outputId":"f22f27a1-1b6a-4ca5-ad31-6a11431b8f8e"},"source":["plt.plot(clf.loss_curve_)\n","plt.title('Loss vs Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"false-farming"},"source":["pred = clf.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"final-hopkins"},"source":["import pandas as pd\n","df = pd.DataFrame({'Actaul': y, 'Predicted': pred})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"confidential-judges","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1750021763557,"user_tz":-330,"elapsed":32,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}},"outputId":"4befa54f-8c2a-4bdb-ede4-391e36c6a527"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Actaul  Predicted\n","0       1          1\n","1       0          0\n","2       0          0\n","3       0          0\n","4       1          1"],"text/html":["\n","  <div id=\"df-0c7b5578-6cee-4ec9-a5b5-d4df37761819\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Actaul</th>\n","      <th>Predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c7b5578-6cee-4ec9-a5b5-d4df37761819')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0c7b5578-6cee-4ec9-a5b5-d4df37761819 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0c7b5578-6cee-4ec9-a5b5-d4df37761819');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-629781fa-2053-4d20-afe3-60fb734169d7\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-629781fa-2053-4d20-afe3-60fb734169d7')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-629781fa-2053-4d20-afe3-60fb734169d7 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_0f4c5875-076b-4843-8e81-e78d7bcebfcd\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_0f4c5875-076b-4843-8e81-e78d7bcebfcd button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Actaul\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"tC2rt1ZxrgC7"},"source":["## Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pke4SQ5NgNFX"},"source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"sdgfdjhfgjfhjh\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cTetkuegP7d"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFQw0ddId_Ej"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CXztFuygSBG","cellView":"form","outputId":"653d75f0-6f14-4dd8-bab8-a5c644b163b3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750021763980,"user_tz":-330,"elapsed":388,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}}},"source":["#@title Run this cell to submit your notebook  { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your submission is successful.\n","Ref Id: 3969\n","Date of submission:  16 Jun 2025\n","Time of submission:  02:39:24\n","View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\n"]}]}]}