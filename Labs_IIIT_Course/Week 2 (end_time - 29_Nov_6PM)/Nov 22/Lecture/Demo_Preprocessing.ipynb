{"cells":[{"cell_type":"markdown","metadata":{"id":"H7W6IuZjFVoD"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bGgCVs0KGzDN"},"source":["### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"KM4ievIW3yZy"},"source":["## Setup Steps"]},{"cell_type":"code","metadata":{"id":"AOr82B5k3yZ1"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ut7W3PwZ3yZ2"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"7AivmwAY3yZ3"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"Demo_Preprocessing\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Purchase_Dataset.csv\")\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/submissions/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword(), \"batch\" : \"\"}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts() and getComments():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"id\" : Id, \"file_hash\" : file_hash,\n","              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook, \"batch\" : \"\"}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n","        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"xnDSZSsDCMtk"},"source":["# Preprocessing Data\n","\n","The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"CIEwhefyCMtt"},"source":["## Standardization\n","\n","Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n","In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n","\n","For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n","\n","The function scale provides a quick and easy way to perform this operation on a single array-like dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"eM2PgGr7CMtx"},"outputs":[],"source":["from sklearn import preprocessing\n","import numpy as np\n","X = np.array([[ 1., -1.,  2.],\n","              [ 2.,  0.,  0.],\n","              [ 0.,  1., -1.]])\n","X_scaled = preprocessing.scale(X)\n","\n","X_scaled\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"fqqgZbOACMt1"},"outputs":[],"source":["print(X_scaled.mean(axis=0))\n","\n","\n","print(X_scaled.std(axis=0))\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"GqB7HLhvCMt2"},"source":["But this has some drawbacks.\n","\n","The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"M99ghLd-CMt3"},"outputs":[],"source":["scaler = preprocessing.StandardScaler().fit(X)\n","scaler"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"PuEwSaUaCMt5"},"outputs":[],"source":["scaler.mean_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"e6PMDhdPCMt7"},"outputs":[],"source":["scaler.scale_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"bBqiwqIkCMt8"},"outputs":[],"source":["scaler.transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"bvNrYeo1CMt-"},"outputs":[],"source":["scaler.transform([[-1.,  1., 0.]])"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NdriVEcdCMt-"},"source":["#### Scaling features to a range\n","\n","An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\n","The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n","\n","Here is an example to scale a toy data matrix to the [0, 1] range:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"qbZmkvYSCMt_"},"outputs":[],"source":["X_train = np.array([[ 1., -1.,  2.],\n","                    [ 2.,  0.,  0.],\n","                    [ 0.,  1., -1.]])\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(X_train)\n","X_train_minmax"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"PK-iVzDxCMuA"},"source":["## Normalization\n","\n","Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n","\n","This assumption is the base of the Vector Space Model often used in text classification and clustering contexts.\n","\n","The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1 or l2 norms:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"dgT1BESOCMuA"},"outputs":[],"source":["X = [[ 1., -1.,  2.],\n","     [ 2.,  0.,  0.],\n","     [ 0.,  1., -1.]]\n","X_normalized = preprocessing.normalize(X, norm='l2')\n","\n","X_normalized\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"pn4PkxgbCMuB"},"source":["The preprocessing module further provides a utility class Normalizer that implements the same operation using the Transformer API (even though the fit method is useless in this case: the class is stateless as this operation treats samples independently).\n","\n","This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"y-LkQ6OkCMuD"},"outputs":[],"source":["normalizer = preprocessing.Normalizer().fit(X)\n","normalizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"cMhkBdAuCMuF"},"outputs":[],"source":["normalizer.transform(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"4mRDJcZVCMuF"},"outputs":[],"source":["normalizer.transform([[-1.,  1., 0.]])"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2iPdp2sZCMuG"},"source":["## Binarization\n","\n","Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the sklearn.neural_network.BernoulliRBM.\n","\n","It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.\n","\n","As for the Normalizer, the utility class Binarizer is meant to be used in the early stages of sklearn.pipeline.Pipeline. The fit method does nothing as each sample is treated independently of others:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"2v8haakuCMuG"},"outputs":[],"source":["X = [[ 1., -1.,  2.],\n","     [ 2.,  0.,  0.],\n","     [ 0.,  1., -1.]]\n","\n","binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n","binarizer"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"E7LMCbYuCMuJ"},"outputs":[],"source":["binarizer.transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"l_8NHbFHCMuK"},"outputs":[],"source":["binarizer = preprocessing.Binarizer(threshold=1.1)\n","binarizer.transform(X)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"q0Z6Tg84CMuL"},"source":["## Encoding Categorical Features\n","\n","Often features are not given as continuous values but categorical. For example a person could have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [0, 1, 3] while [\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\n","\n","Such integer representation can not be used directly with scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).\n","\n","One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder. This estimator transforms each categorical feature with m possible values into m binary features, with only one active.\n","\n","Continuing the example above:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"GhF7c6frCMuL"},"outputs":[],"source":["enc = preprocessing.OneHotEncoder()\n","enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n","\n","\n","enc.transform([[0, 1, 3]]).toarray()"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"sVclv6HGCMuM"},"source":["## Imputation of missing values\n","\n","For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n","\n","The Imputer class provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. This class also allows for different missing values encodings.\n","\n","The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"VkxsznU9CMuM"},"outputs":[],"source":["import numpy as np\n","from sklearn.impute import SimpleImputer # Import SimpleImputer instead of Imputer\n","\n","# Replace 'Imputer' with 'SimpleImputer'\n","imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imp.fit([[1, 2], [np.nan, 3], [7, 6]])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"N-Ky0XjUCMuO"},"outputs":[],"source":["X = [[np.nan, 2], [6, np.nan], [7, 6]]\n","print(imp.transform(X))"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"0b6LK39UCMuO"},"source":["## Generating Polynomial Features\n","\n","Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in PolynomialFeatures:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"OR0_n-UnCMuP"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import PolynomialFeatures\n","X = np.arange(6).reshape(3, 2)\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"bkq51gqvCMuQ"},"outputs":[],"source":["poly = PolynomialFeatures(2)\n","poly.fit_transform(X)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"sOZm-wgdCMuR"},"source":["## Label Binarization\n","\n","LabelBinarizer is a utility class to help create a label indicator matrix from a list of multi-class labels:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"5u9vNimxCMuR"},"outputs":[],"source":["from sklearn import preprocessing\n","lb = preprocessing.LabelBinarizer()\n","lb.fit([1, 2, 6, 4, 2])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"-tJ-eZR6CMuS"},"outputs":[],"source":["lb.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"lwRXfKLGCMuU"},"outputs":[],"source":["lb.transform([1,6])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"qoEFc591CMuV"},"outputs":[],"source":["lb = preprocessing.MultiLabelBinarizer()\n","lb.fit_transform([(1, 2), (3,)])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"c6DGF9BPCMuW"},"outputs":[],"source":["lb.classes_"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"SElCLHnoCMuW"},"source":["## Label Encoding\n","\n","LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. This is sometimes useful for writing efficient Cython routines. LabelEncoder can be used as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"v44Vt862CMuX"},"outputs":[],"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit([1, 2, 2, 6])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"VJt-JlqSCMuY"},"outputs":[],"source":["le.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"vPaiytSCCMul"},"outputs":[],"source":["le.transform([1, 1, 2, 6])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"wp2U6F-oCMum"},"outputs":[],"source":["le.inverse_transform([0, 0, 1, 2])"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kgE76ed1CMun"},"source":["## Custom Transformers\n","\n","Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with FunctionTransformer. For example, to build a transformer that applies a log transformation in a pipeline, do:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"id":"CK175b-QCMun"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import FunctionTransformer\n","transformer = FunctionTransformer(np.log1p)\n","X = np.array([[0, 1], [2, 3]])\n","transformer.transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"UNGbjNYsCMuo"},"outputs":[],"source":["np.log1p?"]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r35isHfTVGKc"},"source":["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for Ungrading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}