{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gGDY44vTUS9r"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n","### Not for Grading\n"]},{"cell_type":"markdown","metadata":{"id":"KPOdKTG0US9t"},"source":["### Learning Objectives:\n","\n","At the end of the experiment, you will be able to:\n","\n","* understand word2vec in action."]},{"cell_type":"markdown","metadata":{"id":"GDTJ3UXdUS9v"},"source":["In this experiment we will use **Mahabharata** as our text corpus"]},{"cell_type":"markdown","metadata":{"id":"FhA-4D7SrIpo"},"source":["## Setup Steps"]},{"cell_type":"code","metadata":{"id":"DrAZGKNorOH8"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"2418580\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLpBVa3MrRPn"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"9898989865\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRvlfkw5rV_Q","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"Demo_Mahabharata\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Saturday_Experiment/MB.txt\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Saturday_Experiment/word2vec.png\")\n","\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts() and getComments():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"id\" : Id, \"file_hash\" : file_hash,\n","              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iiith.talentsprint.com/notebook_submissions\")\n","        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FL95z915CeZh"},"source":["# Import required packages"]},{"cell_type":"code","source":["!pip3 install gensim"],"metadata":{"id":"s3S8QiYH0ykA"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qd6Z7aW0qmgX"},"source":["# Import nltk package\n","import nltk\n","\n","# Download wordnet from NLTK to perform Stemmer\n","nltk.download(\"wordnet\")\n","\n","# Python library for Vector space modeling and topic modeling\n","import gensim\n","\n","# Regular Expression\n","import re\n","\n","# Basic Python Packages\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnSw4jIMw2Y1"},"source":["### Pre-Processing and Develop Word Embeddings\n","\n","* Load the Mahabharata corpus\n","* Perform Stemming and removing the stop words\n","* Generate word embeddings"]},{"cell_type":"code","metadata":{"id":"eMXInB-2gUSL"},"source":["# Stemmer with Python nltk package\n","stemmer = nltk.PorterStemmer()\n","\n","# Download all the stopwords from the NLTK package using nltk.download('stopwords')\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","stopWords = set(stopwords.words(\"english\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzDazox-frQe"},"source":["MB_words = []\n","\n","# Open the text file in read mode\n","with open(\"MB.txt\", \"r\") as file:\n","\n","   # Store each line in the file as a separate element in a list\n","   lines = file.readlines()\n","\n","   # Take each line from the list of lines and collect all the words\n","   for line in lines:\n","      # findall() function returns a list containing all matches between a-z\n","      words = re.findall(r\"(\\b[a-z][a-z]*\\b)\", line.lower())\n","\n","      # Stemming each word in to a list, if the word is not in stopwords\n","      words = [stemmer.stem(word) for word in words if word not in stopWords]\n","\n","      MB_words.append(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(MB_words)"],"metadata":{"id":"gKfEsZL4ORmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MB_words"],"metadata":{"id":"gDFXLwSSOhfn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the Gensim model on the MB_words"],"metadata":{"id":"jPyZkgoZCONR"}},{"cell_type":"markdown","source":["The implementation below is creating a Word2Vec model that will learn the semantic relationships between words based on the data MB_words. The model will then be able to represent words as numerical vectors.\n","\n","Both skip-gram and CBOW are algorithms used to train Word2Vec models. Here's a simplified explanation of the differences:\n","\n","Skip-gram: `This algorithm predicts surrounding words (context) based on a central word. It helps the model learn the meaning of a word by considering the words that often appear around it.`\n","\n","CBOW: `This algorithm predicts a central word based on its surrounding words. It helps the model learn the meaning of a word by considering the context in which it appears.`\n","\n","Training Model from gensim package for getting vocabulary and vectors:\n","\n","Model Parameters:\n","\n","  `min_count: ignore words that appear less than the specified count`\n","\n","  `size*: Dimensionality of the word vectors. By default it is 100`\n","\n","  `sg: ({0, 1}, optional) â€“ Training algorithm: 1 for skip-gram; otherwise CBOW.`\n","\n","    1: If sg is set to 1, the model will use the skip-gram algorithm.\n","    0: If sg is set to 0 (or not provided), the model will use the CBOW (Continuous Bag-of-Words) algorithm.\n","\n"," [Documentation Link](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html)"],"metadata":{"id":"C8VP3EYy-MqE"}},{"cell_type":"code","metadata":{"id":"Dg3z8-NQola3"},"source":["# Train the gensim model on the MB_words\n","model = gensim.models.Word2Vec(MB_words, min_count=120)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6eDzmJnnKiPM"},"source":["# Total number of words in the trained model\n","print(\"Total number of words in the trained model: \", len(model.wv.index_to_key))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.wv.index_to_key[0]"],"metadata":{"id":"JR4BXKB6xEHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.wv.key_to_index"],"metadata":{"id":"J63JFKyOwGG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a_RF-5WLn_Z"},"source":["# Number of vectors generated for each word\n","print(\"Dimensionality of word embeddings: \", len(model.wv.vectors[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.wv.vectors.shape"],"metadata":{"id":"xudOVBuGRmrD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HrlBP5AtSsR"},"source":["### Construct the word and vector list by iterating through the vocabulary of the pretrained word2vec model."]},{"cell_type":"code","metadata":{"id":"y_xS-MhisKw7"},"source":["words_list = []\n","vector_list = []\n","\n","for i,word in enumerate(model.wv.index_to_key):\n","    try :\n","        words_list.append(word)\n","        vector_list.append(model.wv.vectors[i])  # append(model[i])\n","    except :\n","        pass\n","\n","words_list = np.array(words_list)\n","vector_list = np.array(vector_list)\n","\n","print(words_list.shape, vector_list.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words_list"],"metadata":{"id":"c4L2gdozSH0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vector_list.shape"],"metadata":{"id":"cl0Asf5JSo-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vector_list[1]"],"metadata":{"id":"uUmCBw0Jypzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPCOgxJjUS-w"},"source":["### Visualization and Plotting the reduced Word2Vec representation\n","\n","* As vector_list dimensions are huge, reduce the dimensions of the vectors to 2D using PCA"]},{"cell_type":"code","metadata":{"id":"Nu7hGg94s9IX"},"source":["# Check the shape of the vector_list before reducing its dimensions\n","print(\"Shape of the vectors_list before reducing the dimensions: \", vector_list.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6NfDieGYMmAe"},"source":["* Applying PCA to reduce the dimensions of the vectors"]},{"cell_type":"code","metadata":{"id":"n3NQ4_aStB1k"},"source":["# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n","from sklearn.decomposition import PCA\n","\n","# n_components in PCA specifies the no.of dimensions\n","pca = PCA(n_components=2)\n","\n","# Fit and transform the vectors using PCA model\n","reduced_vector = pca.fit_transform(vector_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spd_K0-BuAj6"},"source":["# Check the shape of the reduced_vector after reducing its dimensions\n","print(\"Shape of the vectors_list after reducing the dimensions to 2D: \", reduced_vector.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfznW8tgRkiH"},"source":["* Visualize the reduced Word2Vec representation"]},{"cell_type":"code","metadata":{"id":"0G1YmkAKQ5he"},"source":["colors = [\"green\" for i in range(len(reduced_vector))]\n","\n","x = []\n","y = []\n","for vec in reduced_vector:\n","    x.append(vec[0])\n","    y.append(vec[1])\n","\n","plt.figure(figsize=(28,20))\n","for i in range(len(words_list)):\n","    plt.scatter(x[i],y[i], color=colors[i])\n","    plt.annotate(words_list[i], xy=(x[i], y[i]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_-v2yVWouGM2"},"source":["### Choose few characters from Mahabharata and find the similar characters\n","\n","* Find the location of the chosen characters in word_list\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Megx8-C0uFJ1"},"source":["MB_characters = ['krishna', 'arjuna', 'pandu', 'bhima', 'sakuni', 'duryodhana', 'bhishma', 'kunti', 'karna', 'madri', 'nakula', 'sahadeva', 'draupadi']\n","\n","# Get the location of MB_characters from the words_list\n","locs = [np.where(words_list == x)[0][0] for x in MB_characters]\n","\n","print(\"The location of the selected characters \\n\", locs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ohI96wuYN_F7"},"source":["* Visualization of the chosen characters in the Mahabharata"]},{"cell_type":"code","source":["model.wv.key_to_index['krishna']"],"metadata":{"id":"MZ5sQ8Puz3mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8En0XkSSuagA"},"source":["# Generating the vectors for chosen characters and reducing the dimensions using PCA to plot in 2-D plane\n","\n","fig = plt.figure(figsize=(14,6))\n","ax = fig.add_subplot(111)\n","for character, pos in zip(MB_characters, locs):\n","  print(character)\n","  char_v = vector_list[pos]\n","  # 'char_v' contains the vector representation of each character\n","  # Adding one more dimension to the 'char_v', while PCA allows only 2-d array\n","  # Converting it back to 1-d array to plot the transformed vector\n","  value = pca.transform([char_v])[0]\n","  ax.plot(value[0], value[1],  \"r*\")\n","  plt.annotate(words_list[pos], xy=value, xytext=value+0.01)\n","\n","plt.show()\n","fig.savefig('word2vec.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_vVIRCiu9_Y"},"source":["* Find the top-5 similar characters for the selected characters"]},{"cell_type":"code","metadata":{"id":"cRQEkPQWu9Qf"},"source":["names= []\n","for character in MB_characters:\n","    near = model.wv.most_similar(character, topn = 10)\n","    nearNames = [x[0] for x in near]\n","    names.append(nearNames)\n","\n","pd.DataFrame(names,columns=['Similarity_1','Similarity_2','Similarity_3','Similarity_4','Similarity_5', '6', '7', '8', '9', '10'], index = MB_characters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tC2rt1ZxrgC7"},"source":["## Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pke4SQ5NgNFX"},"source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cTetkuegP7d"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFQw0ddId_Ej"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CXztFuygSBG","cellView":"form"},"source":["#@title Run this cell to submit your notebook  { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}